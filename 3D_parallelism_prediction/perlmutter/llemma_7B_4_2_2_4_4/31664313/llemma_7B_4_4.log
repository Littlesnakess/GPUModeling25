[2024-10-09 13:43:30,043] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NeoXArgs.from_ymls() ['/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/configs/llemma_7B_4_2_2.yml', '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/configs/local_setup.yml']
INFO:root:NeoXArgs.calculate_derived() Total number of GPUs determined to be: 16
-------------------- arguments --------------------
  attention_config ................ ['flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash']updated
  batch_size ...................... 4...........................updated
  checkpoint_activations .......... True........................updated
  checkpoint_factor ............... 500.........................updated
  config_files .................... {'llemma_7B_4_2_2.yml': '{\n  "pipe_parallel_size": 4,\n  "model_parallel_size": 2,\n  "make_vocab_size_divisible_by": 128,\n\n  # model settings\n  "num_layers": 32,\n  "hidden_size": 4096,\n  "num_attention_heads": 32,\n  # NB: These rotary embedding and sequence length parameters\n  # May differ from CodeLlama configs. They match what we used for\n  # Llemma continued pretraining. See https://arxiv.org/abs/2310.10631\n  # For detailed discussion\n  "seq_length": 4096,\n  "max_position_embeddings": 4096,\n  "pos_emb": "rotary",\n  "rotary_pct": 1,\n  "rotary_emb_base": 10000,\n  "no_weight_tying": true,\n  "gpt_j_residual": false,\n  "output_layer_parallelism": "column",\n  "norm": "rmsnorm",\n  "rms_norm_epsilon": 1.0e-5,\n\n  "attention_config": [[["flash"], 32]],\n\n  "scaled_upper_triang_masked_softmax_fusion": true,\n  "bias_gelu_fusion": false,\n  "use_bias_in_norms": false,\n  "use_bias_in_attn_linear": false,\n  # "activation": "swiglu",\n  "mlp_multiple_of": 256,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0001,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-8\n     }\n   },\n\n   "zero_optimization": {\n    "stage": 1,\n    "allgather_partitions": true,\n    "allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 1260000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n  # trained on 256 gpus\n  "train_micro_batch_size_per_gpu": 4,\n  "gradient_accumulation_steps": 8,\n  "data_impl": "mmap",\n\n  "checkpoint_activations": true,\n  "checkpoint_num_layers": 1,\n  "partition_activations": true,\n  "synchronize_each_layer": true,\n\n  "gradient_clipping": 1.0,\n  "weight_decay": 0.1,\n  "hidden_dropout": 0,\n  "attention_dropout": 0,\n\n  # "precision": "bfloat16",\n  # "fp32_allreduce": true,\n  # "bf16": {\n  #   "enabled": true\n  # },\n  # "data_types": {\n  #   "grad_accum_dtype": "fp32"\n  # },\n\n  # precision settings\n  "fp16": {\n    "fp16": true,\n    "enabled": true,\n    "loss_scale": 0,\n    "loss_scale_window": 1000,\n    "hysteresis": 2,\n    "min_loss_scale": 1\n  },\n\n  "train_iters": 12,\n  "lr_decay_iters": 12,\n  "distributed_backend": "nccl",\n  "lr_decay_style": "cosine",\n  # "decay_lr_to": 0.033,\n  # "warmup_iters": 500,\n  "checkpoint_factor": 500,\n  "eval_interval": 250,\n  "eval_iters": 50,\n\n  "log_interval": 100,\n  "steps_per_print": 100,\n  "wall_clock_breakdown": false,\n\n  # "tokenizer_type": "SPMTokenizer",\n  # "vocab-file": "codellama/tokenizer.model", # use tokenizer.model from Meta CodeLlama download\n\n  # "save": "/path/to/save/llema-replication",\n  # #"load": "", # once run is started, to restart from intermediate ckpt use "load" = "save"\n  # "load": "/path/to/converted/codellama_7b_weights_with_mp2",\n\n  # "finetune": true, # set to false once resuming from intermediate finetuning step\n\n  # profiling settings\n  "profile": True,\n  "profile_step_start": 2,\n  "profile_step_stop": 11,\n\n}', 'local_setup.yml': '# Suggested data paths when using GPT-NeoX locally\n{\n  # "data_path": "/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/data/pile_text_document",\n  # "data_path": "/pscratch/sd/z/zhaozh/data/pile/processed/pile_text_document",\n\n  "data-path": "/pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document",\n\n  # or for weighted datasets:\n  # "train-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "test-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "valid-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "train-data-weights": [1., 2.],\n  # "test-data-weights": [2., 1.],\n  # "valid-data-weights": [0.5, 0.4],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # "weight_by_num_documents": false,\n  # "weighted_sampler_alpha": 0.3,\n\n  "vocab_file": "/pscratch/sd/z/zhaozh/data/pile/gpt2-vocab.json",\n  "merge_file": "/pscratch/sd/z/zhaozh/data/pile/gpt2-merges.txt",\n\n  "save": "checkpoints-test",\n  "load": "checkpoints-test2",\n  "checkpoint_validation_with_forward_pass": False,\n\n  "tensorboard_dir": "tensorboard",\n  "log_dir": "logs",\n  "use_wandb": True,\n  "wandb_host": "https://api.wandb.ai",\n  "wandb_project": "neox"\n}\n'}updated
  data_impl ....................... mmap........................updated
  data_path ....................... /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_documentupdated
  dynamic_loss_scale .............. True........................updated
  eval_interval ................... 250.........................updated
  eval_iters ...................... 50..........................updated
  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}updated
  global_num_gpus ................. 16..........................updated
  gradient_accumulation_steps ..... 8...........................updated
  hidden_size ..................... 4096........................updated
  is_pipe_parallel ................ True........................updated
  load ............................ checkpoints-test2...........updated
  log_dir ......................... logs........................updated
  lr .............................. 0.0001......................updated
  lr_decay_iters .................. 12..........................updated
  lr_decay_style .................. cosine......................updated
  max_position_embeddings ......... 4096........................updated
  merge_file ...................... /pscratch/sd/z/zhaozh/data/pile/gpt2-merges.txtupdated
  mlp_multiple_of ................. 256.........................updated
  model_parallel_size ............. 2...........................updated
  no_weight_tying ................. True........................updated
  norm ............................ rmsnorm.....................updated
  num_attention_heads ............. 32..........................updated
  num_layers ...................... 32..........................updated
  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0001, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated
  optimizer_type .................. Adam........................updated
  partition_activations ........... True........................updated
  pipe_parallel_size .............. 4...........................updated
  pos_emb ......................... rotary......................updated
  precision ....................... fp16........................updated
  profile ......................... True........................updated
  profile_step_start .............. 2...........................updated
  profile_step_stop ............... 11..........................updated
  rms_norm_epsilon ................ 1e-05.......................updated
  save ............................ checkpoints-test............updated
  save_iters ...................... []..........................updated
  scaled_upper_triang_masked_softmax_fusion  True...............updated
  seq_length ...................... 4096........................updated
  sparsity_config ................. {}..........................updated
  steps_per_print ................. 100.........................updated
  synchronize_each_layer .......... True........................updated
  tensorboard_dir ................. tensorboard.................updated
  text_gen_type ................... unconditional...............updated
  train_batch_size ................ 64..........................updated
  train_iters ..................... 12..........................updated
  train_micro_batch_size_per_gpu .. 4...........................updated
  use_bias_in_attn_linear ......... False.......................updated
  use_bias_in_norms ............... False.......................updated
  use_wandb ....................... True........................updated
  user_script ..................... /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/train_profiling.pyupdated
  vocab_file ...................... /pscratch/sd/z/zhaozh/data/pile/gpt2-vocab.jsonupdated
  wandb_group ..................... fcx2yppk_gek2yuss...........updated
  zero_allgather_bucket_size ...... 1260000000..................updated
  zero_contiguous_gradients ....... True........................updated
  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 1260000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 1260000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
  zero_reduce_bucket_size ......... 1260000000..................updated
  zero_reduce_scatter ............. True........................updated
  zero_stage ...................... 1...........................updated
  account ......................... None........................default
  activation ...................... gelu........................default
  activation_checkpointing ........ None........................default
  adlr_autoresume ................. False.......................default
  adlr_autoresume_interval ........ 1000........................default
  allow_chopped ................... True........................default
  amp ............................. None........................default
  apply_query_key_layer_scaling ... False.......................default
  attention_dropout ............... 0...........................default
  attention_softmax_in_fp32 ....... False.......................default
  autotuning ...................... None........................default
  autotuning_run .................. None........................default
  base_shapes_file ................ None........................default
  bf16 ............................ None........................default
  bias_dropout_fusion ............. False.......................default
  bias_gelu_fusion ................ False.......................default
  char_level_ppl .................. False.......................default
  checkpoint ...................... None........................default
  checkpoint_in_cpu ............... False.......................default
  checkpoint_num_layers ........... 1...........................default
  checkpoint_scale ................ linear......................default
  checkpoint_validation_with_forward_pass  False................default
  clip_grad ....................... 1.0.........................default
  comet_experiment_name ........... None........................default
  comet_others .................... None........................default
  comet_project ................... None........................default
  comet_tags ...................... None........................default
  comet_workspace ................. None........................default
  comment ......................... None........................default
  comms_logger .................... None........................default
  communication_data_type ......... None........................default
  compression_training ............ None........................default
  contiguous_checkpointing ........ False.......................default
  coord_check ..................... False.......................default
  create_moe_param_group .......... True........................default
  csv_monitor ..................... None........................default
  curriculum_learning ............. None........................default
  curriculum_seqlen ............... 0...........................default
  data_efficiency ................. None........................default
  data_types ...................... None........................default
  dataset_impl .................... gpt2........................default
  deepscale ....................... False.......................default
  deepscale_config ................ None........................default
  deepspeed ....................... True........................default
  deepspeed_activation_checkpointing  True......................default
  deepspeed_extra_args ............ None........................default
  deepspeed_mpi ................... False.......................default
  deepspeed_slurm ................. False.......................default
  detect_nvlink_pairs ............. False.......................default
  distributed_backend ............. nccl........................default
  do_test ......................... None........................default
  do_train ........................ None........................default
  do_valid ........................ None........................default
  dpo_beta ........................ 0.1.........................default
  dpo_fp32 ........................ True........................default
  dpo_reference_free .............. False.......................default
  dump_state ...................... False.......................default
  elasticity ...................... None........................default
  enable_expert_tensor_parallelism  False.......................default
  eod_mask_loss ................... False.......................default
  eval_results_prefix ............. ............................default
  eval_tasks ...................... None........................default
  exclude ......................... None........................default
  exit_interval ................... None........................default
  expansion_factor ................ None........................default
  expert_interval ................. 2...........................default
  extra_save_iters ................ None........................default
  finetune ........................ False.......................default
  flops_profiler .................. None........................default
  force_multi ..................... False.......................default
  fp16_lm_cross_entropy ........... False.......................default
  fp32_allreduce .................. False.......................default
  git_hash ........................ d79c5331....................default
  gmlp_attn_dim ................... 64..........................default
  gpt_j_residual .................. False.......................default
  gpt_j_tied ...................... False.......................default
  gradient_clipping ............... 1.0.........................default
  gradient_noise_scale_cpu_offload  False.......................default
  gradient_noise_scale_n_batches .. 5...........................default
  gradient_predivide_factor ....... 1.0.........................default
  hidden_dropout .................. 0...........................default
  hostfile ........................ None........................default
  hysteresis ...................... 2...........................default
  include ......................... None........................default
  init_method ..................... normal......................default
  init_method_std ................. 0.02........................default
  intermediate_size ............... None........................default
  iteration ....................... None........................default
  keep_last_n_checkpoints ......... None........................default
  kto_beta ........................ 0.1.........................default
  kto_desirable_weight ............ 1.0.........................default
  kto_fp32 ........................ True........................default
  kto_undesirable_weight .......... 1.0.........................default
  launcher ........................ pdsh........................default
  layernorm_epsilon ............... 1e-05.......................default
  layernorm_fusion ................ False.......................default
  lazy_mpu_init ................... False.......................default
  local_rank ...................... None........................default
  log_grad_norm ................... False.......................default
  log_grad_pct_zeros .............. False.......................default
  log_gradient_noise_scale ........ False.......................default
  log_interval .................... 100.........................default
  log_optimizer_states ............ False.......................default
  log_param_norm .................. False.......................default
  loss_scale ...................... None........................default
  loss_scale_window ............... 1000.0......................default
  make_vocab_size_divisible_by .... 128.........................default
  mamba_causal_conv_fusion ........ False.......................default
  mamba_inner_func_fusion ......... False.......................default
  mamba_selective_fp32_params ..... True........................default
  mamba_selective_scan_fusion ..... False.......................default
  mamba_use_bias_in_conv .......... True........................default
  mamba_use_bias_in_linears ....... False.......................default
  master_addr ..................... None........................default
  master_port ..................... 29500.......................default
  maximum_tokens .................. 64..........................default
  memory_profiling ................ False.......................default
  memory_profiling_path ........... None........................default
  min_lr .......................... 0.0.........................default
  min_scale ....................... 1.0.........................default
  mmap_warmup ..................... False.......................default
  moe_eval_capacity_factor ........ 1.0.........................default
  moe_expert_parallel_size ........ 1...........................default
  moe_glu ......................... False.......................default
  moe_jitter_eps .................. None........................default
  moe_lbl_in_fp32 ................. False.......................default
  moe_loss_coeff .................. 0.1.........................default
  moe_min_capacity ................ 4...........................default
  moe_num_experts ................. 1...........................default
  moe_token_dropping .............. False.......................default
  moe_top_k ....................... 1...........................default
  moe_train_capacity_factor ....... 1.0.........................default
  moe_type ........................ megablocks..................default
  moe_use_residual ................ True........................default
  mup_attn_temp ................... 1.0.........................default
  mup_embedding_mult .............. 1.0.........................default
  mup_init_scale .................. 1.0.........................default
  mup_output_temp ................. 1.0.........................default
  mup_rp_embedding_mult ........... 1.0.........................default
  mup_width_scale ................. 2...........................default
  neg_test_data_paths ............. None........................default
  neg_test_label_data_paths ....... None........................default
  neg_train_data_paths ............ None........................default
  neg_train_label_data_paths ...... None........................default
  neg_valid_data_paths ............ None........................default
  neg_valid_label_data_paths ...... None........................default
  no_load_optim ................... False.......................default
  no_load_rng ..................... False.......................default
  no_save_optim ................... False.......................default
  no_save_rng ..................... False.......................default
  no_ssh_check .................... False.......................default
  num_gpus ........................ None........................default
  num_kv_heads .................... None........................default
  num_nodes ....................... -1..........................default
  num_samples ..................... 1...........................default
  num_unique_layers ............... None........................default
  num_workers ..................... 2...........................default
  onnx_safe ....................... False.......................default
  opt_pos_emb_offset .............. 0...........................default
  output_layer_init_method ........ scaled_normal...............default
  output_layer_parallelism ........ column......................default
  override_lr_scheduler ........... False.......................default
  pack_impl ....................... packed......................default
  padded_vocab_size ............... None........................default
  param_sharing_style ............. grouped.....................default
  pipe_partition_method ........... type:transformer|mlp........default
  pos_test_data_paths ............. None........................default
  pos_test_label_data_paths ....... None........................default
  pos_train_data_paths ............ None........................default
  pos_train_label_data_paths ...... None........................default
  pos_valid_data_paths ............ None........................default
  pos_valid_label_data_paths ...... None........................default
  precompute_model_name ........... None........................default
  prescale_gradients .............. False.......................default
  profile_backward ................ False.......................default
  prompt_end ...................... 
...........................default
  rank ............................ None........................default
  recompute ....................... False.......................default
  return_logits ................... False.......................default
  rmsnorm_fusion .................. False.......................default
  rope_fusion ..................... False.......................default
  rotary_emb_base ................. 10000.......................default
  rotary_pct ...................... 1...........................default
  rotary_save_freqs_buffer ........ False.......................default
  rpe_max_distance ................ 128.........................default
  rpe_num_buckets ................. 32..........................default
  s3_chunk_size ................... 104857600...................default
  s3_path ......................... None........................default
  sample_input_file ............... None........................default
  sample_output_file .............. samples.txt.................default
  save_base_shapes ................ False.......................default
  scaled_masked_softmax_fusion .... False.......................default
  scalenorm_epsilon ............... 1e-08.......................default
  scheduler ....................... None........................default
  seed ............................ 1234........................default
  sequence_parallel ............... False.......................default
  short_seq_prob .................. 0.1.........................default
  sliding_window_width ............ None........................default
  soft_prompt_tuning .............. None........................default
  sparse_attention ................ None........................default
  sparse_gradients ................ False.......................default
  split ........................... 969, 30, 1..................default
  temperature ..................... 0.0.........................default
  tensorboard ..................... None........................default
  test_data_paths ................. None........................default
  test_data_weights ............... None........................default
  test_label_data_paths ........... None........................default
  test_reward_data_paths .......... None........................default
  tokenizer_type .................. GPT2BPETokenizer............default
  top_k ........................... 0...........................default
  top_p ........................... 0.0.........................default
  train_data_paths ................ None........................default
  train_data_weights .............. None........................default
  train_impl ...................... normal......................default
  train_label_data_paths .......... None........................default
  train_reward_data_paths ......... None........................default
  use_bias_in_mlp ................. True........................default
  use_bnb_optimizer ............... False.......................default
  use_checkpoint_lr_scheduler ..... False.......................default
  use_comet ....................... None........................default
  use_cpu_initialization .......... False.......................default
  use_flashattn_swiglu ............ False.......................default
  use_mup ......................... False.......................default
  use_qk_layernorm ................ False.......................default
  use_shared_fs ................... True........................default
  use_tutel ....................... False.......................default
  valid_data_paths ................ None........................default
  valid_data_weights .............. None........................default
  valid_label_data_paths .......... None........................default
  valid_reward_data_paths ......... None........................default
  wall_clock_breakdown ............ False.......................default
  wandb ........................... None........................default
  wandb_host ...................... https://api.wandb.ai........default
  wandb_init_all_ranks ............ False.......................default
  wandb_project ................... neox........................default
  wandb_team ...................... None........................default
  warmup .......................... 0.01........................default
  weight_by_num_documents ......... False.......................default
  weight_decay .................... 0.1.........................default
  weighted_sampler_alpha .......... 1.0.........................default
  world_size ...................... None........................default
---------------- end of arguments ----------------
NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 2 
[2024-10-09 13:44:02,121] [INFO] [multinode_runner.py:73:get_cmd] Running on the following workers: nid001876,nid002436,nid003892,nid003893
[2024-10-09 13:44:02,121] [INFO] [runner.py:586:main] cmd = pdsh -S -f 1024 -w nid001876,nid002436,nid003892,nid003893 export PYTHONPATH=/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/slurms:/opt/nersc/pymon; export NCCL_NET_GDR_LEVEL=PHB; export NCCL_SOCKET_IFNAME=hsn;  cd /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/slurms; /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/bin/python -u -m deepspeed.launcher.launch --world_info=eyJuaWQwMDE4NzYiOiBbMCwgMSwgMiwgM10sICJuaWQwMDI0MzYiOiBbMCwgMSwgMiwgM10sICJuaWQwMDM4OTIiOiBbMCwgMSwgMiwgM10sICJuaWQwMDM4OTMiOiBbMCwgMSwgMiwgM119 --node_rank=%n --master_addr=nid001876 --master_port=29500 /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/train_profiling.py --deepspeed_config 'eyJ0cmFpbl9iYXRjaF9zaXplIjogNjQsICJ0cmFpbl9taWNyb19iYXRjaF9zaXplX3Blcl9ncHUiOiA0LCAiZ3JhZGllbnRfYWNjdW11bGF0aW9uX3N0ZXBzIjogOCwgIm9wdGltaXplciI6IHsidHlwZSI6ICJBZGFtIiwgInBhcmFtcyI6IHsibHIiOiAwLjAwMDEsICJiZXRhcyI6IFswLjksIDAuOTVdLCAiZXBzIjogMWUtMDh9fSwgImZwMTYiOiB7ImZwMTYiOiB0cnVlLCAiZW5hYmxlZCI6IHRydWUsICJsb3NzX3NjYWxlIjogMCwgImxvc3Nfc2NhbGVfd2luZG93IjogMTAwMCwgImh5c3RlcmVzaXMiOiAyLCAibWluX2xvc3Nfc2NhbGUiOiAxfSwgInplcm9fb3B0aW1pemF0aW9uIjogeyJzdGFnZSI6IDEsICJhbGxnYXRoZXJfcGFydGl0aW9ucyI6IHRydWUsICJhbGxnYXRoZXJfYnVja2V0X3NpemUiOiAxMjYwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogdHJ1ZSwgInJlZHVjZV9zY2F0dGVyIjogdHJ1ZSwgInJlZHVjZV9idWNrZXRfc2l6ZSI6IDEyNjAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IHRydWUsICJjcHVfb2ZmbG9hZCI6IGZhbHNlfSwgInN0ZXBzX3Blcl9wcmludCI6IDEwMH0=' --megatron_config '{"train_batch_size": 64, "train_micro_batch_size_per_gpu": 4, "gradient_accumulation_steps": 8, "optimizer": {"type": "Adam", "params": {"lr": 0.0001, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 1260000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 1260000000, "contiguous_gradients": true, "cpu_offload": false}, "steps_per_print": 100, "precision": "fp16", "num_layers": 32, "hidden_size": 4096, "mlp_multiple_of": 256, "num_attention_heads": 32, "seq_length": 4096, "max_position_embeddings": 4096, "norm": "rmsnorm", "rms_norm_epsilon": 1e-05, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash"], "sparsity_config": {}, "scaled_upper_triang_masked_softmax_fusion": true, "use_bias_in_norms": false, "use_bias_in_attn_linear": false, "lr_decay_style": "cosine", "lr_decay_iters": 12, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 1260000000, "zero_allgather_bucket_size": 1260000000, "lr": 0.0001, "data_path": "/pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document", "data_impl": "mmap", "save": "checkpoints-test", "config_files": {"llemma_7B_4_2_2.yml": "{\n  \"pipe_parallel_size\": 4,\n  \"model_parallel_size\": 2,\n  \"make_vocab_size_divisible_by\": 128,\n\n  # model settings\n  \"num_layers\": 32,\n  \"hidden_size\": 4096,\n  \"num_attention_heads\": 32,\n  # NB: These rotary embedding and sequence length parameters\n  # May differ from CodeLlama configs. They match what we used for\n  # Llemma continued pretraining. See https://arxiv.org/abs/2310.10631\n  # For detailed discussion\n  \"seq_length\": 4096,\n  \"max_position_embeddings\": 4096,\n  \"pos_emb\": \"rotary\",\n  \"rotary_pct\": 1,\n  \"rotary_emb_base\": 10000,\n  \"no_weight_tying\": true,\n  \"gpt_j_residual\": false,\n  \"output_layer_parallelism\": \"column\",\n  \"norm\": \"rmsnorm\",\n  \"rms_norm_epsilon\": 1.0e-5,\n\n  \"attention_config\": [[[\"flash\"], 32]],\n\n  \"scaled_upper_triang_masked_softmax_fusion\": true,\n  \"bias_gelu_fusion\": false,\n  \"use_bias_in_norms\": false,\n  \"use_bias_in_attn_linear\": false,\n  # \"activation\": \"swiglu\",\n  \"mlp_multiple_of\": 256,\n\n   \"optimizer\": {\n     \"type\": \"Adam\",\n     \"params\": {\n       \"lr\": 0.0001,\n       \"betas\": [0.9, 0.95],\n       \"eps\": 1.0e-8\n     }\n   },\n\n   \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": true,\n    \"allgather_bucket_size\": 1260000000,\n    \"overlap_comm\": true,\n    \"reduce_scatter\": true,\n    \"reduce_bucket_size\": 1260000000,\n    \"contiguous_gradients\": true,\n    \"cpu_offload\": false\n  },\n\n  # trained on 256 gpus\n  \"train_micro_batch_size_per_gpu\": 4,\n  \"gradient_accumulation_steps\": 8,\n  \"data_impl\": \"mmap\",\n\n  \"checkpoint_activations\": true,\n  \"checkpoint_num_layers\": 1,\n  \"partition_activations\": true,\n  \"synchronize_each_layer\": true,\n\n  \"gradient_clipping\": 1.0,\n  \"weight_decay\": 0.1,\n  \"hidden_dropout\": 0,\n  \"attention_dropout\": 0,\n\n  # \"precision\": \"bfloat16\",\n  # \"fp32_allreduce\": true,\n  # \"bf16\": {\n  #   \"enabled\": true\n  # },\n  # \"data_types\": {\n  #   \"grad_accum_dtype\": \"fp32\"\n  # },\n\n  # precision settings\n  \"fp16\": {\n    \"fp16\": true,\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n\n  \"train_iters\": 12,\n  \"lr_decay_iters\": 12,\n  \"distributed_backend\": \"nccl\",\n  \"lr_decay_style\": \"cosine\",\n  # \"decay_lr_to\": 0.033,\n  # \"warmup_iters\": 500,\n  \"checkpoint_factor\": 500,\n  \"eval_interval\": 250,\n  \"eval_iters\": 50,\n\n  \"log_interval\": 100,\n  \"steps_per_print\": 100,\n  \"wall_clock_breakdown\": false,\n\n  # \"tokenizer_type\": \"SPMTokenizer\",\n  # \"vocab-file\": \"codellama/tokenizer.model\", # use tokenizer.model from Meta CodeLlama download\n\n  # \"save\": \"/path/to/save/llema-replication\",\n  # #\"load\": \"\", # once run is started, to restart from intermediate ckpt use \"load\" = \"save\"\n  # \"load\": \"/path/to/converted/codellama_7b_weights_with_mp2\",\n\n  # \"finetune\": true, # set to false once resuming from intermediate finetuning step\n\n  # profiling settings\n  \"profile\": True,\n  \"profile_step_start\": 2,\n  \"profile_step_stop\": 11,\n\n}", "local_setup.yml": "# Suggested data paths when using GPT-NeoX locally\n{\n  # \"data_path\": \"/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/data/pile_text_document\",\n  # \"data_path\": \"/pscratch/sd/z/zhaozh/data/pile/processed/pile_text_document\",\n\n  \"data-path\": \"/pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document\",\n\n  # or for weighted datasets:\n  # \"train-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"test-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"valid-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"train-data-weights\": [1., 2.],\n  # \"test-data-weights\": [2., 1.],\n  # \"valid-data-weights\": [0.5, 0.4],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # \"weight_by_num_documents\": false,\n  # \"weighted_sampler_alpha\": 0.3,\n\n  \"vocab_file\": \"/pscratch/sd/z/zhaozh/data/pile/gpt2-vocab.json\",\n  \"merge_file\": \"/pscratch/sd/z/zhaozh/data/pile/gpt2-merges.txt\",\n\n  \"save\": \"checkpoints-test\",\n  \"load\": \"checkpoints-test2\",\n  \"checkpoint_validation_with_forward_pass\": False,\n\n  \"tensorboard_dir\": \"tensorboard\",\n  \"log_dir\": \"logs\",\n  \"use_wandb\": True,\n  \"wandb_host\": \"https://api.wandb.ai\",\n  \"wandb_project\": \"neox\"\n}\n"}, "load": "checkpoints-test2", "checkpoint_factor": 500, "batch_size": 4, "train_iters": 12, "eval_iters": 50, "eval_interval": 250, "vocab_file": "/pscratch/sd/z/zhaozh/data/pile/gpt2-vocab.json", "merge_file": "/pscratch/sd/z/zhaozh/data/pile/gpt2-merges.txt", "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "dynamic_loss_scale": true, "pipe_parallel_size": 4, "model_parallel_size": 2, "world_size": 1, "is_pipe_parallel": true, "use_wandb": true, "wandb_group": "fcx2yppk_gek2yuss", "log_dir": "logs", "tensorboard_dir": "tensorboard", "profile": true, "profile_step_start": 2, "profile_step_stop": 11, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/train_profiling.py", "save_iters": [], "global_num_gpus": 16}'
nid001876: ***************************************************************************
nid001876:                           NOTICE TO USERS
nid001876: 
nid001876: Lawrence Berkeley National Laboratory operates this computer system under 
nid001876: contract to the U.S. Department of Energy.  This computer system is the 
nid001876: property of the United States Government and is for authorized use only.
nid001876: Users (authorized or unauthorized) have no explicit or implicit 
nid001876: expectation of privacy.
nid001876: 
nid001876: Any or all uses of this system and all files on this system may be
nid001876: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001876: to authorized site, Department of Energy, and law enforcement personnel,
nid001876: as well as authorized officials of other agencies, both domestic and foreign.
nid001876: By using this system, the user consents to such interception, monitoring,
nid001876: recording, copying, auditing, inspection, and disclosure at the discretion
nid001876: of authorized site or Department of Energy personnel.
nid001876: 
nid001876: Unauthorized or improper use of this system may result in administrative
nid001876: disciplinary action and civil and criminal penalties. By continuing to use
nid001876: this system you indicate your awareness of and consent to these terms and
nid001876: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001876: stated in this warning.
nid001876: 
nid001876: *****************************************************************************
nid001876: 
nid001876: Login connection to host x1003c3s3b0n0:
nid001876: 
nid003892: ***************************************************************************
nid003892:                           NOTICE TO USERS
nid003892: 
nid003892: Lawrence Berkeley National Laboratory operates this computer system under 
nid003892: contract to the U.S. Department of Energy.  This computer system is the 
nid003892: property of the United States Government and is for authorized use only.
nid003892: Users (authorized or unauthorized) have no explicit or implicit 
nid003892: expectation of privacy.
nid003892: 
nid003892: Any or all uses of this system and all files on this system may be
nid003892: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid003892: to authorized site, Department of Energy, and law enforcement personnel,
nid003892: as well as authorized officials of other agencies, both domestic and foreign.
nid003892: By using this system, the user consents to such interception, monitoring,
nid003892: recording, copying, auditing, inspection, and disclosure at the discretion
nid003892: of authorized site or Department of Energy personnel.
nid003892: 
nid003892: Unauthorized or improper use of this system may result in administrative
nid003892: disciplinary action and civil and criminal penalties. By continuing to use
nid003892: this system you indicate your awareness of and consent to these terms and
nid003892: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid003892: stated in this warning.
nid003892: 
nid003892: *****************************************************************************
nid003892: 
nid003892: Login connection to host x1203c2s3b0n0:
nid003892: 
nid003893: ***************************************************************************
nid003893:                           NOTICE TO USERS
nid003893: 
nid003893: Lawrence Berkeley National Laboratory operates this computer system under 
nid003893: contract to the U.S. Department of Energy.  This computer system is the 
nid003893: property of the United States Government and is for authorized use only.
nid003893: Users (authorized or unauthorized) have no explicit or implicit 
nid003893: expectation of privacy.
nid003893: 
nid003893: Any or all uses of this system and all files on this system may be
nid003893: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid003893: to authorized site, Department of Energy, and law enforcement personnel,
nid003893: as well as authorized officials of other agencies, both domestic and foreign.
nid003893: By using this system, the user consents to such interception, monitoring,
nid003893: recording, copying, auditing, inspection, and disclosure at the discretion
nid003893: of authorized site or Department of Energy personnel.
nid003893: 
nid003893: Unauthorized or improper use of this system may result in administrative
nid003893: disciplinary action and civil and criminal penalties. By continuing to use
nid003893: this system you indicate your awareness of and consent to these terms and
nid003893: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid003893: stated in this warning.
nid003893: 
nid003893: *****************************************************************************
nid003893: 
nid003893: Login connection to host x1203c2s3b0n1:
nid003893: 
nid002436: ***************************************************************************
nid002436:                           NOTICE TO USERS
nid002436: 
nid002436: Lawrence Berkeley National Laboratory operates this computer system under 
nid002436: contract to the U.S. Department of Energy.  This computer system is the 
nid002436: property of the United States Government and is for authorized use only.
nid002436: Users (authorized or unauthorized) have no explicit or implicit 
nid002436: expectation of privacy.
nid002436: 
nid002436: Any or all uses of this system and all files on this system may be
nid002436: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid002436: to authorized site, Department of Energy, and law enforcement personnel,
nid002436: as well as authorized officials of other agencies, both domestic and foreign.
nid002436: By using this system, the user consents to such interception, monitoring,
nid002436: recording, copying, auditing, inspection, and disclosure at the discretion
nid002436: of authorized site or Department of Energy personnel.
nid002436: 
nid002436: Unauthorized or improper use of this system may result in administrative
nid002436: disciplinary action and civil and criminal penalties. By continuing to use
nid002436: this system you indicate your awareness of and consent to these terms and
nid002436: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid002436: stated in this warning.
nid002436: 
nid002436: *****************************************************************************
nid002436: 
nid002436: Login connection to host x1101c4s7b0n0:
nid002436: 
nid001876: [2024-10-09 13:44:25,407] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003892: [2024-10-09 13:44:26,337] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001876: [2024-10-09 13:44:27,483] [INFO] [launch.py:138:main] 0 NCCL_NET_GDR_LEVEL=PHB
nid001876: [2024-10-09 13:44:27,483] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=hsn
nid001876: [2024-10-09 13:44:27,483] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001876': [0, 1, 2, 3], 'nid002436': [0, 1, 2, 3], 'nid003892': [0, 1, 2, 3], 'nid003893': [0, 1, 2, 3]}
nid001876: [2024-10-09 13:44:27,483] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=0
nid001876: [2024-10-09 13:44:27,483] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001876': [0, 1, 2, 3], 'nid002436': [4, 5, 6, 7], 'nid003892': [8, 9, 10, 11], 'nid003893': [12, 13, 14, 15]})
nid001876: [2024-10-09 13:44:27,483] [INFO] [launch.py:163:main] dist_world_size=16
nid001876: [2024-10-09 13:44:27,483] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001876: [2024-10-09 13:44:29,389] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001876: [2024-10-09 13:44:29,390] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001876: [2024-10-09 13:44:29,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001876: [2024-10-09 13:44:29,398] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003892: [2024-10-09 13:44:30,282] [INFO] [launch.py:138:main] 2 NCCL_NET_GDR_LEVEL=PHB
nid003892: [2024-10-09 13:44:30,282] [INFO] [launch.py:138:main] 2 NCCL_SOCKET_IFNAME=hsn
nid003892: [2024-10-09 13:44:30,283] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001876': [0, 1, 2, 3], 'nid002436': [0, 1, 2, 3], 'nid003892': [0, 1, 2, 3], 'nid003893': [0, 1, 2, 3]}
nid003892: [2024-10-09 13:44:30,283] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=2
nid003892: [2024-10-09 13:44:30,283] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001876': [0, 1, 2, 3], 'nid002436': [4, 5, 6, 7], 'nid003892': [8, 9, 10, 11], 'nid003893': [12, 13, 14, 15]})
nid003892: [2024-10-09 13:44:30,283] [INFO] [launch.py:163:main] dist_world_size=16
nid003892: [2024-10-09 13:44:30,283] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid002436: [2024-10-09 13:44:31,068] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003893: [2024-10-09 13:44:31,465] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003892: [2024-10-09 13:44:32,322] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003892: [2024-10-09 13:44:32,346] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003892: [2024-10-09 13:44:32,349] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003892: [2024-10-09 13:44:32,353] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001876: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001876: 
nid001876: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001876: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001876: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001876: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001876: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001876: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001876: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001876: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001876: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001876: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001876: STAGE:2024-10-09 13:44:32 1718222:1718222 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 13:44:32 1718220:1718220 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001876: 
nid001876: STAGE:2024-10-09 13:44:32 1718221:1718221 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001876: STAGE:2024-10-09 13:44:32 1718219:1718219 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001876: STAGE:2024-10-09 13:44:33 1718219:1718219 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 13:44:33 1718220:1718220 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001876: STAGE:2024-10-09 13:44:33 1718221:1718221 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001876: 
nid001876: STAGE:2024-10-09 13:44:33 1718222:1718222 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001876: NeoXArgs.configure_distributed_args() using world size: 16 and model-parallel size: 2 
nid001876: > building GPT2BPETokenizer tokenizer ...
nid001876:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
nid001876: > setting up tensorboard ...
nid002436: [2024-10-09 13:44:35,021] [INFO] [launch.py:138:main] 1 NCCL_NET_GDR_LEVEL=PHB
nid002436: [2024-10-09 13:44:35,021] [INFO] [launch.py:138:main] 1 NCCL_SOCKET_IFNAME=hsn
nid002436: [2024-10-09 13:44:35,021] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001876': [0, 1, 2, 3], 'nid002436': [0, 1, 2, 3], 'nid003892': [0, 1, 2, 3], 'nid003893': [0, 1, 2, 3]}
nid002436: [2024-10-09 13:44:35,021] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=1
nid002436: [2024-10-09 13:44:35,021] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001876': [0, 1, 2, 3], 'nid002436': [4, 5, 6, 7], 'nid003892': [8, 9, 10, 11], 'nid003893': [12, 13, 14, 15]})
nid002436: [2024-10-09 13:44:35,021] [INFO] [launch.py:163:main] dist_world_size=16
nid002436: [2024-10-09 13:44:35,021] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid003893: [2024-10-09 13:44:35,446] [INFO] [launch.py:138:main] 3 NCCL_NET_GDR_LEVEL=PHB
nid003893: [2024-10-09 13:44:35,446] [INFO] [launch.py:138:main] 3 NCCL_SOCKET_IFNAME=hsn
nid003893: [2024-10-09 13:44:35,446] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001876': [0, 1, 2, 3], 'nid002436': [0, 1, 2, 3], 'nid003892': [0, 1, 2, 3], 'nid003893': [0, 1, 2, 3]}
nid003893: [2024-10-09 13:44:35,446] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=3
nid003893: [2024-10-09 13:44:35,446] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001876': [0, 1, 2, 3], 'nid002436': [4, 5, 6, 7], 'nid003892': [8, 9, 10, 11], 'nid003893': [12, 13, 14, 15]})
nid003893: [2024-10-09 13:44:35,446] [INFO] [launch.py:163:main] dist_world_size=16
nid003893: [2024-10-09 13:44:35,446] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid003892: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003892: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003892: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003892: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003892: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003892: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003892: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003892: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003892: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003892: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003892: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003892: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003892: STAGE:2024-10-09 13:44:36 1918351:1918351 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 13:44:36 1918354:1918354 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003892: 
nid003892: STAGE:2024-10-09 13:44:36 1918352:1918352 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 13:44:36 1918353:1918353 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003892: 
nid002436: [2024-10-09 13:44:37,022] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002436: [2024-10-09 13:44:37,070] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002436: [2024-10-09 13:44:37,075] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002436: [2024-10-09 13:44:37,077] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003892: STAGE:2024-10-09 13:44:37 1918354:1918354 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 13:44:37 1918352:1918352 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 13:44:37 1918353:1918353 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003892: 
nid003892: 
nid003892: STAGE:2024-10-09 13:44:37 1918351:1918351 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003893: [2024-10-09 13:44:37,496] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003893: [2024-10-09 13:44:37,499] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003893: [2024-10-09 13:44:37,506] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003893: [2024-10-09 13:44:37,507] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002436: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002436: 
nid002436: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002436: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002436: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002436: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002436: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002436: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002436: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002436: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002436: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002436: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002436: STAGE:2024-10-09 13:44:41 1846721:1846721 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 13:44:41 1846718:1846718 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002436: 
nid002436: STAGE:2024-10-09 13:44:41 1846720:1846720 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002436: STAGE:2024-10-09 13:44:41 1846719:1846719 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003893: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003893: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003893: 
nid003893: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003893: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003893: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003893: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003893: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003893: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003893: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003893: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003893: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003893: STAGE:2024-10-09 13:44:41 1161181:1161181 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 13:44:41 1161180:1161180 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 13:44:41 1161178:1161178 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003893: STAGE:2024-10-09 13:44:41 1161179:1161179 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003893: 
nid003893: 
nid002436: STAGE:2024-10-09 13:44:42 1846720:1846720 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002436: STAGE:2024-10-09 13:44:42 1846721:1846721 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002436: STAGE:2024-10-09 13:44:42 1846719:1846719 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002436: STAGE:2024-10-09 13:44:42 1846718:1846718 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003893: STAGE:2024-10-09 13:44:42 1161180:1161180 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003893: STAGE:2024-10-09 13:44:42 1161178:1161178 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003893: STAGE:2024-10-09 13:44:42 1161179:1161179 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003893: STAGE:2024-10-09 13:44:42 1161181:1161181 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001876: [2024-10-09 13:51:01,374] [INFO] [comm.py:637:init_distributed] cdb=None
nid003893: [2024-10-09 13:51:01,553] [INFO] [comm.py:637:init_distributed] cdb=None
nid003893: [2024-10-09 13:51:01,713] [INFO] [comm.py:637:init_distributed] cdb=None
nid003892: [2024-10-09 13:51:01,954] [INFO] [comm.py:637:init_distributed] cdb=None
nid001876: [2024-10-09 13:51:02,126] [INFO] [comm.py:637:init_distributed] cdb=None
nid001876: Loading extension module scaled_upper_triang_masked_softmax_cuda...
nid001876: Detected CUDA files, patching ldflags
nid001876: Emitting ninja build file /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/fused_kernels/build/build.ninja...
nid001876: Building extension module scaled_masked_softmax_cuda...
nid001876: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
nid001876: ninja: no work to do.
nid001876: Loading extension module scaled_masked_softmax_cuda...
nid001876: Detected CUDA files, patching ldflags
nid001876: Emitting ninja build file /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/fused_kernels/build/build.ninja...
nid001876: Building extension module fused_rotary_positional_embedding...
nid001876: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
nid001876: ninja: no work to do.
nid001876: Loading extension module fused_rotary_positional_embedding...
nid001876: > initializing torch distributed ...
nid001876: [2024-10-09 13:51:02,264] [INFO] [comm.py:637:init_distributed] cdb=None
nid001876: [2024-10-09 13:51:02,265] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
nid003893: [2024-10-09 13:51:02,428] [INFO] [comm.py:637:init_distributed] cdb=None
nid003892: [2024-10-09 13:51:02,573] [INFO] [comm.py:637:init_distributed] cdb=None
nid002436: [2024-10-09 13:51:02,777] [INFO] [comm.py:637:init_distributed] cdb=None
nid002436: [2024-10-09 13:51:02,878] [INFO] [comm.py:637:init_distributed] cdb=None
nid002436: [2024-10-09 13:51:02,909] [INFO] [comm.py:637:init_distributed] cdb=None
nid002436: [2024-10-09 13:51:03,069] [INFO] [comm.py:637:init_distributed] cdb=None
nid003893: [2024-10-09 13:51:03,114] [INFO] [comm.py:637:init_distributed] cdb=None
nid001876: [2024-10-09 13:51:03,114] [INFO] [comm.py:637:init_distributed] cdb=None
nid003892: [2024-10-09 13:51:03,119] [INFO] [comm.py:637:init_distributed] cdb=None
nid003892: [2024-10-09 13:51:03,195] [INFO] [comm.py:637:init_distributed] cdb=None
nid001876: > initializing model parallel with size 2
nid001876: MPU DP: [0, 2]
nid001876: MPU DP: [1, 3]
nid001876: MPU DP: [4, 6]
nid001876: MPU DP: [5, 7]
nid001876: MPU DP: [8, 10]
nid001876: MPU DP: [9, 11]
nid001876: MPU DP: [12, 14]
nid001876: MPU DP: [13, 15]
nid001876: MPU PP: [0, 4, 8, 12]
nid001876: MPU PP: [1, 5, 9, 13]
nid001876: MPU PP: [2, 6, 10, 14]
nid001876: MPU PP: [3, 7, 11, 15]
nid001876: MPU IO: [0, 2, 12, 14]
nid001876: MPU MP: [0, 1]
nid001876: MPU MP: [2, 3]
nid001876: MPU MP: [4, 5]
nid001876: MPU MP: [6, 7]
nid001876: MPU MP: [8, 9]
nid001876: MPU MP: [10, 11]
nid001876: MPU MP: [12, 13]
nid001876: MPU MP: [14, 15]
nid001876: > setting random seeds to 1234 ...
nid001876: [2024-10-09 13:51:06,855] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
nid001876: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid003892: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid003893: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002436: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001876: make: Nothing to be done for 'default'.
nid001876: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid003892: make: Nothing to be done for 'default'.
nid003892: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid003893: make: Nothing to be done for 'default'.
nid003893: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002436: make: Nothing to be done for 'default'.
nid002436: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001876: building GPT2 model ...
nid001876: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
nid001876: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=1, model=0): 2, ProcessCoord(pipe=0, data=1, model=1): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=0, model=1): 5, ProcessCoord(pipe=1, data=1, model=0): 6, ProcessCoord(pipe=1, data=1, model=1): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=0, model=1): 9, ProcessCoord(pipe=2, data=1, model=0): 10, ProcessCoord(pipe=2, data=1, model=1): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=0, model=1): 13, ProcessCoord(pipe=3, data=1, model=0): 14, ProcessCoord(pipe=3, data=1, model=1): 15}
nid001876: [2024-10-09 13:51:07,414] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp
nid001876: stage=0 layers=10
nid001876:      0: EmbeddingPipe
nid001876:      1: _pre_transformer_block
nid001876:      2: ParallelTransformerLayerPipe
nid001876:      3: ParallelTransformerLayerPipe
nid001876:      4: ParallelTransformerLayerPipe
nid001876:      5: ParallelTransformerLayerPipe
nid001876:      6: ParallelTransformerLayerPipe
nid001876:      7: ParallelTransformerLayerPipe
nid001876:      8: ParallelTransformerLayerPipe
nid001876:      9: ParallelTransformerLayerPipe
nid001876: stage=1 layers=9
nid001876:     10: ParallelTransformerLayerPipe
nid001876:     11: ParallelTransformerLayerPipe
nid001876:     12: ParallelTransformerLayerPipe
nid001876:     13: ParallelTransformerLayerPipe
nid001876:     14: ParallelTransformerLayerPipe
nid001876:     15: ParallelTransformerLayerPipe
nid001876:     16: ParallelTransformerLayerPipe
nid001876:     17: ParallelTransformerLayerPipe
nid001876:     18: ParallelTransformerLayerPipe
nid001876: stage=2 layers=9
nid001876:     19: ParallelTransformerLayerPipe
nid001876:     20: ParallelTransformerLayerPipe
nid001876:     21: ParallelTransformerLayerPipe
nid001876:     22: ParallelTransformerLayerPipe
nid001876:     23: ParallelTransformerLayerPipe
nid001876:     24: ParallelTransformerLayerPipe
nid001876:     25: ParallelTransformerLayerPipe
nid001876:     26: ParallelTransformerLayerPipe
nid001876:     27: ParallelTransformerLayerPipe
nid001876: stage=3 layers=9
nid001876:     28: ParallelTransformerLayerPipe
nid001876:     29: ParallelTransformerLayerPipe
nid001876:     30: ParallelTransformerLayerPipe
nid001876:     31: ParallelTransformerLayerPipe
nid001876:     32: ParallelTransformerLayerPipe
nid001876:     33: ParallelTransformerLayerPipe
nid001876:     34: _post_transformer_block
nid001876:     35: NormPipe
nid001876:     36: ParallelLinearPipe
nid001876:   loss: partial
nid003893: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001876: Configuring Optimizer type: Adam with params: {'lr': 0.0001, 'betas': [0.9, 0.95], 'eps': 1e-08}
nid001876: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003892: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002436: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003893: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003892: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001876: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002436: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003892: Detected CUDA files, patching ldflags
nid003892: Emitting ninja build file /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
nid003892: Building extension module fused_adam...
nid003892: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
nid003893: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003893: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001876: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003893: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001876: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003893: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003893: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003893: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002436: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002436: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001876: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001876: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002436: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001876: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002436: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001876: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002436: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002436: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003892: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003892: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003892: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003892: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003892: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003892: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003892: [1/3] /opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/TH -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/THC -isystem /opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++14 -c /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
nid003892: [2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/TH -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/THC -isystem /opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
nid003892: [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/lib64 -lcudart -o fused_adam.so
nid003892: Loading extension module fused_adam...
nid003893: Loading extension module fused_adam...
nid003893: Loading extension module fused_adam...
nid003893: Loading extension module fused_adam...
nid003892: Time to load fused_adam op: 26.223527193069458 seconds
nid003892: [2024-10-09 13:51:33,803] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid003893: Time to load fused_adam op: 24.867847204208374 secondsTime to load fused_adam op: 24.803593397140503 seconds
nid003893: 
nid003893: Time to load fused_adam op: 26.233412504196167 seconds
nid003893: [2024-10-09 13:51:33,807] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid003893: [2024-10-09 13:51:33,807] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid003893: [2024-10-09 13:51:33,807] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid003893: Loading extension module fused_adam...
nid003893: Time to load fused_adam op: 24.820487022399902 seconds
nid003893: [2024-10-09 13:51:33,817] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid003892: Loading extension module fused_adam...
nid003892: Loading extension module fused_adam...
nid003892: Loading extension module fused_adam...
nid003892: Time to load fused_adam op: 24.015321969985962 seconds
nid003892: Time to load fused_adam op: 24.166511058807373 seconds
nid003892: Time to load fused_adam op: 24.013022661209106 seconds
nid003892: [2024-10-09 13:51:33,886] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid003892: [2024-10-09 13:51:33,886] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid003892: [2024-10-09 13:51:33,886] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid001876: Loading extension module fused_adam...
nid001876: Loading extension module fused_adam...
nid001876: Loading extension module fused_adam...
nid001876: Loading extension module fused_adam...
nid002436: Loading extension module fused_adam...
nid002436: Loading extension module fused_adam...
nid002436: Loading extension module fused_adam...
nid002436: Loading extension module fused_adam...
nid001876: Time to load fused_adam op: 24.901007652282715 seconds
nid001876: Time to load fused_adam op: 24.85426092147827 seconds
nid001876: Time to load fused_adam op: 24.87385392189026 seconds
nid001876: Time to load fused_adam op: 26.315818309783936 seconds
nid001876: > learning rate decay style: cosine
nid001876: DeepSpeed is enabled.
nid001876: [2024-10-09 13:51:33,895] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4+02e2ebf, git-hash=02e2ebf, git-branch=HEAD
nid001876: [2024-10-09 13:51:33,895] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid001876: [2024-10-09 13:51:33,895] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid001876: [2024-10-09 13:51:33,895] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid001876: [2024-10-09 13:51:33,896] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid002436: Time to load fused_adam op: 24.88912057876587 seconds
nid002436: Time to load fused_adam op: 24.852355241775513 secondsTime to load fused_adam op: 24.86052656173706 seconds
nid002436: 
nid002436: Time to load fused_adam op: 26.318763732910156 seconds
nid002436: [2024-10-09 13:51:33,900] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid002436: [2024-10-09 13:51:33,900] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid002436: [2024-10-09 13:51:33,900] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid002436: [2024-10-09 13:51:33,900] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
nid001876: [2024-10-09 13:51:34,375] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
nid001876: [2024-10-09 13:51:34,376] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
nid001876: [2024-10-09 13:51:34,376] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
nid001876: [2024-10-09 13:51:34,376] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
nid001876: [2024-10-09 13:51:34,376] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
nid001876: [2024-10-09 13:51:34,376] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
nid001876: [2024-10-09 13:51:34,377] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 1260000000
nid001876: [2024-10-09 13:51:34,377] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 1260000000
nid001876: [2024-10-09 13:51:34,377] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
nid001876: [2024-10-09 13:51:34,377] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
nid003892: [2024-10-09 13:51:34,906] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003893: [2024-10-09 13:51:35,000] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003893: [2024-10-09 13:51:35,032] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003893: [2024-10-09 13:51:35,089] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003893: [2024-10-09 13:51:35,107] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003892: [2024-10-09 13:51:35,489] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003892: [2024-10-09 13:51:35,502] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003892: [2024-10-09 13:51:35,511] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002436: [2024-10-09 13:51:35,741] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002436: [2024-10-09 13:51:35,752] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002436: [2024-10-09 13:51:35,760] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002436: [2024-10-09 13:51:35,765] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001876: [2024-10-09 13:51:35,879] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001876: [2024-10-09 13:51:35,907] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001876: [2024-10-09 13:51:35,925] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001876: [2024-10-09 13:51:35,983] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
nid001876: [2024-10-09 13:51:35,984] [INFO] [utils.py:803:see_memory_usage] MA 3.39 GB         Max_MA 3.39 GB         CA 3.39 GB         Max_CA 3 GB 
nid001876: [2024-10-09 13:51:35,984] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 34.72 GB, percent = 13.8%
nid001876: [2024-10-09 13:51:36,072] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
nid001876: [2024-10-09 13:51:36,073] [INFO] [utils.py:803:see_memory_usage] MA 6.77 GB         Max_MA 8.47 GB         CA 8.47 GB         Max_CA 8 GB 
nid001876: [2024-10-09 13:51:36,073] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 34.73 GB, percent = 13.8%
nid001876: [2024-10-09 13:51:36,073] [INFO] [stage_1_and_2.py:517:__init__] optimizer state initialized
nid001876: [2024-10-09 13:51:36,119] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
nid001876: [2024-10-09 13:51:36,120] [INFO] [utils.py:803:see_memory_usage] MA 6.77 GB         Max_MA 6.77 GB         CA 8.47 GB         Max_CA 8 GB 
nid001876: [2024-10-09 13:51:36,120] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 34.77 GB, percent = 13.8%
nid001876: [2024-10-09 13:51:36,121] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
nid001876: [2024-10-09 13:51:36,121] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
nid001876: [2024-10-09 13:51:36,121] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f5533b6ee50>
nid001876: [2024-10-09 13:51:36,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001876: [2024-10-09 13:51:36,121] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
nid001876: [2024-10-09 13:51:36,121] [INFO] [config.py:983:print]   activation_checkpointing_config  {
nid001876:     "partition_activations": false, 
nid001876:     "contiguous_memory_optimization": false, 
nid001876:     "cpu_checkpointing": false, 
nid001876:     "number_checkpoints": null, 
nid001876:     "synchronize_checkpoint_boundary": false, 
nid001876:     "profile": false
nid001876: }
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   amp_enabled .................. False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   amp_params ................... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   autotuning_config ............ {
nid001876:     "enabled": false, 
nid001876:     "start_step": null, 
nid001876:     "end_step": null, 
nid001876:     "metric_path": null, 
nid001876:     "arg_mappings": null, 
nid001876:     "metric": "throughput", 
nid001876:     "model_info": null, 
nid001876:     "results_dir": "autotuning_results", 
nid001876:     "exps_dir": "autotuning_exps", 
nid001876:     "overwrite": true, 
nid001876:     "fast": true, 
nid001876:     "start_profile_step": 3, 
nid001876:     "end_profile_step": 5, 
nid001876:     "tuner_type": "gridsearch", 
nid001876:     "tuner_early_stopping": 5, 
nid001876:     "tuner_num_trials": 50, 
nid001876:     "model_info_path": null, 
nid001876:     "mp_size": 1, 
nid001876:     "max_train_batch_size": null, 
nid001876:     "min_train_batch_size": 1, 
nid001876:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
nid001876:     "min_train_micro_batch_size_per_gpu": 1, 
nid001876:     "num_tuning_micro_batch_sizes": 3
nid001876: }
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f55310a28b0>
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   communication_data_type ...... None
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   disable_allgather ............ False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   dump_state ................... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   elasticity_enabled ........... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   flops_profiler_config ........ {
nid001876:     "enabled": false, 
nid001876:     "recompute_fwd_factor": 0.0, 
nid001876:     "profile_step": 1, 
nid001876:     "module_depth": -1, 
nid001876:     "top_modules": 1, 
nid001876:     "detailed": true, 
nid001876:     "output_file": null
nid001876: }
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   fp16_enabled ................. True
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   global_rank .................. 0
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 8
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   gradient_clipping ............ 0.0
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
nid001876: [2024-10-09 13:51:36,122] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 65536
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   loss_scale ................... 0
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   memory_breakdown ............. False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   mics_shard_size .............. -1
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   nebula_config ................ {
nid001876:     "enabled": false, 
nid001876:     "persistent_storage_path": null, 
nid001876:     "persistent_time_interval": 100, 
nid001876:     "num_of_version_in_retention": 2, 
nid001876:     "enable_nebula_load": true, 
nid001876:     "load_path": null
nid001876: }
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   optimizer_name ............... adam
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.95], 'eps': 1e-08}
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   pld_enabled .................. False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   pld_params ................... False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   prescale_gradients ........... False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   scheduler_name ............... None
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   scheduler_params ............. None
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   sparse_attention ............. None
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   steps_per_print .............. 100
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   train_batch_size ............. 64
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  4
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   use_node_local_storage ....... False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   weight_quantization_config ... None
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   world_size ................... 2
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1260000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1260000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   zero_enabled ................. True
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:983:print]   zero_optimization_stage ...... 1
nid001876: [2024-10-09 13:51:36,123] [INFO] [config.py:969:print_user_config]   json = {
nid001876:     "train_batch_size": 64, 
nid001876:     "train_micro_batch_size_per_gpu": 4, 
nid001876:     "gradient_accumulation_steps": 8, 
nid001876:     "optimizer": {
nid001876:         "type": "Adam", 
nid001876:         "params": {
nid001876:             "lr": 0.0001, 
nid001876:             "betas": [0.9, 0.95], 
nid001876:             "eps": 1e-08
nid001876:         }
nid001876:     }, 
nid001876:     "fp16": {
nid001876:         "fp16": true, 
nid001876:         "enabled": true, 
nid001876:         "loss_scale": 0, 
nid001876:         "loss_scale_window": 1000, 
nid001876:         "hysteresis": 2, 
nid001876:         "min_loss_scale": 1
nid001876:     }, 
nid001876:     "zero_optimization": {
nid001876:         "stage": 1, 
nid001876:         "allgather_partitions": true, 
nid001876:         "allgather_bucket_size": 1.260000e+09, 
nid001876:         "overlap_comm": true, 
nid001876:         "reduce_scatter": true, 
nid001876:         "reduce_bucket_size": 1.260000e+09, 
nid001876:         "contiguous_gradients": true, 
nid001876:         "cpu_offload": false
nid001876:     }, 
nid001876:     "steps_per_print": 100
nid001876: }
nid001876: [2024-10-09 13:51:36,123] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=8 micro_batch_size=4
nid001876: [2024-10-09 13:51:36,124] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001876: [2024-10-09 13:51:36,713] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=10 [0, 10) STAGE_PARAMS=908754944 (908.755M) TOTAL_PARAMS=6856908800 (6856.909M) UNIQUE_PARAMS=6856908800 (6856.909M)
nid001876: [2024-10-09 13:51:36,713] [INFO] [engine.py:158:__init__] RANK=1 STAGE=0 LAYERS=10 [0, 10) STAGE_PARAMS=908754944 (908.755M) TOTAL_PARAMS=6856908800 (6856.909M) UNIQUE_PARAMS=6856908800 (6856.909M)
nid003892: [2024-10-09 13:51:36,713] [INFO] [engine.py:158:__init__] RANK=9 STAGE=2 LAYERS=9 [19, 28) STAGE_PARAMS=906153984 (906.154M) TOTAL_PARAMS=6856908800 (6856.909M) UNIQUE_PARAMS=6856908800 (6856.909M)
nid002436: [2024-10-09 13:51:36,713] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=9 [10, 19) STAGE_PARAMS=906153984 (906.154M) TOTAL_PARAMS=6856908800 (6856.909M) UNIQUE_PARAMS=6856908800 (6856.909M)
nid002436: [2024-10-09 13:51:36,713] [INFO] [engine.py:158:__init__] RANK=5 STAGE=1 LAYERS=9 [10, 19) STAGE_PARAMS=906153984 (906.154M) TOTAL_PARAMS=6856908800 (6856.909M) UNIQUE_PARAMS=6856908800 (6856.909M)
nid003892: [2024-10-09 13:51:36,713] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=9 [19, 28) STAGE_PARAMS=906153984 (906.154M) TOTAL_PARAMS=6856908800 (6856.909M) UNIQUE_PARAMS=6856908800 (6856.909M)
nid003893: [2024-10-09 13:51:36,713] [INFO] [engine.py:158:__init__] RANK=13 STAGE=3 LAYERS=9 [28, 37) STAGE_PARAMS=707391488 (707.391M) TOTAL_PARAMS=6856908800 (6856.909M) UNIQUE_PARAMS=6856908800 (6856.909M)
nid003893: [2024-10-09 13:51:36,713] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=9 [28, 37) STAGE_PARAMS=707391488 (707.391M) TOTAL_PARAMS=6856908800 (6856.909M) UNIQUE_PARAMS=6856908800 (6856.909M)
nid003893:  > number of parameters on model parallel rank 0: 707391488
nid001876:  > number of parameters on model parallel rank 0: 908754944
nid003893:  > number of parameters on model parallel rank 1: 707391488
nid001876:  > number of parameters on model parallel rank 1: 908754944
nid002436:  > number of parameters on model parallel rank 0: 906153984
nid003892:  > number of parameters on model parallel rank 0: 906153984
nid002436:  > number of parameters on model parallel rank 1: 906153984
nid003892:  > number of parameters on model parallel rank 1: 906153984
nid001876:  > total params: 6,856,908,800
nid001876: > building train, validation, and test datasets ...
nid001876:     reading sizes...
nid001876:     reading pointers...
nid001876:     reading document index...
nid001876:     creating numpy buffer of mmap...
nid001876:     creating memory view of numpy buffer...
nid001876:  > dataset split:
nid001876:     train:
nid001876:      document indices in [0, 204076229) total of 204076229 documents
nid001876:     validation:
nid001876:      document indices in [204076229, 210394379) total of 6318150 documents
nid001876:     test:
nid001876:      document indices in [210394379, 210604984) total of 210605 documents
nid001876:  > WARNING: could not find index map files, building the indices on rank 0 ...
nid001876:  > elapsed time to build and save doc-idx mapping (seconds): 12.304420
nid001876:     using:
nid001876:      number of documents:       204076229
nid001876:      number of epochs:          1
nid001876:      sequence length:           4096
nid001876:      total number of samples:   88562310
nid001876:  > elapsed time to build and save sample-idx mapping (seconds): 3.002254
nid001876:  > elapsed time to build and save shuffle-idx mapping (seconds): 4.463915
nid001876:  > loading doc-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_train_indexmap_768ns_4096sl_1234s_packedpi_ac_doc_idx.npy
nid001876:  > loading sample-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_train_indexmap_768ns_4096sl_1234s_packedpi_ac_sample_idx.npy
nid001876:  > loading shuffle-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_train_indexmap_768ns_4096sl_1234s_packedpi_ac_shuffle_idx.npy
nid001876:     loaded indexed file in 0.008 seconds
nid001876:     total number of samples: 88562311
nid001876:     total number of epochs: 1
nid001876:  > WARNING: could not find index map files, building the indices on rank 0 ...
nid001876:  > elapsed time to build and save doc-idx mapping (seconds): 0.243151
nid001876:     using:
nid001876:      number of documents:       6318150
nid001876:      number of epochs:          1
nid001876:      sequence length:           4096
nid001876:      total number of samples:   2738760
nid001876:  > elapsed time to build and save sample-idx mapping (seconds): 0.084511
nid001876:  > elapsed time to build and save shuffle-idx mapping (seconds): 0.072320
nid001876:  > loading doc-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_valid_indexmap_3200ns_4096sl_1234s_packedpi_ac_doc_idx.npy
nid001876:  > loading sample-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_valid_indexmap_3200ns_4096sl_1234s_packedpi_ac_sample_idx.npy
nid001876:  > loading shuffle-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_valid_indexmap_3200ns_4096sl_1234s_packedpi_ac_shuffle_idx.npy
nid001876:     loaded indexed file in 0.006 seconds
nid001876:     total number of samples: 2738761
nid001876:     total number of epochs: 1
nid001876:  > WARNING: could not find index map files, building the indices on rank 0 ...
nid001876:  > elapsed time to build and save doc-idx mapping (seconds): 0.007400
nid001876:     using:
nid001876:      number of documents:       210605
nid001876:      number of epochs:          1
nid001876:      sequence length:           4096
nid001876:      total number of samples:   89890
nid001876:  > elapsed time to build and save sample-idx mapping (seconds): 0.003118
nid001876:  > elapsed time to build and save shuffle-idx mapping (seconds): 0.003486
nid001876:  > loading doc-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_test_indexmap_3200ns_4096sl_1234s_packedpi_ac_doc_idx.npy
nid001876:  > loading sample-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_test_indexmap_3200ns_4096sl_1234s_packedpi_ac_sample_idx.npy
nid001876:  > loading shuffle-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_test_indexmap_3200ns_4096sl_1234s_packedpi_ac_shuffle_idx.npy
nid001876:     loaded indexed file in 0.003 seconds
nid001876:     total number of samples: 89891
nid001876:     total number of epochs: 1
nid001876: setting training data start iteration to 0
nid001876: setting validation data start iteration to 0
nid001876: done with setups ...
nid001876: time (ms) | model and optimizer: 31203.60 | train/valid/test data iterators: 23688.64
nid001876: training ...
nid001876: [2024-10-09 13:52:02,380] [INFO] [checkpointing.py:540:forward] Activation Checkpointing Information
nid001876: [2024-10-09 13:52:02,380] [INFO] [checkpointing.py:541:forward] ----Partition Activations True, CPU CHECKPOINTING False
nid001876: [2024-10-09 13:52:02,380] [INFO] [checkpointing.py:542:forward] ----contiguous Memory Checkpointing False with 32 total layers
nid001876: [2024-10-09 13:52:02,380] [INFO] [checkpointing.py:544:forward] ----Synchronization True
nid001876: [2024-10-09 13:52:02,380] [INFO] [checkpointing.py:545:forward] ----Profiling time in checkpointing False
nid003893: STAGE:2024-10-09 13:52:37 1161181:1161181 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001876: STAGE:2024-10-09 13:52:37 1718219:1718219 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002436: STAGE:2024-10-09 13:52:37 1846718:1846718 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003893: [W CPUAllocator.cpp:231] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event
nid001876: [2024-10-09 13:52:46,970] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
nid001876: [2024-10-09 13:52:56,552] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
nid001876: [2024-10-09 13:53:06,140] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
nid003893: STAGE:2024-10-09 13:54:04 1161181:1161181 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001876: STAGE:2024-10-09 13:54:05 1718219:1718219 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002436: STAGE:2024-10-09 13:54:05 1846718:1846718 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003893: STAGE:2024-10-09 13:55:03 1161181:1161181 output_json.cpp:417] Completed Stage: Post Processing
nid001876: STAGE:2024-10-09 13:55:15 1718219:1718219 output_json.cpp:417] Completed Stage: Post Processing
nid002436: STAGE:2024-10-09 13:55:22 1846718:1846718 output_json.cpp:417] Completed Stage: Post Processing
nid003892: Connection to nid003892 closed by remote host.
nid002436: Connection to nid002436 closed by remote host.
nid003893: Connection to nid003893 closed by remote host.
nid001876: Connection to nid001876 closed by remote host.
pdsh@nid001876: nid003892: ssh exited with exit code 255
pdsh@nid001876: nid002436: ssh exited with exit code 255
pdsh@nid001876: nid001876: ssh exited with exit code 255
pdsh@nid001876: nid003893: ssh exited with exit code 255
