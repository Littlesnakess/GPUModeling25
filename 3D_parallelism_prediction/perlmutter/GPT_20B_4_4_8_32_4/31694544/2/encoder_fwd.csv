cpu_op_0,cpu_op_0_id,cpu_op_0_input_dim,cpu_op_1,cpu_op_1_id,cpu_op_1_input_dim,kernel,kernel_id,kernel_overhead(us),kernel_dur(us)
aten::to,248,"[[], [], [], [], [], []]",aten::copy_,251,"[[], [], []]",Memcpy HtoD (Pageable -> Device),320252,0,1.0
aten::layer_norm,255,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,256,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",320274,16916.666666666668,190.2888888888889
aten::layer_norm,262,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,263,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",320296,386.6666666666667,190.725
aten::linear,272,"[[2048, 4, 6144], [4608, 6144], [4608]]",aten::addmm,277,"[[4608], [8192, 6144], [6144, 4608], [], []]",ampere_fp16_s16816gemm_fp16_128x256_ldg8_relu_f2f_stages_64x3_tn,320315,1996.6666666666667,1793.713888888889
aten::to,299,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,302,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),320333,1153.3333333333333,5.001388888888889
aten::to,303,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,306,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),320343,9766.666666666666,5.001388888888889
None,None,None,None,None,None,kernel_0,320356,22597.0,14.944444444444445
None,None,None,None,None,None,kernel_1,320368,3334.0,12.811111111111112
None,None,None,None,None,None,kernel_0,320380,7358.666666666667,16.29861111111111
None,None,None,None,None,None,kernel_1,320392,1614.0,12.352777777777778
aten::cat,348,"[[], []]",aten::cat,348,"[[], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<c10::Half, unsigned int, 4, 64, 64>(c10::Half*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<c10::Half, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",320400,6642.333333333333,104.75555555555556
aten::cat,349,"[[], []]",aten::cat,349,"[[], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<c10::Half, unsigned int, 4, 64, 64>(c10::Half*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<c10::Half, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",320409,391.3333333333333,105.3875
aten::baddbmm,359,"[[64, 2048, 2048], [64, 2048, 96], [64, 96, 2048], [], []]",aten::baddbmm,359,"[[64, 2048, 2048], [64, 2048, 96], [64, 96, 2048], [], []]",ampere_fp16_s1688gemm_fp16_256x128_ldg8_f2f_stages_32x1_tn,320427,398.0,663.0847222222222
ScaledUpperTriangMaskedSoftmax,362,"[[64, 2048, 2048]]",ScaledUpperTriangMaskedSoftmax,362,"[[64, 2048, 2048]]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_forward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half const*, float, int, int, int)",320436,398.3333333333333,678.9819444444445
aten::bmm,379,"[[64, 2048, 2048], [64, 2048, 96]]",aten::bmm,379,"[[64, 2048, 2048], [64, 2048, 96]]",ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_nn,320482,394.0,494.6666666666667
aten::contiguous,383,"[[2048, 4, 16, 96], []]",aten::copy_,387,"[[2048, 4, 16, 96], [2048, 4, 16, 96], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",320493,399.3333333333333,62.548611111111114
aten::linear,389,"[[2048, 4, 1536], [6144, 1536], []]",aten::mm,396,"[[8192, 1536], [1536, 6144]]",ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_tn,320510,1550.0,644.1430555555555
aten::add,401,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,401,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",320521,409.3333333333333,224.19583333333333
aten::linear,406,"[[2048, 4, 6144], [6144, 6144], []]",aten::mm,413,"[[8192, 6144], [6144, 6144]]",void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8>(cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8::Params),320534,404.3333333333333,2297.4125
None,None,None,None,None,None,kernel_2,320547,398.0,169.95277777777778
aten::linear,421,"[[2048, 4, 6144], [6144, 6144], []]",aten::mm,428,"[[8192, 6144], [6144, 6144]]",void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8>(cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8::Params),320559,395.6666666666667,2294.738888888889
aten::add,433,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,433,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",320567,392.3333333333333,220.68333333333334
aten::add,435,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,435,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3>)",320575,386.0,223.55694444444444
_ReduceFromModelParallelRegion,436,"[[2048, 4, 6144]]",nccl:all_reduce,439,"[[2048, 4, 6144]]","ncclKernel_AllReduce_RING_LL_Sum_half(ncclDevComm*, unsigned long, ncclWork*)",320593,511.0,765.6194444444444
aten::add,443,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,443,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3>)",320632,478.0,223.9986111111111
