cpu_op_0,cpu_op_0_id,cpu_op_0_input_dim,cpu_op_1,cpu_op_1_id,cpu_op_1_input_dim,kernel,kernel_id,kernel_overhead(us),kernel_dur(us)
aten::to,122035,"[[], [], [], [], [], []]",aten::copy_,122038,"[[], [], []]",Memcpy HtoD (Pageable -> Device),448108,0,1.0
aten::layer_norm,122042,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,122043,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",448130,5639.666666666667,168.9125
aten::layer_norm,122049,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,122050,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",448152,132.0,169.3
aten::linear,122059,"[[2048, 4, 6144], [4608, 6144], [4608]]",aten::addmm,122064,"[[4608], [8192, 6144], [6144, 4608], [], []]",ampere_fp16_s16816gemm_fp16_128x256_ldg8_relu_f2f_stages_64x3_tn,448171,567.6666666666666,1777.4166666666667
aten::to,122086,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,122089,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),448189,420.0,5.0
aten::_to_copy,122091,"[[2048, 1, 1, 24], [], [], [], [], [], []]",aten::copy_,122093,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),448199,3092.0,5.0
None,None,None,None,None,None,kernel_0,448212,7444.333333333333,13.191666666666666
None,None,None,None,None,None,kernel_1,448224,1085.0,12.0125
None,None,None,None,None,None,kernel_0,448236,2481.6666666666665,14.075
None,None,None,None,None,None,kernel_1,448248,903.0,12.0
aten::cat,122135,"[[], []]",aten::cat,122135,"[[], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<c10::Half, unsigned int, 4, 64, 64>(c10::Half*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<c10::Half, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",448256,2218.0,103.225
aten::cat,122136,"[[], []]",aten::cat,122136,"[[], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<c10::Half, unsigned int, 4, 64, 64>(c10::Half*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<c10::Half, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",448265,128.0,103.72083333333333
aten::baddbmm,122146,"[[64, 2048, 2048], [64, 2048, 96], [64, 96, 2048], [], []]",aten::baddbmm,122146,"[[64, 2048, 2048], [64, 2048, 96], [64, 96, 2048], [], []]",ampere_fp16_s1688gemm_fp16_256x128_ldg8_f2f_stages_32x1_tn,448283,130.0,641.4416666666667
ScaledUpperTriangMaskedSoftmax,122149,"[[64, 2048, 2048]]",ScaledUpperTriangMaskedSoftmax,122149,"[[64, 2048, 2048]]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_forward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half const*, float, int, int, int)",448292,132.33333333333334,602.6416666666667
aten::bmm,122166,"[[64, 2048, 2048], [64, 2048, 96]]",aten::bmm,122166,"[[64, 2048, 2048], [64, 2048, 96]]",ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_nn,448338,133.33333333333334,403.85833333333335
aten::contiguous,122170,"[[2048, 4, 16, 96], []]",aten::copy_,122174,"[[2048, 4, 16, 96], [2048, 4, 16, 96], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",448349,136.33333333333334,60.95
aten::linear,122176,"[[2048, 4, 1536], [6144, 1536], []]",aten::mm,122183,"[[8192, 1536], [1536, 6144]]",ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_tn,448366,504.6666666666667,634.0666666666667
aten::add,122188,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,122188,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",448377,128.33333333333334,215.24583333333334
aten::linear,122193,"[[2048, 4, 6144], [6144, 6144], []]",aten::mm,122200,"[[8192, 6144], [6144, 6144]]",void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8>(cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8::Params),448390,133.0,2287.358333333333
None,None,None,None,None,None,kernel_2,448403,132.0,160.28333333333333
aten::linear,122208,"[[2048, 4, 6144], [6144, 6144], []]",aten::mm,122215,"[[8192, 6144], [6144, 6144]]",void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8>(cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8::Params),448415,131.66666666666666,2287.108333333333
aten::add,122220,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,122220,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",448423,135.0,212.69166666666666
aten::add,122222,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,122222,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3>)",448431,126.0,173.24166666666667
_ReduceFromModelParallelRegion,122223,"[[2048, 4, 6144]]",nccl:all_reduce,122226,"[[2048, 4, 6144]]","ncclKernel_AllReduce_RING_LL_Sum_half(ncclDevComm*, unsigned long, ncclWork*)",448449,176.66666666666666,756.0791666666667
aten::add,122230,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,122230,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3>)",448488,166.0,173.65416666666667
