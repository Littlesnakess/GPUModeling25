cpu_op_0,cpu_op_0_id,cpu_op_0_input_dim,cpu_op_1,cpu_op_1_id,cpu_op_1_input_dim,kernel,kernel_id,kernel_overhead(us),kernel_dur(us)
aten::to,126,"[[], [], [], [], [], []]",aten::copy_,129,"[[], [], []]",Memcpy HtoD (Pageable -> Device),215732,0,1.0
aten::layer_norm,133,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,134,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",215754,11127.666666666666,168.71875
aten::layer_norm,140,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,141,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",215776,241.0,169.4
aten::linear,150,"[[2048, 4, 6144], [4608, 6144], [4608]]",aten::addmm,155,"[[4608], [8192, 6144], [6144, 4608], [], []]",ampere_fp16_s16816gemm_fp16_128x256_ldg8_relu_f2f_stages_64x3_tn,215795,1253.0,1782.1666666666667
aten::to,177,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,180,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),215813,780.0,5.002083333333333
aten::to,181,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,184,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),215823,6188.666666666667,5.002083333333333
None,None,None,None,None,None,kernel_0,215836,14815.333333333334,13.264583333333333
None,None,None,None,None,None,kernel_1,215848,2260.3333333333335,12.020833333333334
None,None,None,None,None,None,kernel_0,215860,4903.0,14.127083333333333
None,None,None,None,None,None,kernel_1,215872,1260.0,12.002083333333333
aten::cat,226,"[[], []]",aten::cat,226,"[[], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<c10::Half, unsigned int, 4, 64, 64>(c10::Half*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<c10::Half, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",215880,4243.0,103.26666666666667
aten::cat,227,"[[], []]",aten::cat,227,"[[], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<c10::Half, unsigned int, 4, 64, 64>(c10::Half*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<c10::Half, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",215889,243.66666666666666,103.70833333333333
aten::baddbmm,237,"[[64, 2048, 2048], [64, 2048, 96], [64, 96, 2048], [], []]",aten::baddbmm,237,"[[64, 2048, 2048], [64, 2048, 96], [64, 96, 2048], [], []]",ampere_fp16_s1688gemm_fp16_256x128_ldg8_f2f_stages_32x1_tn,215907,246.66666666666666,650.4520833333333
ScaledUpperTriangMaskedSoftmax,240,"[[64, 2048, 2048]]",ScaledUpperTriangMaskedSoftmax,240,"[[64, 2048, 2048]]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_forward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half const*, float, int, int, int)",215916,249.66666666666666,603.9333333333333
aten::bmm,257,"[[64, 2048, 2048], [64, 2048, 96]]",aten::bmm,257,"[[64, 2048, 2048], [64, 2048, 96]]",ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_nn,215962,246.0,402.15416666666664
aten::contiguous,261,"[[2048, 4, 16, 96], []]",aten::copy_,265,"[[2048, 4, 16, 96], [2048, 4, 16, 96], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",215973,256.3333333333333,60.99791666666667
aten::linear,267,"[[2048, 4, 1536], [6144, 1536], []]",aten::mm,274,"[[8192, 1536], [1536, 6144]]",ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_tn,215990,980.6666666666666,643.6479166666667
aten::add,279,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,279,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",216001,247.0,215.34375
aten::linear,284,"[[2048, 4, 6144], [6144, 6144], []]",aten::mm,291,"[[8192, 6144], [6144, 6144]]",void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8>(cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8::Params),216014,261.0,2318.75625
None,None,None,None,None,None,kernel_2,216027,253.33333333333334,160.33333333333334
aten::linear,299,"[[2048, 4, 6144], [6144, 6144], []]",aten::mm,306,"[[8192, 6144], [6144, 6144]]",void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8>(cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x256_64x3_tn_align8::Params),216039,248.66666666666666,2318.483333333333
aten::add,311,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,311,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",216047,260.6666666666667,213.125
aten::add,313,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,313,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3>)",216055,238.66666666666666,173.24791666666667
_ReduceFromModelParallelRegion,314,"[[2048, 4, 6144]]",nccl:all_reduce,317,"[[2048, 4, 6144]]","ncclKernel_AllReduce_RING_LL_Sum_half(ncclDevComm*, unsigned long, ncclWork*)",216073,340.0,753.55
aten::add,321,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,321,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<c10::Half>, at::detail::Array<char*, 3>)",216112,255.33333333333334,174.67083333333332
