[2024-10-09 21:39:10,813] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NeoXArgs.from_ymls() ['/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/configs/llama_13B_4_8_2.yml', '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/configs/local_setup.yml']
INFO:root:NeoXArgs.calculate_derived() Total number of GPUs determined to be: 64
-------------------- arguments --------------------
  attention_config ................ ['global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global']updated
  batch_size ...................... 4...........................updated
  checkpoint_activations .......... True........................updated
  checkpoint_factor ............... 10000.......................updated
  config_files .................... {'llama_13B_4_8_2.yml': '{\n  "pipe_parallel_size": 4,\n  "model_parallel_size": 8,\n  "make_vocab_size_divisible_by": 128,\n\n  # model settings\n  "num_layers": 40,\n  "hidden_size": 5120,\n  "num_attention_heads": 40,\n  "seq_length": 2048,\n  "max_position_embeddings": 2048,\n  "pos_emb": "rotary",\n  "rotary_pct": 1,\n  "no_weight_tying": true,\n  "gpt_j_residual": false,\n  "output_layer_parallelism": "column",\n  "norm": "rmsnorm",\n  "rms_norm_epsilon": 1.0e-6,\n\n  "scaled_upper_triang_masked_softmax_fusion": true,\n  "bias_gelu_fusion": false,\n  "use_bias_in_norms": false,\n  "use_bias_in_attn_linear": false,\n  # "activation": "swiglu",\n  "mlp_multiple_of": 256,\n\n\n\n  # training setting\n  # finetuning option\n  # "finetune": true,\n\n  # init methods\n  "init_method": "small_init",\n  "output_layer_init_method": "wang_init",\n\n  # optimizer settings\n  "optimizer": {\n    "type": "Adam",\n    "params": {\n     "lr": 0.0002,\n     "betas": [0.9, 0.95],\n     "eps":  1.0e-8,\n    }\n  },\n  "min_lr": 0.00002,\n  "override_lr_scheduler": true,\n\n  # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n   "zero_optimization": {\n   "stage": 1,\n   "allgather_partitions": True,\n   "allgather_bucket_size": 500000000,\n   "overlap_comm": True,\n   "reduce_scatter": True,\n   "reduce_bucket_size": 500000000,\n   "contiguous_gradients": True,\n  },\n\n  # batch / data settings\n  "train_micro_batch_size_per_gpu": 4,\n  "gradient_accumulation_steps": 16,\n  "data_impl": "mmap",\n\n  # activation checkpointing\n  "checkpoint_activations": true,\n  "checkpoint_num_layers": 1,\n  "partition_activations": true,\n  "synchronize_each_layer": true,\n\n  # regularization\n  "gradient_clipping": 1.0,\n  "weight_decay": 0.1,\n  "hidden_dropout": 0,\n  "attention_dropout": 0,\n\n  # precision settings\n  "fp16": {\n    "fp16": true,\n    "enabled": true,\n    "loss_scale": 0,\n    "loss_scale_window": 1000,\n    "hysteresis": 2,\n    "min_loss_scale": 1\n  },\n\n  # misc. training settings\n  "train_iters": 8,\n  "lr_decay_iters": 8,\n  "distributed_backend": "nccl",\n  "lr_decay_style": "cosine",\n  "warmup": 0.01,\n  "checkpoint_factor": 10000,\n  "eval_interval": 1000,\n  "eval_iters": 100,\n\n  # logging\n  "log_interval": 1,\n  "steps_per_print": 1,\n  "keep_last_n_checkpoints": 4,\n  "wall_clock_breakdown": true,\n  "mlp_multiple_of": 256,\n\n  # profiling settings\n  "profile": True,\n  "profile_step_start": 2,\n  "profile_step_stop": 7,\n\n}\n', 'local_setup.yml': '# Suggested data paths when using GPT-NeoX locally\n{\n  # "data_path": "/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/data/pile_text_document",\n  # "data_path": "/pscratch/sd/z/zhaozh/data/pile/processed/pile_text_document",\n\n  "data-path": "/pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document",\n\n  # or for weighted datasets:\n  # "train-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "test-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "valid-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "train-data-weights": [1., 2.],\n  # "test-data-weights": [2., 1.],\n  # "valid-data-weights": [0.5, 0.4],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # "weight_by_num_documents": false,\n  # "weighted_sampler_alpha": 0.3,\n\n  "vocab_file": "/pscratch/sd/z/zhaozh/data/pile/gpt2-vocab.json",\n  "merge_file": "/pscratch/sd/z/zhaozh/data/pile/gpt2-merges.txt",\n\n  "save": "checkpoints-test",\n  "load": "checkpoints-test2",\n  "checkpoint_validation_with_forward_pass": False,\n\n  "tensorboard_dir": "tensorboard",\n  "log_dir": "logs",\n  "use_wandb": True,\n  "wandb_host": "https://api.wandb.ai",\n  "wandb_project": "neox"\n}\n'}updated
  data_impl ....................... mmap........................updated
  data_path ....................... /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_documentupdated
  dynamic_loss_scale .............. True........................updated
  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}updated
  global_num_gpus ................. 64..........................updated
  gradient_accumulation_steps ..... 16..........................updated
  hidden_size ..................... 5120........................updated
  init_method ..................... small_init..................updated
  is_pipe_parallel ................ True........................updated
  keep_last_n_checkpoints ......... 4...........................updated
  load ............................ checkpoints-test2...........updated
  log_dir ......................... logs........................updated
  log_interval .................... 1...........................updated
  lr .............................. 0.0002......................updated
  lr_decay_iters .................. 8...........................updated
  lr_decay_style .................. cosine......................updated
  max_position_embeddings ......... 2048........................updated
  merge_file ...................... /pscratch/sd/z/zhaozh/data/pile/gpt2-merges.txtupdated
  min_lr .......................... 2e-05.......................updated
  mlp_multiple_of ................. 256.........................updated
  model_parallel_size ............. 8...........................updated
  no_weight_tying ................. True........................updated
  norm ............................ rmsnorm.....................updated
  num_attention_heads ............. 40..........................updated
  num_layers ...................... 40..........................updated
  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated
  optimizer_type .................. Adam........................updated
  output_layer_init_method ........ wang_init...................updated
  override_lr_scheduler ........... True........................updated
  partition_activations ........... True........................updated
  pipe_parallel_size .............. 4...........................updated
  pos_emb ......................... rotary......................updated
  precision ....................... fp16........................updated
  profile ......................... True........................updated
  profile_step_start .............. 2...........................updated
  profile_step_stop ............... 7...........................updated
  rms_norm_epsilon ................ 1e-06.......................updated
  save ............................ checkpoints-test............updated
  save_iters ...................... []..........................updated
  scaled_upper_triang_masked_softmax_fusion  True...............updated
  seq_length ...................... 2048........................updated
  sparsity_config ................. {}..........................updated
  steps_per_print ................. 1...........................updated
  synchronize_each_layer .......... True........................updated
  tensorboard_dir ................. tensorboard.................updated
  text_gen_type ................... unconditional...............updated
  train_batch_size ................ 128.........................updated
  train_iters ..................... 8...........................updated
  train_micro_batch_size_per_gpu .. 4...........................updated
  use_bias_in_attn_linear ......... False.......................updated
  use_bias_in_norms ............... False.......................updated
  use_wandb ....................... True........................updated
  user_script ..................... /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/train_profiling.pyupdated
  vocab_file ...................... /pscratch/sd/z/zhaozh/data/pile/gpt2-vocab.jsonupdated
  wall_clock_breakdown ............ True........................updated
  wandb_group ..................... rq3n4xyg_rgy49pi6...........updated
  zero_allgather_bucket_size ...... 500000000...................updated
  zero_contiguous_gradients ....... True........................updated
  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True}updated
  zero_reduce_bucket_size ......... 500000000...................updated
  zero_reduce_scatter ............. True........................updated
  zero_stage ...................... 1...........................updated
  account ......................... None........................default
  activation ...................... gelu........................default
  activation_checkpointing ........ None........................default
  adlr_autoresume ................. False.......................default
  adlr_autoresume_interval ........ 1000........................default
  allow_chopped ................... True........................default
  amp ............................. None........................default
  apply_query_key_layer_scaling ... False.......................default
  attention_dropout ............... 0...........................default
  attention_softmax_in_fp32 ....... False.......................default
  autotuning ...................... None........................default
  autotuning_run .................. None........................default
  base_shapes_file ................ None........................default
  bf16 ............................ None........................default
  bias_dropout_fusion ............. False.......................default
  bias_gelu_fusion ................ False.......................default
  char_level_ppl .................. False.......................default
  checkpoint ...................... None........................default
  checkpoint_in_cpu ............... False.......................default
  checkpoint_num_layers ........... 1...........................default
  checkpoint_scale ................ linear......................default
  checkpoint_validation_with_forward_pass  False................default
  clip_grad ....................... 1.0.........................default
  comet_experiment_name ........... None........................default
  comet_others .................... None........................default
  comet_project ................... None........................default
  comet_tags ...................... None........................default
  comet_workspace ................. None........................default
  comment ......................... None........................default
  comms_logger .................... None........................default
  communication_data_type ......... None........................default
  compression_training ............ None........................default
  contiguous_checkpointing ........ False.......................default
  coord_check ..................... False.......................default
  create_moe_param_group .......... True........................default
  csv_monitor ..................... None........................default
  curriculum_learning ............. None........................default
  curriculum_seqlen ............... 0...........................default
  data_efficiency ................. None........................default
  data_types ...................... None........................default
  dataset_impl .................... gpt2........................default
  deepscale ....................... False.......................default
  deepscale_config ................ None........................default
  deepspeed ....................... True........................default
  deepspeed_activation_checkpointing  True......................default
  deepspeed_extra_args ............ None........................default
  deepspeed_mpi ................... False.......................default
  deepspeed_slurm ................. False.......................default
  detect_nvlink_pairs ............. False.......................default
  distributed_backend ............. nccl........................default
  do_test ......................... None........................default
  do_train ........................ None........................default
  do_valid ........................ None........................default
  dpo_beta ........................ 0.1.........................default
  dpo_fp32 ........................ True........................default
  dpo_reference_free .............. False.......................default
  dump_state ...................... False.......................default
  elasticity ...................... None........................default
  enable_expert_tensor_parallelism  False.......................default
  eod_mask_loss ................... False.......................default
  eval_interval ................... 1000........................default
  eval_iters ...................... 100.........................default
  eval_results_prefix ............. ............................default
  eval_tasks ...................... None........................default
  exclude ......................... None........................default
  exit_interval ................... None........................default
  expansion_factor ................ None........................default
  expert_interval ................. 2...........................default
  extra_save_iters ................ None........................default
  finetune ........................ False.......................default
  flops_profiler .................. None........................default
  force_multi ..................... False.......................default
  fp16_lm_cross_entropy ........... False.......................default
  fp32_allreduce .................. False.......................default
  git_hash ........................ d79c5331....................default
  gmlp_attn_dim ................... 64..........................default
  gpt_j_residual .................. False.......................default
  gpt_j_tied ...................... False.......................default
  gradient_clipping ............... 1.0.........................default
  gradient_noise_scale_cpu_offload  False.......................default
  gradient_noise_scale_n_batches .. 5...........................default
  gradient_predivide_factor ....... 1.0.........................default
  hidden_dropout .................. 0...........................default
  hostfile ........................ None........................default
  hysteresis ...................... 2...........................default
  include ......................... None........................default
  init_method_std ................. 0.02........................default
  intermediate_size ............... None........................default
  iteration ....................... None........................default
  kto_beta ........................ 0.1.........................default
  kto_desirable_weight ............ 1.0.........................default
  kto_fp32 ........................ True........................default
  kto_undesirable_weight .......... 1.0.........................default
  launcher ........................ pdsh........................default
  layernorm_epsilon ............... 1e-05.......................default
  layernorm_fusion ................ False.......................default
  lazy_mpu_init ................... False.......................default
  local_rank ...................... None........................default
  log_grad_norm ................... False.......................default
  log_grad_pct_zeros .............. False.......................default
  log_gradient_noise_scale ........ False.......................default
  log_optimizer_states ............ False.......................default
  log_param_norm .................. False.......................default
  loss_scale ...................... None........................default
  loss_scale_window ............... 1000.0......................default
  make_vocab_size_divisible_by .... 128.........................default
  mamba_causal_conv_fusion ........ False.......................default
  mamba_inner_func_fusion ......... False.......................default
  mamba_selective_fp32_params ..... True........................default
  mamba_selective_scan_fusion ..... False.......................default
  mamba_use_bias_in_conv .......... True........................default
  mamba_use_bias_in_linears ....... False.......................default
  master_addr ..................... None........................default
  master_port ..................... 29500.......................default
  maximum_tokens .................. 64..........................default
  memory_profiling ................ False.......................default
  memory_profiling_path ........... None........................default
  min_scale ....................... 1.0.........................default
  mmap_warmup ..................... False.......................default
  moe_eval_capacity_factor ........ 1.0.........................default
  moe_expert_parallel_size ........ 1...........................default
  moe_glu ......................... False.......................default
  moe_jitter_eps .................. None........................default
  moe_lbl_in_fp32 ................. False.......................default
  moe_loss_coeff .................. 0.1.........................default
  moe_min_capacity ................ 4...........................default
  moe_num_experts ................. 1...........................default
  moe_token_dropping .............. False.......................default
  moe_top_k ....................... 1...........................default
  moe_train_capacity_factor ....... 1.0.........................default
  moe_type ........................ megablocks..................default
  moe_use_residual ................ True........................default
  mup_attn_temp ................... 1.0.........................default
  mup_embedding_mult .............. 1.0.........................default
  mup_init_scale .................. 1.0.........................default
  mup_output_temp ................. 1.0.........................default
  mup_rp_embedding_mult ........... 1.0.........................default
  mup_width_scale ................. 2...........................default
  neg_test_data_paths ............. None........................default
  neg_test_label_data_paths ....... None........................default
  neg_train_data_paths ............ None........................default
  neg_train_label_data_paths ...... None........................default
  neg_valid_data_paths ............ None........................default
  neg_valid_label_data_paths ...... None........................default
  no_load_optim ................... False.......................default
  no_load_rng ..................... False.......................default
  no_save_optim ................... False.......................default
  no_save_rng ..................... False.......................default
  no_ssh_check .................... False.......................default
  num_gpus ........................ None........................default
  num_kv_heads .................... None........................default
  num_nodes ....................... -1..........................default
  num_samples ..................... 1...........................default
  num_unique_layers ............... None........................default
  num_workers ..................... 2...........................default
  onnx_safe ....................... False.......................default
  opt_pos_emb_offset .............. 0...........................default
  output_layer_parallelism ........ column......................default
  pack_impl ....................... packed......................default
  padded_vocab_size ............... None........................default
  param_sharing_style ............. grouped.....................default
  pipe_partition_method ........... type:transformer|mlp........default
  pos_test_data_paths ............. None........................default
  pos_test_label_data_paths ....... None........................default
  pos_train_data_paths ............ None........................default
  pos_train_label_data_paths ...... None........................default
  pos_valid_data_paths ............ None........................default
  pos_valid_label_data_paths ...... None........................default
  precompute_model_name ........... None........................default
  prescale_gradients .............. False.......................default
  profile_backward ................ False.......................default
  prompt_end ...................... 
...........................default
  rank ............................ None........................default
  recompute ....................... False.......................default
  return_logits ................... False.......................default
  rmsnorm_fusion .................. False.......................default
  rope_fusion ..................... False.......................default
  rotary_emb_base ................. 10000.......................default
  rotary_pct ...................... 1...........................default
  rotary_save_freqs_buffer ........ False.......................default
  rpe_max_distance ................ 128.........................default
  rpe_num_buckets ................. 32..........................default
  s3_chunk_size ................... 104857600...................default
  s3_path ......................... None........................default
  sample_input_file ............... None........................default
  sample_output_file .............. samples.txt.................default
  save_base_shapes ................ False.......................default
  scaled_masked_softmax_fusion .... False.......................default
  scalenorm_epsilon ............... 1e-08.......................default
  scheduler ....................... None........................default
  seed ............................ 1234........................default
  sequence_parallel ............... False.......................default
  short_seq_prob .................. 0.1.........................default
  sliding_window_width ............ None........................default
  soft_prompt_tuning .............. None........................default
  sparse_attention ................ None........................default
  sparse_gradients ................ False.......................default
  split ........................... 969, 30, 1..................default
  temperature ..................... 0.0.........................default
  tensorboard ..................... None........................default
  test_data_paths ................. None........................default
  test_data_weights ............... None........................default
  test_label_data_paths ........... None........................default
  test_reward_data_paths .......... None........................default
  tokenizer_type .................. GPT2BPETokenizer............default
  top_k ........................... 0...........................default
  top_p ........................... 0.0.........................default
  train_data_paths ................ None........................default
  train_data_weights .............. None........................default
  train_impl ...................... normal......................default
  train_label_data_paths .......... None........................default
  train_reward_data_paths ......... None........................default
  use_bias_in_mlp ................. True........................default
  use_bnb_optimizer ............... False.......................default
  use_checkpoint_lr_scheduler ..... False.......................default
  use_comet ....................... None........................default
  use_cpu_initialization .......... False.......................default
  use_flashattn_swiglu ............ False.......................default
  use_mup ......................... False.......................default
  use_qk_layernorm ................ False.......................default
  use_shared_fs ................... True........................default
  use_tutel ....................... False.......................default
  valid_data_paths ................ None........................default
  valid_data_weights .............. None........................default
  valid_label_data_paths .......... None........................default
  valid_reward_data_paths ......... None........................default
  wandb ........................... None........................default
  wandb_host ...................... https://api.wandb.ai........default
  wandb_init_all_ranks ............ False.......................default
  wandb_project ................... neox........................default
  wandb_team ...................... None........................default
  warmup .......................... 0.01........................default
  weight_by_num_documents ......... False.......................default
  weight_decay .................... 0.1.........................default
  weighted_sampler_alpha .......... 1.0.........................default
  world_size ...................... None........................default
---------------- end of arguments ----------------
NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 8 
[2024-10-09 21:39:40,455] [INFO] [multinode_runner.py:73:get_cmd] Running on the following workers: nid001364,nid001365,nid001368,nid001369,nid001372,nid001373,nid001376,nid001377,nid002109,nid002261,nid002264,nid002265,nid002268,nid002424,nid003037,nid003209
[2024-10-09 21:39:40,455] [INFO] [runner.py:586:main] cmd = pdsh -S -f 1024 -w nid001364,nid001365,nid001368,nid001369,nid001372,nid001373,nid001376,nid001377,nid002109,nid002261,nid002264,nid002265,nid002268,nid002424,nid003037,nid003209 export PYTHONPATH=/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/slurms:/opt/nersc/pymon; export NCCL_NET_GDR_LEVEL=PHB; export NCCL_SOCKET_IFNAME=hsn;  cd /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/slurms; /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/bin/python -u -m deepspeed.launcher.launch --world_info=eyJuaWQwMDEzNjQiOiBbMCwgMSwgMiwgM10sICJuaWQwMDEzNjUiOiBbMCwgMSwgMiwgM10sICJuaWQwMDEzNjgiOiBbMCwgMSwgMiwgM10sICJuaWQwMDEzNjkiOiBbMCwgMSwgMiwgM10sICJuaWQwMDEzNzIiOiBbMCwgMSwgMiwgM10sICJuaWQwMDEzNzMiOiBbMCwgMSwgMiwgM10sICJuaWQwMDEzNzYiOiBbMCwgMSwgMiwgM10sICJuaWQwMDEzNzciOiBbMCwgMSwgMiwgM10sICJuaWQwMDIxMDkiOiBbMCwgMSwgMiwgM10sICJuaWQwMDIyNjEiOiBbMCwgMSwgMiwgM10sICJuaWQwMDIyNjQiOiBbMCwgMSwgMiwgM10sICJuaWQwMDIyNjUiOiBbMCwgMSwgMiwgM10sICJuaWQwMDIyNjgiOiBbMCwgMSwgMiwgM10sICJuaWQwMDI0MjQiOiBbMCwgMSwgMiwgM10sICJuaWQwMDMwMzciOiBbMCwgMSwgMiwgM10sICJuaWQwMDMyMDkiOiBbMCwgMSwgMiwgM119 --node_rank=%n --master_addr=nid001364 --master_port=29500 /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/train_profiling.py --deepspeed_config 'eyJ0cmFpbl9iYXRjaF9zaXplIjogMTI4LCAidHJhaW5fbWljcm9fYmF0Y2hfc2l6ZV9wZXJfZ3B1IjogNCwgImdyYWRpZW50X2FjY3VtdWxhdGlvbl9zdGVwcyI6IDE2LCAib3B0aW1pemVyIjogeyJ0eXBlIjogIkFkYW0iLCAicGFyYW1zIjogeyJsciI6IDAuMDAwMiwgImJldGFzIjogWzAuOSwgMC45NV0sICJlcHMiOiAxZS0wOH19LCAiZnAxNiI6IHsiZnAxNiI6IHRydWUsICJlbmFibGVkIjogdHJ1ZSwgImxvc3Nfc2NhbGUiOiAwLCAibG9zc19zY2FsZV93aW5kb3ciOiAxMDAwLCAiaHlzdGVyZXNpcyI6IDIsICJtaW5fbG9zc19zY2FsZSI6IDF9LCAiemVyb19vcHRpbWl6YXRpb24iOiB7InN0YWdlIjogMSwgImFsbGdhdGhlcl9wYXJ0aXRpb25zIjogdHJ1ZSwgImFsbGdhdGhlcl9idWNrZXRfc2l6ZSI6IDUwMDAwMDAwMCwgIm92ZXJsYXBfY29tbSI6IHRydWUsICJyZWR1Y2Vfc2NhdHRlciI6IHRydWUsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IHRydWV9LCAic3RlcHNfcGVyX3ByaW50IjogMSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZX0=' --megatron_config '{"train_batch_size": 128, "train_micro_batch_size_per_gpu": 4, "gradient_accumulation_steps": 16, "optimizer": {"type": "Adam", "params": {"lr": 0.0002, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true}, "steps_per_print": 1, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 40, "hidden_size": 5120, "mlp_multiple_of": 256, "num_attention_heads": 40, "seq_length": 2048, "max_position_embeddings": 2048, "norm": "rmsnorm", "rms_norm_epsilon": 1e-06, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "scaled_upper_triang_masked_softmax_fusion": true, "init_method": "small_init", "output_layer_init_method": "wang_init", "use_bias_in_norms": false, "use_bias_in_attn_linear": false, "lr_decay_style": "cosine", "lr_decay_iters": 8, "min_lr": 2e-05, "override_lr_scheduler": true, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0002, "data_path": "/pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document", "data_impl": "mmap", "save": "checkpoints-test", "config_files": {"llama_13B_4_8_2.yml": "{\n  \"pipe_parallel_size\": 4,\n  \"model_parallel_size\": 8,\n  \"make_vocab_size_divisible_by\": 128,\n\n  # model settings\n  \"num_layers\": 40,\n  \"hidden_size\": 5120,\n  \"num_attention_heads\": 40,\n  \"seq_length\": 2048,\n  \"max_position_embeddings\": 2048,\n  \"pos_emb\": \"rotary\",\n  \"rotary_pct\": 1,\n  \"no_weight_tying\": true,\n  \"gpt_j_residual\": false,\n  \"output_layer_parallelism\": \"column\",\n  \"norm\": \"rmsnorm\",\n  \"rms_norm_epsilon\": 1.0e-6,\n\n  \"scaled_upper_triang_masked_softmax_fusion\": true,\n  \"bias_gelu_fusion\": false,\n  \"use_bias_in_norms\": false,\n  \"use_bias_in_attn_linear\": false,\n  # \"activation\": \"swiglu\",\n  \"mlp_multiple_of\": 256,\n\n\n\n  # training setting\n  # finetuning option\n  # \"finetune\": true,\n\n  # init methods\n  \"init_method\": \"small_init\",\n  \"output_layer_init_method\": \"wang_init\",\n\n  # optimizer settings\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n     \"lr\": 0.0002,\n     \"betas\": [0.9, 0.95],\n     \"eps\":  1.0e-8,\n    }\n  },\n  \"min_lr\": 0.00002,\n  \"override_lr_scheduler\": true,\n\n  # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n   \"zero_optimization\": {\n   \"stage\": 1,\n   \"allgather_partitions\": True,\n   \"allgather_bucket_size\": 500000000,\n   \"overlap_comm\": True,\n   \"reduce_scatter\": True,\n   \"reduce_bucket_size\": 500000000,\n   \"contiguous_gradients\": True,\n  },\n\n  # batch / data settings\n  \"train_micro_batch_size_per_gpu\": 4,\n  \"gradient_accumulation_steps\": 16,\n  \"data_impl\": \"mmap\",\n\n  # activation checkpointing\n  \"checkpoint_activations\": true,\n  \"checkpoint_num_layers\": 1,\n  \"partition_activations\": true,\n  \"synchronize_each_layer\": true,\n\n  # regularization\n  \"gradient_clipping\": 1.0,\n  \"weight_decay\": 0.1,\n  \"hidden_dropout\": 0,\n  \"attention_dropout\": 0,\n\n  # precision settings\n  \"fp16\": {\n    \"fp16\": true,\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n\n  # misc. training settings\n  \"train_iters\": 8,\n  \"lr_decay_iters\": 8,\n  \"distributed_backend\": \"nccl\",\n  \"lr_decay_style\": \"cosine\",\n  \"warmup\": 0.01,\n  \"checkpoint_factor\": 10000,\n  \"eval_interval\": 1000,\n  \"eval_iters\": 100,\n\n  # logging\n  \"log_interval\": 1,\n  \"steps_per_print\": 1,\n  \"keep_last_n_checkpoints\": 4,\n  \"wall_clock_breakdown\": true,\n  \"mlp_multiple_of\": 256,\n\n  # profiling settings\n  \"profile\": True,\n  \"profile_step_start\": 2,\n  \"profile_step_stop\": 7,\n\n}\n", "local_setup.yml": "# Suggested data paths when using GPT-NeoX locally\n{\n  # \"data_path\": \"/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/data/pile_text_document\",\n  # \"data_path\": \"/pscratch/sd/z/zhaozh/data/pile/processed/pile_text_document\",\n\n  \"data-path\": \"/pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document\",\n\n  # or for weighted datasets:\n  # \"train-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"test-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"valid-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"train-data-weights\": [1., 2.],\n  # \"test-data-weights\": [2., 1.],\n  # \"valid-data-weights\": [0.5, 0.4],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # \"weight_by_num_documents\": false,\n  # \"weighted_sampler_alpha\": 0.3,\n\n  \"vocab_file\": \"/pscratch/sd/z/zhaozh/data/pile/gpt2-vocab.json\",\n  \"merge_file\": \"/pscratch/sd/z/zhaozh/data/pile/gpt2-merges.txt\",\n\n  \"save\": \"checkpoints-test\",\n  \"load\": \"checkpoints-test2\",\n  \"checkpoint_validation_with_forward_pass\": False,\n\n  \"tensorboard_dir\": \"tensorboard\",\n  \"log_dir\": \"logs\",\n  \"use_wandb\": True,\n  \"wandb_host\": \"https://api.wandb.ai\",\n  \"wandb_project\": \"neox\"\n}\n"}, "load": "checkpoints-test2", "checkpoint_factor": 10000, "batch_size": 4, "train_iters": 8, "keep_last_n_checkpoints": 4, "vocab_file": "/pscratch/sd/z/zhaozh/data/pile/gpt2-vocab.json", "merge_file": "/pscratch/sd/z/zhaozh/data/pile/gpt2-merges.txt", "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "dynamic_loss_scale": true, "pipe_parallel_size": 4, "model_parallel_size": 8, "world_size": 1, "is_pipe_parallel": true, "use_wandb": true, "wandb_group": "rq3n4xyg_rgy49pi6", "log_dir": "logs", "tensorboard_dir": "tensorboard", "log_interval": 1, "profile": true, "profile_step_start": 2, "profile_step_stop": 7, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/train_profiling.py", "save_iters": [], "global_num_gpus": 64}'
nid001376: ***************************************************************************
nid001376:                           NOTICE TO USERS
nid001376: 
nid001376: Lawrence Berkeley National Laboratory operates this computer system under 
nid001376: contract to the U.S. Department of Energy.  This computer system is the 
nid001376: property of the United States Government and is for authorized use only.
nid001376: Users (authorized or unauthorized) have no explicit or implicit 
nid001376: expectation of privacy.
nid001376: 
nid001376: Any or all uses of this system and all files on this system may be
nid001376: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001376: to authorized site, Department of Energy, and law enforcement personnel,
nid001376: as well as authorized officials of other agencies, both domestic and foreign.
nid001376: By using this system, the user consents to such interception, monitoring,
nid001376: recording, copying, auditing, inspection, and disclosure at the discretion
nid001376: of authorized site or Department of Energy personnel.
nid001376: 
nid001376: Unauthorized or improper use of this system may result in administrative
nid001376: disciplinary action and civil and criminal penalties. By continuing to use
nid001376: this system you indicate your awareness of and consent to these terms and
nid001376: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001376: stated in this warning.
nid001376: 
nid001376: *****************************************************************************
nid001376: 
nid001376: Login connection to host x1001c3s6b0n0:
nid001376: 
nid001364: ***************************************************************************
nid001364:                           NOTICE TO USERS
nid001364: 
nid001364: Lawrence Berkeley National Laboratory operates this computer system under 
nid001364: contract to the U.S. Department of Energy.  This computer system is the 
nid001364: property of the United States Government and is for authorized use only.
nid001364: Users (authorized or unauthorized) have no explicit or implicit 
nid001364: expectation of privacy.
nid001364: 
nid001364: Any or all uses of this system and all files on this system may be
nid001364: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001364: to authorized site, Department of Energy, and law enforcement personnel,
nid001364: as well as authorized officials of other agencies, both domestic and foreign.
nid001364: By using this system, the user consents to such interception, monitoring,
nid001364: recording, copying, auditing, inspection, and disclosure at the discretion
nid001364: of authorized site or Department of Energy personnel.
nid001364: 
nid001364: Unauthorized or improper use of this system may result in administrative
nid001364: disciplinary action and civil and criminal penalties. By continuing to use
nid001364: this system you indicate your awareness of and consent to these terms and
nid001364: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001364: stated in this warning.
nid001364: 
nid001364: *****************************************************************************
nid001364: 
nid001364: Login connection to host x1001c3s3b0n0:
nid001364: 
nid001377: ***************************************************************************
nid001377:                           NOTICE TO USERS
nid001377: 
nid001377: Lawrence Berkeley National Laboratory operates this computer system under 
nid001377: contract to the U.S. Department of Energy.  This computer system is the 
nid001377: property of the United States Government and is for authorized use only.
nid001377: Users (authorized or unauthorized) have no explicit or implicit 
nid001377: expectation of privacy.
nid001377: 
nid001377: Any or all uses of this system and all files on this system may be
nid001377: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001377: to authorized site, Department of Energy, and law enforcement personnel,
nid001377: as well as authorized officials of other agencies, both domestic and foreign.
nid001377: By using this system, the user consents to such interception, monitoring,
nid001377: recording, copying, auditing, inspection, and disclosure at the discretion
nid001377: of authorized site or Department of Energy personnel.
nid001377: 
nid001377: Unauthorized or improper use of this system may result in administrative
nid001377: disciplinary action and civil and criminal penalties. By continuing to use
nid001377: this system you indicate your awareness of and consent to these terms and
nid001377: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001377: stated in this warning.
nid001377: 
nid001377: *****************************************************************************
nid001377: 
nid001377: Login connection to host x1001c3s6b0n1:
nid001377: 
nid001373: ***************************************************************************
nid001373:                           NOTICE TO USERS
nid001373: 
nid001373: Lawrence Berkeley National Laboratory operates this computer system under 
nid001373: contract to the U.S. Department of Energy.  This computer system is the 
nid001373: property of the United States Government and is for authorized use only.
nid001373: Users (authorized or unauthorized) have no explicit or implicit 
nid001373: expectation of privacy.
nid001373: 
nid001373: Any or all uses of this system and all files on this system may be
nid001373: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001373: to authorized site, Department of Energy, and law enforcement personnel,
nid001373: as well as authorized officials of other agencies, both domestic and foreign.
nid001373: By using this system, the user consents to such interception, monitoring,
nid001373: recording, copying, auditing, inspection, and disclosure at the discretion
nid001373: of authorized site or Department of Energy personnel.
nid001373: 
nid001373: Unauthorized or improper use of this system may result in administrative
nid001373: disciplinary action and civil and criminal penalties. By continuing to use
nid001373: this system you indicate your awareness of and consent to these terms and
nid001373: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001373: stated in this warning.
nid001373: 
nid001373: *****************************************************************************
nid001373: 
nid001373: Login connection to host x1001c3s5b0n1:
nid001373: 
nid002264: ***************************************************************************
nid002264:                           NOTICE TO USERS
nid002264: 
nid002264: Lawrence Berkeley National Laboratory operates this computer system under 
nid002264: contract to the U.S. Department of Energy.  This computer system is the 
nid002264: property of the United States Government and is for authorized use only.
nid002264: Users (authorized or unauthorized) have no explicit or implicit 
nid002264: expectation of privacy.
nid002264: 
nid002264: Any or all uses of this system and all files on this system may be
nid002264: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid002264: to authorized site, Department of Energy, and law enforcement personnel,
nid002264: as well as authorized officials of other agencies, both domestic and foreign.
nid002264: By using this system, the user consents to such interception, monitoring,
nid002264: recording, copying, auditing, inspection, and disclosure at the discretion
nid002264: of authorized site or Department of Energy personnel.
nid002264: 
nid002264: Unauthorized or improper use of this system may result in administrative
nid002264: disciplinary action and civil and criminal penalties. By continuing to use
nid002264: this system you indicate your awareness of and consent to these terms and
nid002264: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid002264: stated in this warning.
nid002264: 
nid002264: *****************************************************************************
nid002264: 
nid002264: Login connection to host x1100c7s4b0n0:
nid002264: 
nid001369: ***************************************************************************
nid001369:                           NOTICE TO USERS
nid001369: 
nid001369: Lawrence Berkeley National Laboratory operates this computer system under 
nid001369: contract to the U.S. Department of Energy.  This computer system is the 
nid001369: property of the United States Government and is for authorized use only.
nid001369: Users (authorized or unauthorized) have no explicit or implicit 
nid001369: expectation of privacy.
nid001369: 
nid001369: Any or all uses of this system and all files on this system may be
nid001369: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001369: to authorized site, Department of Energy, and law enforcement personnel,
nid001369: as well as authorized officials of other agencies, both domestic and foreign.
nid001369: By using this system, the user consents to such interception, monitoring,
nid001369: recording, copying, auditing, inspection, and disclosure at the discretion
nid001369: of authorized site or Department of Energy personnel.
nid001369: 
nid001369: Unauthorized or improper use of this system may result in administrative
nid001369: disciplinary action and civil and criminal penalties. By continuing to use
nid001369: this system you indicate your awareness of and consent to these terms and
nid001369: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001369: stated in this warning.
nid001369: 
nid001369: *****************************************************************************
nid001369: 
nid001369: Login connection to host x1001c3s4b0n1:
nid001369: 
nid002268: ***************************************************************************
nid002268:                           NOTICE TO USERS
nid002268: 
nid002268: Lawrence Berkeley National Laboratory operates this computer system under 
nid002268: contract to the U.S. Department of Energy.  This computer system is the 
nid002268: property of the United States Government and is for authorized use only.
nid002268: Users (authorized or unauthorized) have no explicit or implicit 
nid002268: expectation of privacy.
nid002268: 
nid002268: Any or all uses of this system and all files on this system may be
nid002268: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid002268: to authorized site, Department of Energy, and law enforcement personnel,
nid002268: as well as authorized officials of other agencies, both domestic and foreign.
nid002268: By using this system, the user consents to such interception, monitoring,
nid002268: recording, copying, auditing, inspection, and disclosure at the discretion
nid002268: of authorized site or Department of Energy personnel.
nid002268: 
nid002268: Unauthorized or improper use of this system may result in administrative
nid002268: disciplinary action and civil and criminal penalties. By continuing to use
nid002268: this system you indicate your awareness of and consent to these terms and
nid002268: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid002268: stated in this warning.
nid002268: 
nid002268: *****************************************************************************
nid002268: 
nid002268: Login connection to host x1100c7s5b0n0:
nid002268: 
nid002424: ***************************************************************************
nid002424:                           NOTICE TO USERS
nid002424: 
nid002424: Lawrence Berkeley National Laboratory operates this computer system under 
nid002424: contract to the U.S. Department of Energy.  This computer system is the 
nid002424: property of the United States Government and is for authorized use only.
nid002424: Users (authorized or unauthorized) have no explicit or implicit 
nid002424: expectation of privacy.
nid002424: 
nid002424: Any or all uses of this system and all files on this system may be
nid002424: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid002424: to authorized site, Department of Energy, and law enforcement personnel,
nid002424: as well as authorized officials of other agencies, both domestic and foreign.
nid002424: By using this system, the user consents to such interception, monitoring,
nid002424: recording, copying, auditing, inspection, and disclosure at the discretion
nid002424: of authorized site or Department of Energy personnel.
nid002424: 
nid002424: Unauthorized or improper use of this system may result in administrative
nid002424: disciplinary action and civil and criminal penalties. By continuing to use
nid002424: this system you indicate your awareness of and consent to these terms and
nid002424: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid002424: stated in this warning.
nid002424: 
nid002424: *****************************************************************************
nid002424: 
nid002424: Login connection to host x1101c4s4b0n0:
nid002424: 
nid003209: ***************************************************************************
nid003209:                           NOTICE TO USERS
nid003209: 
nid003209: Lawrence Berkeley National Laboratory operates this computer system under 
nid003209: contract to the U.S. Department of Energy.  This computer system is the 
nid003209: property of the United States Government and is for authorized use only.
nid003209: Users (authorized or unauthorized) have no explicit or implicit 
nid003209: expectation of privacy.
nid003209: 
nid003209: Any or all uses of this system and all files on this system may be
nid003209: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid003209: to authorized site, Department of Energy, and law enforcement personnel,
nid003209: as well as authorized officials of other agencies, both domestic and foreign.
nid003209: By using this system, the user consents to such interception, monitoring,
nid003209: recording, copying, auditing, inspection, and disclosure at the discretion
nid003209: of authorized site or Department of Energy personnel.
nid003209: 
nid003209: Unauthorized or improper use of this system may result in administrative
nid003209: disciplinary action and civil and criminal penalties. By continuing to use
nid003209: this system you indicate your awareness of and consent to these terms and
nid003209: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid003209: stated in this warning.
nid003209: 
nid003209: *****************************************************************************
nid003209: 
nid003209: Login connection to host x1200c5s0b0n1:
nid003209: 
nid001365: ***************************************************************************
nid001365:                           NOTICE TO USERS
nid001365: 
nid001365: Lawrence Berkeley National Laboratory operates this computer system under 
nid001365: contract to the U.S. Department of Energy.  This computer system is the 
nid001365: property of the United States Government and is for authorized use only.
nid001365: Users (authorized or unauthorized) have no explicit or implicit 
nid001365: expectation of privacy.
nid001365: 
nid001365: Any or all uses of this system and all files on this system may be
nid001365: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001365: to authorized site, Department of Energy, and law enforcement personnel,
nid001365: as well as authorized officials of other agencies, both domestic and foreign.
nid001365: By using this system, the user consents to such interception, monitoring,
nid001365: recording, copying, auditing, inspection, and disclosure at the discretion
nid001365: of authorized site or Department of Energy personnel.
nid001365: 
nid001365: Unauthorized or improper use of this system may result in administrative
nid001365: disciplinary action and civil and criminal penalties. By continuing to use
nid001365: this system you indicate your awareness of and consent to these terms and
nid001365: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001365: stated in this warning.
nid001365: 
nid001365: *****************************************************************************
nid001365: 
nid001365: Login connection to host x1001c3s3b0n1:
nid001365: 
nid002109: ***************************************************************************
nid002109:                           NOTICE TO USERS
nid002109: 
nid002109: Lawrence Berkeley National Laboratory operates this computer system under 
nid002109: contract to the U.S. Department of Energy.  This computer system is the 
nid002109: property of the United States Government and is for authorized use only.
nid002109: Users (authorized or unauthorized) have no explicit or implicit 
nid002109: expectation of privacy.
nid002109: 
nid002109: Any or all uses of this system and all files on this system may be
nid002109: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid002109: to authorized site, Department of Energy, and law enforcement personnel,
nid002109: as well as authorized officials of other agencies, both domestic and foreign.
nid002109: By using this system, the user consents to such interception, monitoring,
nid002109: recording, copying, auditing, inspection, and disclosure at the discretion
nid002109: of authorized site or Department of Energy personnel.
nid002109: 
nid002109: Unauthorized or improper use of this system may result in administrative
nid002109: disciplinary action and civil and criminal penalties. By continuing to use
nid002109: this system you indicate your awareness of and consent to these terms and
nid002109: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid002109: stated in this warning.
nid002109: 
nid002109: *****************************************************************************
nid002109: 
nid002109: Login connection to host x1100c2s5b0n1:
nid002109: 
nid001372: ***************************************************************************
nid001372:                           NOTICE TO USERS
nid001372: 
nid001372: Lawrence Berkeley National Laboratory operates this computer system under 
nid001372: contract to the U.S. Department of Energy.  This computer system is the 
nid001372: property of the United States Government and is for authorized use only.
nid001372: Users (authorized or unauthorized) have no explicit or implicit 
nid001372: expectation of privacy.
nid001372: 
nid001372: Any or all uses of this system and all files on this system may be
nid001372: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001372: to authorized site, Department of Energy, and law enforcement personnel,
nid001372: as well as authorized officials of other agencies, both domestic and foreign.
nid001372: By using this system, the user consents to such interception, monitoring,
nid001372: recording, copying, auditing, inspection, and disclosure at the discretion
nid001372: of authorized site or Department of Energy personnel.
nid001372: 
nid001372: Unauthorized or improper use of this system may result in administrative
nid001372: disciplinary action and civil and criminal penalties. By continuing to use
nid001372: this system you indicate your awareness of and consent to these terms and
nid001372: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001372: stated in this warning.
nid001372: 
nid001372: *****************************************************************************
nid001372: 
nid001372: Login connection to host x1001c3s5b0n0:
nid001372: 
nid003037: ***************************************************************************
nid003037:                           NOTICE TO USERS
nid003037: 
nid003037: Lawrence Berkeley National Laboratory operates this computer system under 
nid003037: contract to the U.S. Department of Energy.  This computer system is the 
nid003037: property of the United States Government and is for authorized use only.
nid003037: Users (authorized or unauthorized) have no explicit or implicit 
nid003037: expectation of privacy.
nid003037: 
nid003037: Any or all uses of this system and all files on this system may be
nid003037: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid003037: to authorized site, Department of Energy, and law enforcement personnel,
nid003037: as well as authorized officials of other agencies, both domestic and foreign.
nid003037: By using this system, the user consents to such interception, monitoring,
nid003037: recording, copying, auditing, inspection, and disclosure at the discretion
nid003037: of authorized site or Department of Energy personnel.
nid003037: 
nid003037: Unauthorized or improper use of this system may result in administrative
nid003037: disciplinary action and civil and criminal penalties. By continuing to use
nid003037: this system you indicate your awareness of and consent to these terms and
nid003037: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid003037: stated in this warning.
nid003037: 
nid003037: *****************************************************************************
nid003037: 
nid003037: Login connection to host x1103c7s5b0n1:
nid003037: 
nid002265: ***************************************************************************
nid002265:                           NOTICE TO USERS
nid002265: 
nid002265: Lawrence Berkeley National Laboratory operates this computer system under 
nid002265: contract to the U.S. Department of Energy.  This computer system is the 
nid002265: property of the United States Government and is for authorized use only.
nid002265: Users (authorized or unauthorized) have no explicit or implicit 
nid002265: expectation of privacy.
nid002265: 
nid002265: Any or all uses of this system and all files on this system may be
nid002265: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid002265: to authorized site, Department of Energy, and law enforcement personnel,
nid002265: as well as authorized officials of other agencies, both domestic and foreign.
nid002265: By using this system, the user consents to such interception, monitoring,
nid002265: recording, copying, auditing, inspection, and disclosure at the discretion
nid002265: of authorized site or Department of Energy personnel.
nid002265: 
nid002265: Unauthorized or improper use of this system may result in administrative
nid002265: disciplinary action and civil and criminal penalties. By continuing to use
nid002265: this system you indicate your awareness of and consent to these terms and
nid002265: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid002265: stated in this warning.
nid002265: 
nid002265: *****************************************************************************
nid002265: 
nid002265: Login connection to host x1100c7s4b0n1:
nid002265: 
nid001368: ***************************************************************************
nid001368:                           NOTICE TO USERS
nid001368: 
nid001368: Lawrence Berkeley National Laboratory operates this computer system under 
nid001368: contract to the U.S. Department of Energy.  This computer system is the 
nid001368: property of the United States Government and is for authorized use only.
nid001368: Users (authorized or unauthorized) have no explicit or implicit 
nid001368: expectation of privacy.
nid001368: 
nid001368: Any or all uses of this system and all files on this system may be
nid001368: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001368: to authorized site, Department of Energy, and law enforcement personnel,
nid001368: as well as authorized officials of other agencies, both domestic and foreign.
nid001368: By using this system, the user consents to such interception, monitoring,
nid001368: recording, copying, auditing, inspection, and disclosure at the discretion
nid001368: of authorized site or Department of Energy personnel.
nid001368: 
nid001368: Unauthorized or improper use of this system may result in administrative
nid001368: disciplinary action and civil and criminal penalties. By continuing to use
nid001368: this system you indicate your awareness of and consent to these terms and
nid001368: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001368: stated in this warning.
nid001368: 
nid001368: *****************************************************************************
nid001368: 
nid001368: Login connection to host x1001c3s4b0n0:
nid001368: 
nid002261: ***************************************************************************
nid002261:                           NOTICE TO USERS
nid002261: 
nid002261: Lawrence Berkeley National Laboratory operates this computer system under 
nid002261: contract to the U.S. Department of Energy.  This computer system is the 
nid002261: property of the United States Government and is for authorized use only.
nid002261: Users (authorized or unauthorized) have no explicit or implicit 
nid002261: expectation of privacy.
nid002261: 
nid002261: Any or all uses of this system and all files on this system may be
nid002261: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid002261: to authorized site, Department of Energy, and law enforcement personnel,
nid002261: as well as authorized officials of other agencies, both domestic and foreign.
nid002261: By using this system, the user consents to such interception, monitoring,
nid002261: recording, copying, auditing, inspection, and disclosure at the discretion
nid002261: of authorized site or Department of Energy personnel.
nid002261: 
nid002261: Unauthorized or improper use of this system may result in administrative
nid002261: disciplinary action and civil and criminal penalties. By continuing to use
nid002261: this system you indicate your awareness of and consent to these terms and
nid002261: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid002261: stated in this warning.
nid002261: 
nid002261: *****************************************************************************
nid002261: 
nid002261: Login connection to host x1100c7s3b0n1:
nid002261: 
nid001364: [2024-10-09 21:40:11,009] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002268: [2024-10-09 21:40:12,184] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002264: [2024-10-09 21:40:12,588] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: [2024-10-09 21:40:12,894] [INFO] [launch.py:138:main] 0 NCCL_NET_GDR_LEVEL=PHB
nid001364: [2024-10-09 21:40:12,894] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=hsn
nid001364: [2024-10-09 21:40:12,894] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid001364: [2024-10-09 21:40:12,894] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=0
nid001364: [2024-10-09 21:40:12,894] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid001364: [2024-10-09 21:40:12,895] [INFO] [launch.py:163:main] dist_world_size=64
nid001364: [2024-10-09 21:40:12,895] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid002424: [2024-10-09 21:40:13,500] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001373: [2024-10-09 21:40:14,779] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001365: [2024-10-09 21:40:14,792] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002261: [2024-10-09 21:40:14,800] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: [2024-10-09 21:40:14,842] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: [2024-10-09 21:40:14,846] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: [2024-10-09 21:40:14,863] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: [2024-10-09 21:40:14,867] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001376: [2024-10-09 21:40:15,466] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002109: [2024-10-09 21:40:16,085] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002268: [2024-10-09 21:40:16,143] [INFO] [launch.py:138:main] 12 NCCL_NET_GDR_LEVEL=PHB
nid002268: [2024-10-09 21:40:16,143] [INFO] [launch.py:138:main] 12 NCCL_SOCKET_IFNAME=hsn
nid002268: [2024-10-09 21:40:16,143] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid002268: [2024-10-09 21:40:16,143] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=12
nid002268: [2024-10-09 21:40:16,143] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid002268: [2024-10-09 21:40:16,143] [INFO] [launch.py:163:main] dist_world_size=64
nid002268: [2024-10-09 21:40:16,143] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001372: [2024-10-09 21:40:16,221] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002264: [2024-10-09 21:40:16,452] [INFO] [launch.py:138:main] 10 NCCL_NET_GDR_LEVEL=PHB
nid002264: [2024-10-09 21:40:16,452] [INFO] [launch.py:138:main] 10 NCCL_SOCKET_IFNAME=hsn
nid002264: [2024-10-09 21:40:16,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid002264: [2024-10-09 21:40:16,452] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=10
nid002264: [2024-10-09 21:40:16,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid002264: [2024-10-09 21:40:16,452] [INFO] [launch.py:163:main] dist_world_size=64
nid002264: [2024-10-09 21:40:16,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid003037: [2024-10-09 21:40:16,605] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001368: [2024-10-09 21:40:17,345] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002424: [2024-10-09 21:40:17,358] [INFO] [launch.py:138:main] 13 NCCL_NET_GDR_LEVEL=PHB
nid002424: [2024-10-09 21:40:17,358] [INFO] [launch.py:138:main] 13 NCCL_SOCKET_IFNAME=hsn
nid002424: [2024-10-09 21:40:17,358] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid002424: [2024-10-09 21:40:17,358] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=13
nid002424: [2024-10-09 21:40:17,358] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid002424: [2024-10-09 21:40:17,358] [INFO] [launch.py:163:main] dist_world_size=64
nid002424: [2024-10-09 21:40:17,358] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid002265: [2024-10-09 21:40:17,424] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001364: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001364: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001364: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001364: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001364: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001364: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001364: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001364: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001364: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001364: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001364: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001369: [2024-10-09 21:40:18,004] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: STAGE:2024-10-09 21:40:18 817968:817968 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:18 817967:817967 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:18 817966:817966 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001364: 
nid001364: STAGE:2024-10-09 21:40:18 817969:817969 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001364: 
nid002268: [2024-10-09 21:40:18,166] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002268: [2024-10-09 21:40:18,205] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002268: [2024-10-09 21:40:18,208] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002268: [2024-10-09 21:40:18,209] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002264: [2024-10-09 21:40:18,457] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002264: [2024-10-09 21:40:18,470] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002264: [2024-10-09 21:40:18,474] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002264: [2024-10-09 21:40:18,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001373: [2024-10-09 21:40:18,655] [INFO] [launch.py:138:main] 5 NCCL_NET_GDR_LEVEL=PHB
nid001373: [2024-10-09 21:40:18,655] [INFO] [launch.py:138:main] 5 NCCL_SOCKET_IFNAME=hsn
nid001373: [2024-10-09 21:40:18,655] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid001373: [2024-10-09 21:40:18,655] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=5
nid001373: [2024-10-09 21:40:18,655] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid001373: [2024-10-09 21:40:18,655] [INFO] [launch.py:163:main] dist_world_size=64
nid001373: [2024-10-09 21:40:18,655] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid002261: [2024-10-09 21:40:18,663] [INFO] [launch.py:138:main] 9 NCCL_NET_GDR_LEVEL=PHB
nid002261: [2024-10-09 21:40:18,663] [INFO] [launch.py:138:main] 9 NCCL_SOCKET_IFNAME=hsn
nid002261: [2024-10-09 21:40:18,663] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid002261: [2024-10-09 21:40:18,663] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=9
nid002261: [2024-10-09 21:40:18,663] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid002261: [2024-10-09 21:40:18,663] [INFO] [launch.py:163:main] dist_world_size=64
nid002261: [2024-10-09 21:40:18,663] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001365: [2024-10-09 21:40:18,769] [INFO] [launch.py:138:main] 1 NCCL_NET_GDR_LEVEL=PHB
nid001365: [2024-10-09 21:40:18,770] [INFO] [launch.py:138:main] 1 NCCL_SOCKET_IFNAME=hsn
nid001365: [2024-10-09 21:40:18,770] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid001365: [2024-10-09 21:40:18,770] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=1
nid001365: [2024-10-09 21:40:18,770] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid001365: [2024-10-09 21:40:18,770] [INFO] [launch.py:163:main] dist_world_size=64
nid001365: [2024-10-09 21:40:18,770] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid003209: [2024-10-09 21:40:19,097] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001377: [2024-10-09 21:40:19,222] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: STAGE:2024-10-09 21:40:19 817967:817967 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001364: STAGE:2024-10-09 21:40:19 817969:817969 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001364: STAGE:2024-10-09 21:40:19 817968:817968 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:19 817966:817966 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001364: 
nid001364: NeoXArgs.configure_distributed_args() using world size: 64 and model-parallel size: 8 
nid001364: > building GPT2BPETokenizer tokenizer ...
nid001364:  > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
nid002424: [2024-10-09 21:40:19,397] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002424: [2024-10-09 21:40:19,415] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001376: [2024-10-09 21:40:19,428] [INFO] [launch.py:138:main] 6 NCCL_NET_GDR_LEVEL=PHB
nid001376: [2024-10-09 21:40:19,428] [INFO] [launch.py:138:main] 6 NCCL_SOCKET_IFNAME=hsn
nid001376: [2024-10-09 21:40:19,428] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid001376: [2024-10-09 21:40:19,428] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=6
nid001376: [2024-10-09 21:40:19,428] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid001376: [2024-10-09 21:40:19,428] [INFO] [launch.py:163:main] dist_world_size=64
nid001376: [2024-10-09 21:40:19,428] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid002424: [2024-10-09 21:40:19,435] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002424: [2024-10-09 21:40:19,436] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001364: > setting up tensorboard ...
nid002109: [2024-10-09 21:40:19,930] [INFO] [launch.py:138:main] 8 NCCL_NET_GDR_LEVEL=PHB
nid002109: [2024-10-09 21:40:19,930] [INFO] [launch.py:138:main] 8 NCCL_SOCKET_IFNAME=hsn
nid002109: [2024-10-09 21:40:19,930] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid002109: [2024-10-09 21:40:19,930] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=8
nid002109: [2024-10-09 21:40:19,930] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid002109: [2024-10-09 21:40:19,930] [INFO] [launch.py:163:main] dist_world_size=64
nid002109: [2024-10-09 21:40:19,930] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001372: [2024-10-09 21:40:20,095] [INFO] [launch.py:138:main] 4 NCCL_NET_GDR_LEVEL=PHB
nid001372: [2024-10-09 21:40:20,095] [INFO] [launch.py:138:main] 4 NCCL_SOCKET_IFNAME=hsn
nid001372: [2024-10-09 21:40:20,095] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid001372: [2024-10-09 21:40:20,095] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=4
nid001372: [2024-10-09 21:40:20,095] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid001372: [2024-10-09 21:40:20,095] [INFO] [launch.py:163:main] dist_world_size=64
nid001372: [2024-10-09 21:40:20,095] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid002261: [2024-10-09 21:40:20,701] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002261: [2024-10-09 21:40:20,707] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002261: [2024-10-09 21:40:20,710] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001373: [2024-10-09 21:40:20,712] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002261: [2024-10-09 21:40:20,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003037: [2024-10-09 21:40:20,716] [INFO] [launch.py:138:main] 14 NCCL_NET_GDR_LEVEL=PHB
nid003037: [2024-10-09 21:40:20,716] [INFO] [launch.py:138:main] 14 NCCL_SOCKET_IFNAME=hsn
nid003037: [2024-10-09 21:40:20,716] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid003037: [2024-10-09 21:40:20,716] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=14
nid003037: [2024-10-09 21:40:20,716] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid003037: [2024-10-09 21:40:20,716] [INFO] [launch.py:163:main] dist_world_size=64
nid003037: [2024-10-09 21:40:20,716] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001373: [2024-10-09 21:40:20,726] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001373: [2024-10-09 21:40:20,740] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001373: [2024-10-09 21:40:20,742] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001365: [2024-10-09 21:40:20,781] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001365: [2024-10-09 21:40:20,815] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001365: [2024-10-09 21:40:20,819] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001365: [2024-10-09 21:40:20,823] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002265: [2024-10-09 21:40:21,274] [INFO] [launch.py:138:main] 11 NCCL_NET_GDR_LEVEL=PHB
nid002265: [2024-10-09 21:40:21,274] [INFO] [launch.py:138:main] 11 NCCL_SOCKET_IFNAME=hsn
nid002265: [2024-10-09 21:40:21,274] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid002265: [2024-10-09 21:40:21,274] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=11
nid002265: [2024-10-09 21:40:21,274] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid002265: [2024-10-09 21:40:21,274] [INFO] [launch.py:163:main] dist_world_size=64
nid002265: [2024-10-09 21:40:21,274] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001368: [2024-10-09 21:40:21,281] [INFO] [launch.py:138:main] 2 NCCL_NET_GDR_LEVEL=PHB
nid001368: [2024-10-09 21:40:21,281] [INFO] [launch.py:138:main] 2 NCCL_SOCKET_IFNAME=hsn
nid001368: [2024-10-09 21:40:21,281] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid001368: [2024-10-09 21:40:21,281] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=2
nid001368: [2024-10-09 21:40:21,281] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid001368: [2024-10-09 21:40:21,281] [INFO] [launch.py:163:main] dist_world_size=64
nid001368: [2024-10-09 21:40:21,281] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001376: [2024-10-09 21:40:21,466] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001376: [2024-10-09 21:40:21,501] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001376: [2024-10-09 21:40:21,505] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001376: [2024-10-09 21:40:21,508] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001369: [2024-10-09 21:40:21,898] [INFO] [launch.py:138:main] 3 NCCL_NET_GDR_LEVEL=PHB
nid001369: [2024-10-09 21:40:21,898] [INFO] [launch.py:138:main] 3 NCCL_SOCKET_IFNAME=hsn
nid001369: [2024-10-09 21:40:21,898] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid001369: [2024-10-09 21:40:21,898] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=3
nid001369: [2024-10-09 21:40:21,898] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid001369: [2024-10-09 21:40:21,898] [INFO] [launch.py:163:main] dist_world_size=64
nid001369: [2024-10-09 21:40:21,898] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid002268: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002268: 
nid002268: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002268: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002268: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002268: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002268: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002268: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002268: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002268: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002268: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002268: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002109: [2024-10-09 21:40:21,975] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002109: [2024-10-09 21:40:21,982] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002109: [2024-10-09 21:40:21,983] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002109: [2024-10-09 21:40:21,986] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002268: STAGE:2024-10-09 21:40:22 637367:637367 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002268: STAGE:2024-10-09 21:40:22 637364:637364 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002268: STAGE:2024-10-09 21:40:22 637366:637366 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002268: STAGE:2024-10-09 21:40:22 637365:637365 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001372: [2024-10-09 21:40:22,138] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001372: [2024-10-09 21:40:22,161] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001372: [2024-10-09 21:40:22,164] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001372: [2024-10-09 21:40:22,168] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002264: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002264: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002264: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002264: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002264: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002264: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002264: 
nid002264: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002264: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002264: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002264: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002264: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002264: STAGE:2024-10-09 21:40:22 1280120:1280120 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:22 1280121:1280121 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002264: 
nid002264: STAGE:2024-10-09 21:40:22 1280119:1280119 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002264: STAGE:2024-10-09 21:40:22 1280122:1280122 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003037: [2024-10-09 21:40:22,750] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003037: [2024-10-09 21:40:22,757] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003037: [2024-10-09 21:40:22,763] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003037: [2024-10-09 21:40:22,764] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003209: [2024-10-09 21:40:23,113] [INFO] [launch.py:138:main] 15 NCCL_NET_GDR_LEVEL=PHB
nid003209: [2024-10-09 21:40:23,113] [INFO] [launch.py:138:main] 15 NCCL_SOCKET_IFNAME=hsn
nid003209: [2024-10-09 21:40:23,113] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid003209: [2024-10-09 21:40:23,113] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=15
nid003209: [2024-10-09 21:40:23,113] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid003209: [2024-10-09 21:40:23,113] [INFO] [launch.py:163:main] dist_world_size=64
nid003209: [2024-10-09 21:40:23,113] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001377: [2024-10-09 21:40:23,227] [INFO] [launch.py:138:main] 7 NCCL_NET_GDR_LEVEL=PHB
nid001377: [2024-10-09 21:40:23,227] [INFO] [launch.py:138:main] 7 NCCL_SOCKET_IFNAME=hsn
nid001377: [2024-10-09 21:40:23,227] [INFO] [launch.py:145:main] WORLD INFO DICT: {'nid001364': [0, 1, 2, 3], 'nid001365': [0, 1, 2, 3], 'nid001368': [0, 1, 2, 3], 'nid001369': [0, 1, 2, 3], 'nid001372': [0, 1, 2, 3], 'nid001373': [0, 1, 2, 3], 'nid001376': [0, 1, 2, 3], 'nid001377': [0, 1, 2, 3], 'nid002109': [0, 1, 2, 3], 'nid002261': [0, 1, 2, 3], 'nid002264': [0, 1, 2, 3], 'nid002265': [0, 1, 2, 3], 'nid002268': [0, 1, 2, 3], 'nid002424': [0, 1, 2, 3], 'nid003037': [0, 1, 2, 3], 'nid003209': [0, 1, 2, 3]}
nid001377: [2024-10-09 21:40:23,227] [INFO] [launch.py:151:main] nnodes=16, num_local_procs=4, node_rank=7
nid001377: [2024-10-09 21:40:23,227] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001364': [0, 1, 2, 3], 'nid001365': [4, 5, 6, 7], 'nid001368': [8, 9, 10, 11], 'nid001369': [12, 13, 14, 15], 'nid001372': [16, 17, 18, 19], 'nid001373': [20, 21, 22, 23], 'nid001376': [24, 25, 26, 27], 'nid001377': [28, 29, 30, 31], 'nid002109': [32, 33, 34, 35], 'nid002261': [36, 37, 38, 39], 'nid002264': [40, 41, 42, 43], 'nid002265': [44, 45, 46, 47], 'nid002268': [48, 49, 50, 51], 'nid002424': [52, 53, 54, 55], 'nid003037': [56, 57, 58, 59], 'nid003209': [60, 61, 62, 63]})
nid001377: [2024-10-09 21:40:23,227] [INFO] [launch.py:163:main] dist_world_size=64
nid001377: [2024-10-09 21:40:23,227] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid002268: STAGE:2024-10-09 21:40:23 637365:637365 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002268: STAGE:2024-10-09 21:40:23 637366:637366 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:23 637364:637364 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002268: 
nid002268: STAGE:2024-10-09 21:40:23 637367:637367 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001368: [2024-10-09 21:40:23,316] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001368: [2024-10-09 21:40:23,332] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002264: STAGE:2024-10-09 21:40:23 1280120:1280120 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002264: STAGE:2024-10-09 21:40:23 1280122:1280122 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:23 1280121:1280121 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002264: 
nid002264: STAGE:2024-10-09 21:40:23 1280119:1280119 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001368: [2024-10-09 21:40:23,336] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002265: [2024-10-09 21:40:23,337] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001368: [2024-10-09 21:40:23,341] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002265: [2024-10-09 21:40:23,355] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002265: [2024-10-09 21:40:23,360] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002265: [2024-10-09 21:40:23,362] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002424: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002424: 
nid002424: 
nid002424: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002424: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002424: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002424: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002424: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002424: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002424: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002424: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002424: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002424: STAGE:2024-10-09 21:40:23 46167:46167 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002424: STAGE:2024-10-09 21:40:23 46168:46168 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002424: STAGE:2024-10-09 21:40:23 46166:46166 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002424: STAGE:2024-10-09 21:40:23 46165:46165 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001369: [2024-10-09 21:40:23,940] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001369: [2024-10-09 21:40:23,942] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001369: [2024-10-09 21:40:23,948] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001369: [2024-10-09 21:40:23,949] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid002261: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002261: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002261: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002261: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002261: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002261: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002261: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002261: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002261: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002261: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002261: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002261: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001365: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001365: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001365: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001365: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001365: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001365: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001365: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001365: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001365: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001365: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001365: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001365: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002261: STAGE:2024-10-09 21:40:24 1199359:1199359 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002261: STAGE:2024-10-09 21:40:24 1199356:1199356 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002261: STAGE:2024-10-09 21:40:24 1199357:1199357 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002261: STAGE:2024-10-09 21:40:24 1199358:1199358 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001373: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001373: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001373: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001373: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001373: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001373: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001373: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001373: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001373: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001373: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001373: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001373: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001365: STAGE:2024-10-09 21:40:24 1654931:1654931 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:24 1654928:1654928 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001365: 
nid001365: STAGE:2024-10-09 21:40:24 1654929:1654929 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001365: STAGE:2024-10-09 21:40:24 1654930:1654930 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001373: STAGE:2024-10-09 21:40:24 715980:715980 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:24 715982:715982 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001373: 
nid001373: STAGE:2024-10-09 21:40:24 715981:715981 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001373: STAGE:2024-10-09 21:40:24 715983:715983 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002424: STAGE:2024-10-09 21:40:24 46168:46168 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:24 46167:46167 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:24 46165:46165 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002424: 
nid002424: 
nid002424: STAGE:2024-10-09 21:40:24 46166:46166 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003209: [2024-10-09 21:40:25,168] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003209: [2024-10-09 21:40:25,174] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003209: [2024-10-09 21:40:25,177] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid003209: [2024-10-09 21:40:25,179] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001376: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001376: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001376: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001376: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001376: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001376: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001376: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001376: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001376: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001376: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001376: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001376: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001377: [2024-10-09 21:40:25,281] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001377: [2024-10-09 21:40:25,305] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001377: [2024-10-09 21:40:25,308] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001377: [2024-10-09 21:40:25,312] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001376: STAGE:2024-10-09 21:40:25 1245730:1245730 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:25 1245729:1245729 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:25 1245728:1245728 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001376: 
nid001376: 
nid001376: STAGE:2024-10-09 21:40:25 1245727:1245727 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002261: STAGE:2024-10-09 21:40:25 1199356:1199356 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002261: STAGE:2024-10-09 21:40:25 1199359:1199359 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:25 1199357:1199357 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002261: STAGE:2024-10-09 21:40:25 1199358:1199358 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002261: 
nid001365: STAGE:2024-10-09 21:40:25 1654930:1654930 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:25 1654931:1654931 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001365: 
nid001365: STAGE:2024-10-09 21:40:25 1654928:1654928 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001365: STAGE:2024-10-09 21:40:25 1654929:1654929 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001373: STAGE:2024-10-09 21:40:25 715981:715981 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001373: STAGE:2024-10-09 21:40:25 715982:715982 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001373: STAGE:2024-10-09 21:40:25 715980:715980 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001373: STAGE:2024-10-09 21:40:25 715983:715983 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002109: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002109: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002109: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002109: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002109: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002109: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002109: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002109: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002109: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002109: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002109: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002109: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002109: STAGE:2024-10-09 21:40:26 1194104:1194104 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002109: STAGE:2024-10-09 21:40:26 1194102:1194102 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002109: STAGE:2024-10-09 21:40:26 1194103:1194103 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002109: STAGE:2024-10-09 21:40:26 1194105:1194105 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001372: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001372: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001372: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001372: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001372: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001372: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001372: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001372: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001372: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001372: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001372: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001372: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001372: STAGE:2024-10-09 21:40:26 1207265:1207265 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001372: STAGE:2024-10-09 21:40:26 1207264:1207264 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001372: STAGE:2024-10-09 21:40:26 1207267:1207267 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001372: STAGE:2024-10-09 21:40:26 1207266:1207266 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001376: STAGE:2024-10-09 21:40:26 1245729:1245729 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001376: STAGE:2024-10-09 21:40:26 1245727:1245727 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001376: STAGE:2024-10-09 21:40:26 1245728:1245728 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001376: STAGE:2024-10-09 21:40:26 1245730:1245730 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003037: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003037: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003037: 
nid003037: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003037: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003037: 
nid003037: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003037: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003037: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transferFor s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003037: 
nid003037: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003037: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003037: STAGE:2024-10-09 21:40:26 1969246:1969246 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003037: STAGE:2024-10-09 21:40:26 1969248:1969248 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003037: STAGE:2024-10-09 21:40:26 1969245:1969245 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:26 1969247:1969247 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003037: 
nid002109: STAGE:2024-10-09 21:40:27 1194103:1194103 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:27 1194104:1194104 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002109: 
nid002109: STAGE:2024-10-09 21:40:27 1194105:1194105 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:27 1194102:1194102 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002109: 
nid001368: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001368: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001368: 
nid001368: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001368: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001368: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001368: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001368: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001368: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001368: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001368: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001368: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001368: STAGE:2024-10-09 21:40:27 341074:341074 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:27 341076:341076 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:27 341075:341075 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001368: 
nid001368: STAGE:2024-10-09 21:40:27 341077:341077 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001368: 
nid001372: STAGE:2024-10-09 21:40:27 1207265:1207265 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:27 1207264:1207264 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001372: 
nid001372: STAGE:2024-10-09 21:40:27 1207266:1207266 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001372: STAGE:2024-10-09 21:40:27 1207267:1207267 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002265: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002265: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002265: 
nid002265: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid002265: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002265: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002265: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002265: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid002265: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002265: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002265: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid002265: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001369: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001369: 
nid001369: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001369: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001369: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001369: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001369: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001369: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001369: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001369: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001369: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001369: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003037: STAGE:2024-10-09 21:40:27 1969245:1969245 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:27 1969248:1969248 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:27 1969247:1969247 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003037: 
nid003037: STAGE:2024-10-09 21:40:27 1969246:1969246 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003037: 
nid002265: STAGE:2024-10-09 21:40:27 2229371:2229371 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:27 2229370:2229370 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002265: 
nid002265: STAGE:2024-10-09 21:40:27 2229372:2229372 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid002265: STAGE:2024-10-09 21:40:27 2229373:2229373 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001369: STAGE:2024-10-09 21:40:27 2036379:2036379 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001369: STAGE:2024-10-09 21:40:27 2036380:2036380 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001369: STAGE:2024-10-09 21:40:27 2036381:2036381 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001369: STAGE:2024-10-09 21:40:27 2036378:2036378 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001368: STAGE:2024-10-09 21:40:28 341075:341075 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:28 341076:341076 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001368: 
nid001368: STAGE:2024-10-09 21:40:28 341074:341074 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:28 341077:341077 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001368: 
nid002265: STAGE:2024-10-09 21:40:28 2229373:2229373 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:28 2229370:2229370 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:28 2229372:2229372 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:28 2229371:2229371 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid002265: 
nid002265: 
nid002265: 
nid001369: STAGE:2024-10-09 21:40:28 2036380:2036380 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001369: STAGE:2024-10-09 21:40:28 2036381:2036381 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:28 2036378:2036378 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001369: 
nid001369: STAGE:2024-10-09 21:40:28 2036379:2036379 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001377: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001377: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001377: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001377: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid001377: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001377: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001377: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001377: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid001377: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001377: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001377: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001377: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003209: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003209: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003209: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003209: Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
nid003209: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003209: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003209: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003209: For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
nid003209: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003209: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003209: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid003209: For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
nid001377: STAGE:2024-10-09 21:40:29 378729:378729 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:29 378727:378727 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001377: 
nid001377: STAGE:2024-10-09 21:40:29 378728:378728 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001377: STAGE:2024-10-09 21:40:29 378726:378726 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003209: STAGE:2024-10-09 21:40:29 1846364:1846364 ActivityProfilerController.cpp:294] Completed Stage: Warm UpSTAGE:2024-10-09 21:40:29 1846361:1846361 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003209: 
nid003209: STAGE:2024-10-09 21:40:29 1846362:1846362 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003209: STAGE:2024-10-09 21:40:29 1846363:1846363 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001377: STAGE:2024-10-09 21:40:30 378729:378729 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:30 378727:378727 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:30 378728:378728 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001377: 
nid001377: 
nid001377: STAGE:2024-10-09 21:40:30 378726:378726 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003209: STAGE:2024-10-09 21:40:30 1846364:1846364 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003209: STAGE:2024-10-09 21:40:30 1846363:1846363 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:30 1846361:1846361 ActivityProfilerController.cpp:300] Completed Stage: CollectionSTAGE:2024-10-09 21:40:30 1846362:1846362 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003209: 
nid003209: 
nid001364: [2024-10-09 21:46:50,303] [INFO] [comm.py:637:init_distributed] cdb=None
nid001376: [2024-10-09 21:46:50,488] [INFO] [comm.py:637:init_distributed] cdb=None
nid001376: [2024-10-09 21:46:50,763] [INFO] [comm.py:637:init_distributed] cdb=None
nid003037: [2024-10-09 21:46:50,934] [INFO] [comm.py:637:init_distributed] cdb=None
nid002265: [2024-10-09 21:46:50,934] [INFO] [comm.py:637:init_distributed] cdb=None
nid003037: [2024-10-09 21:46:51,070] [INFO] [comm.py:637:init_distributed] cdb=None
nid003209: [2024-10-09 21:46:51,143] [INFO] [comm.py:637:init_distributed] cdb=None
nid002264: [2024-10-09 21:46:51,326] [INFO] [comm.py:637:init_distributed] cdb=None
nid002109: [2024-10-09 21:46:51,485] [INFO] [comm.py:637:init_distributed] cdb=None
nid001368: [2024-10-09 21:46:51,685] [INFO] [comm.py:637:init_distributed] cdb=None
nid002268: [2024-10-09 21:46:51,765] [INFO] [comm.py:637:init_distributed] cdb=None
nid002264: [2024-10-09 21:46:51,840] [INFO] [comm.py:637:init_distributed] cdb=None
nid002109: [2024-10-09 21:46:51,840] [INFO] [comm.py:637:init_distributed] cdb=None
nid002424: [2024-10-09 21:46:52,019] [INFO] [comm.py:637:init_distributed] cdb=None
nid001365: [2024-10-09 21:46:52,190] [INFO] [comm.py:637:init_distributed] cdb=None
nid002261: [2024-10-09 21:46:52,196] [INFO] [comm.py:637:init_distributed] cdb=None
nid002424: [2024-10-09 21:46:52,360] [INFO] [comm.py:637:init_distributed] cdb=None
nid002424: [2024-10-09 21:46:52,361] [INFO] [comm.py:637:init_distributed] cdb=None
nid001364: [2024-10-09 21:46:52,363] [INFO] [comm.py:637:init_distributed] cdb=None
nid001372: [2024-10-09 21:46:52,365] [INFO] [comm.py:637:init_distributed] cdb=None
nid002264: [2024-10-09 21:46:52,527] [INFO] [comm.py:637:init_distributed] cdb=None
nid001364: Loading extension module scaled_upper_triang_masked_softmax_cuda...
nid001364: Detected CUDA files, patching ldflags
nid001364: Emitting ninja build file /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/fused_kernels/build/build.ninja...
nid001364: Building extension module scaled_masked_softmax_cuda...
nid001364: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
nid001364: ninja: no work to do.
nid001364: Loading extension module scaled_masked_softmax_cuda...
nid001364: Detected CUDA files, patching ldflags
nid001364: Emitting ninja build file /pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/fused_kernels/build/build.ninja...
nid001364: Building extension module fused_rotary_positional_embedding...
nid001364: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
nid001364: ninja: no work to do.
nid001364: Loading extension module fused_rotary_positional_embedding...
nid001364: > initializing torch distributed ...
nid001364: [2024-10-09 21:46:52,691] [INFO] [comm.py:637:init_distributed] cdb=None
nid001364: [2024-10-09 21:46:52,691] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
nid001364: [2024-10-09 21:46:52,702] [INFO] [comm.py:637:init_distributed] cdb=None
nid002268: [2024-10-09 21:46:52,796] [INFO] [comm.py:637:init_distributed] cdb=None
nid002268: [2024-10-09 21:46:52,804] [INFO] [comm.py:637:init_distributed] cdb=None
nid001372: [2024-10-09 21:46:52,924] [INFO] [comm.py:637:init_distributed] cdb=None
nid002268: [2024-10-09 21:46:52,936] [INFO] [comm.py:637:init_distributed] cdb=None
nid001376: [2024-10-09 21:46:52,938] [INFO] [comm.py:637:init_distributed] cdb=None
nid001376: [2024-10-09 21:46:52,938] [INFO] [comm.py:637:init_distributed] cdb=None
nid002265: [2024-10-09 21:46:53,049] [INFO] [comm.py:637:init_distributed] cdb=None
nid001369: [2024-10-09 21:46:53,062] [INFO] [comm.py:637:init_distributed] cdb=None
nid001365: [2024-10-09 21:46:53,149] [INFO] [comm.py:637:init_distributed] cdb=None
nid001365: [2024-10-09 21:46:53,163] [INFO] [comm.py:637:init_distributed] cdb=None
nid003209: [2024-10-09 21:46:53,163] [INFO] [comm.py:637:init_distributed] cdb=None
nid002424: [2024-10-09 21:46:53,248] [INFO] [comm.py:637:init_distributed] cdb=None
nid001373: [2024-10-09 21:46:53,278] [INFO] [comm.py:637:init_distributed] cdb=None
nid003037: [2024-10-09 21:46:53,357] [INFO] [comm.py:637:init_distributed] cdb=None
nid002261: [2024-10-09 21:46:53,481] [INFO] [comm.py:637:init_distributed] cdb=None
nid002261: [2024-10-09 21:46:53,625] [INFO] [comm.py:637:init_distributed] cdb=None
nid001369: [2024-10-09 21:46:53,649] [INFO] [comm.py:637:init_distributed] cdb=None
nid003037: [2024-10-09 21:46:53,724] [INFO] [comm.py:637:init_distributed] cdb=None
nid002109: [2024-10-09 21:46:53,751] [INFO] [comm.py:637:init_distributed] cdb=None
nid002109: [2024-10-09 21:46:53,752] [INFO] [comm.py:637:init_distributed] cdb=None
nid003209: [2024-10-09 21:46:53,752] [INFO] [comm.py:637:init_distributed] cdb=None
nid001369: [2024-10-09 21:46:53,840] [INFO] [comm.py:637:init_distributed] cdb=None
nid001369: [2024-10-09 21:46:53,853] [INFO] [comm.py:637:init_distributed] cdb=None
nid001365: [2024-10-09 21:46:53,853] [INFO] [comm.py:637:init_distributed] cdb=None
nid003209: [2024-10-09 21:46:53,854] [INFO] [comm.py:637:init_distributed] cdb=None
nid002261: [2024-10-09 21:46:53,965] [INFO] [comm.py:637:init_distributed] cdb=None
nid002264: [2024-10-09 21:46:53,992] [INFO] [comm.py:637:init_distributed] cdb=None
nid001368: [2024-10-09 21:46:53,995] [INFO] [comm.py:637:init_distributed] cdb=None
nid001368: [2024-10-09 21:46:54,069] [INFO] [comm.py:637:init_distributed] cdb=None
nid001372: [2024-10-09 21:46:54,098] [INFO] [comm.py:637:init_distributed] cdb=None
nid001377: [2024-10-09 21:46:54,102] [INFO] [comm.py:637:init_distributed] cdb=None
nid001368: [2024-10-09 21:46:54,168] [INFO] [comm.py:637:init_distributed] cdb=None
nid001377: [2024-10-09 21:46:54,303] [INFO] [comm.py:637:init_distributed] cdb=None
nid001377: [2024-10-09 21:46:54,304] [INFO] [comm.py:637:init_distributed] cdb=None
nid001377: [2024-10-09 21:46:54,304] [INFO] [comm.py:637:init_distributed] cdb=None
nid001372: [2024-10-09 21:46:54,313] [INFO] [comm.py:637:init_distributed] cdb=None
nid002265: [2024-10-09 21:46:54,322] [INFO] [comm.py:637:init_distributed] cdb=None
nid002265: [2024-10-09 21:46:54,397] [INFO] [comm.py:637:init_distributed] cdb=None
nid001373: [2024-10-09 21:46:54,515] [INFO] [comm.py:637:init_distributed] cdb=None
nid001373: [2024-10-09 21:46:54,527] [INFO] [comm.py:637:init_distributed] cdb=None
nid001373: [2024-10-09 21:46:54,528] [INFO] [comm.py:637:init_distributed] cdb=None
nid001364: > initializing model parallel with size 8
nid001364: MPU DP: [0, 8]
nid001364: MPU DP: [1, 9]
nid001364: MPU DP: [2, 10]
nid001364: MPU DP: [3, 11]
nid001364: MPU DP: [4, 12]
nid001364: MPU DP: [5, 13]
nid001364: MPU DP: [6, 14]
nid001364: MPU DP: [7, 15]
nid001364: MPU DP: [16, 24]
nid001364: MPU DP: [17, 25]
nid001364: MPU DP: [18, 26]
nid001364: MPU DP: [19, 27]
nid001364: MPU DP: [20, 28]
nid001364: MPU DP: [21, 29]
nid001364: MPU DP: [22, 30]
nid001364: MPU DP: [23, 31]
nid001364: MPU DP: [32, 40]
nid001364: MPU DP: [33, 41]
nid001364: MPU DP: [34, 42]
nid001364: MPU DP: [35, 43]
nid001364: MPU DP: [36, 44]
nid001364: MPU DP: [37, 45]
nid001364: MPU DP: [38, 46]
nid001364: MPU DP: [39, 47]
nid001364: MPU DP: [48, 56]
nid001364: MPU DP: [49, 57]
nid001364: MPU DP: [50, 58]
nid001364: MPU DP: [51, 59]
nid001364: MPU DP: [52, 60]
nid001364: MPU DP: [53, 61]
nid001364: MPU DP: [54, 62]
nid001364: MPU DP: [55, 63]
nid001364: MPU PP: [0, 16, 32, 48]
nid001364: MPU PP: [1, 17, 33, 49]
nid001364: MPU PP: [2, 18, 34, 50]
nid001364: MPU PP: [3, 19, 35, 51]
nid001364: MPU PP: [4, 20, 36, 52]
nid001364: MPU PP: [5, 21, 37, 53]
nid001364: MPU PP: [6, 22, 38, 54]
nid001364: MPU PP: [7, 23, 39, 55]
nid001364: MPU PP: [8, 24, 40, 56]
nid001364: MPU PP: [9, 25, 41, 57]
nid001364: MPU PP: [10, 26, 42, 58]
nid001364: MPU PP: [11, 27, 43, 59]
nid001364: MPU PP: [12, 28, 44, 60]
nid001364: MPU PP: [13, 29, 45, 61]
nid001364: MPU PP: [14, 30, 46, 62]
nid001364: MPU PP: [15, 31, 47, 63]
nid001364: MPU IO: [0, 8, 48, 56]
nid001364: MPU MP: [0, 1, 2, 3, 4, 5, 6, 7]
nid001364: MPU MP: [8, 9, 10, 11, 12, 13, 14, 15]
nid001364: MPU MP: [16, 17, 18, 19, 20, 21, 22, 23]
nid001364: MPU MP: [24, 25, 26, 27, 28, 29, 30, 31]
nid001364: MPU MP: [32, 33, 34, 35, 36, 37, 38, 39]
nid001364: MPU MP: [40, 41, 42, 43, 44, 45, 46, 47]
nid001364: MPU MP: [48, 49, 50, 51, 52, 53, 54, 55]
nid001364: MPU MP: [56, 57, 58, 59, 60, 61, 62, 63]
nid001364: > setting random seeds to 1234 ...
nid001369: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001373: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001364: [2024-10-09 21:46:58,905] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
nid002109: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001368: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001369: make: Nothing to be done for 'default'.
nid001369: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001377: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001376: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002264: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002261: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001364: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002268: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001372: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002424: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid003209: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001373: make: Nothing to be done for 'default'.
nid001373: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002109: make: Nothing to be done for 'default'.
nid002109: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001365: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001368: make: Nothing to be done for 'default'.
nid001368: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid003037: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002261: make: Nothing to be done for 'default'.
nid002261: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001364: make: Nothing to be done for 'default'.
nid001364: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002268: make: Nothing to be done for 'default'.
nid002268: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001372: make: Nothing to be done for 'default'.
nid001372: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid003209: make: Nothing to be done for 'default'.
nid003209: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002264: make: Nothing to be done for 'default'.
nid002264: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002424: make: Nothing to be done for 'default'.
nid002424: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002265: make: Entering directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001365: make: Nothing to be done for 'default'.
nid001365: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid003037: make: Nothing to be done for 'default'.
nid003037: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001364: building GPT2 model ...
nid001364: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
nid001377: make: Nothing to be done for 'default'.
nid001377: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001376: make: Nothing to be done for 'default'.
nid001376: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid002265: make: Nothing to be done for 'default'.
nid002265: make: Leaving directory '/pscratch/sd/z/zby2022/gpt-neox_20240914/gpt-neox/megatron/data'
nid001364: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=0, model=2): 2, ProcessCoord(pipe=0, data=0, model=3): 3, ProcessCoord(pipe=0, data=0, model=4): 4, ProcessCoord(pipe=0, data=0, model=5): 5, ProcessCoord(pipe=0, data=0, model=6): 6, ProcessCoord(pipe=0, data=0, model=7): 7, ProcessCoord(pipe=0, data=1, model=0): 8, ProcessCoord(pipe=0, data=1, model=1): 9, ProcessCoord(pipe=0, data=1, model=2): 10, ProcessCoord(pipe=0, data=1, model=3): 11, ProcessCoord(pipe=0, data=1, model=4): 12, ProcessCoord(pipe=0, data=1, model=5): 13, ProcessCoord(pipe=0, data=1, model=6): 14, ProcessCoord(pipe=0, data=1, model=7): 15, ProcessCoord(pipe=1, data=0, model=0): 16, ProcessCoord(pipe=1, data=0, model=1): 17, ProcessCoord(pipe=1, data=0, model=2): 18, ProcessCoord(pipe=1, data=0, model=3): 19, ProcessCoord(pipe=1, data=0, model=4): 20, ProcessCoord(pipe=1, data=0, model=5): 21, ProcessCoord(pipe=1, data=0, model=6): 22, ProcessCoord(pipe=1, data=0, model=7): 23, ProcessCoord(pipe=1, data=1, model=0): 24, ProcessCoord(pipe=1, data=1, model=1): 25, ProcessCoord(pipe=1, data=1, model=2): 26, ProcessCoord(pipe=1, data=1, model=3): 27, ProcessCoord(pipe=1, data=1, model=4): 28, ProcessCoord(pipe=1, data=1, model=5): 29, ProcessCoord(pipe=1, data=1, model=6): 30, ProcessCoord(pipe=1, data=1, model=7): 31, ProcessCoord(pipe=2, data=0, model=0): 32, ProcessCoord(pipe=2, data=0, model=1): 33, ProcessCoord(pipe=2, data=0, model=2): 34, ProcessCoord(pipe=2, data=0, model=3): 35, ProcessCoord(pipe=2, data=0, model=4): 36, ProcessCoord(pipe=2, data=0, model=5): 37, ProcessCoord(pipe=2, data=0, model=6): 38, ProcessCoord(pipe=2, data=0, model=7): 39, ProcessCoord(pipe=2, data=1, model=0): 40, ProcessCoord(pipe=2, data=1, model=1): 41, ProcessCoord(pipe=2, data=1, model=2): 42, ProcessCoord(pipe=2, data=1, model=3): 43, ProcessCoord(pipe=2, data=1, model=4): 44, ProcessCoord(pipe=2, data=1, model=5): 45, ProcessCoord(pipe=2, data=1, model=6): 46, ProcessCoord(pipe=2, data=1, model=7): 47, ProcessCoord(pipe=3, data=0, model=0): 48, ProcessCoord(pipe=3, data=0, model=1): 49, ProcessCoord(pipe=3, data=0, model=2): 50, ProcessCoord(pipe=3, data=0, model=3): 51, ProcessCoord(pipe=3, data=0, model=4): 52, ProcessCoord(pipe=3, data=0, model=5): 53, ProcessCoord(pipe=3, data=0, model=6): 54, ProcessCoord(pipe=3, data=0, model=7): 55, ProcessCoord(pipe=3, data=1, model=0): 56, ProcessCoord(pipe=3, data=1, model=1): 57, ProcessCoord(pipe=3, data=1, model=2): 58, ProcessCoord(pipe=3, data=1, model=3): 59, ProcessCoord(pipe=3, data=1, model=4): 60, ProcessCoord(pipe=3, data=1, model=5): 61, ProcessCoord(pipe=3, data=1, model=6): 62, ProcessCoord(pipe=3, data=1, model=7): 63}
nid001364: [2024-10-09 21:47:00,150] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp
nid001364: stage=0 layers=12
nid001364:      0: EmbeddingPipe
nid001364:      1: _pre_transformer_block
nid001364:      2: ParallelTransformerLayerPipe
nid001364:      3: ParallelTransformerLayerPipe
nid001364:      4: ParallelTransformerLayerPipe
nid001364:      5: ParallelTransformerLayerPipe
nid001364:      6: ParallelTransformerLayerPipe
nid001364:      7: ParallelTransformerLayerPipe
nid001364:      8: ParallelTransformerLayerPipe
nid001364:      9: ParallelTransformerLayerPipe
nid001364:     10: ParallelTransformerLayerPipe
nid001364:     11: ParallelTransformerLayerPipe
nid001364: stage=1 layers=11
nid001364:     12: ParallelTransformerLayerPipe
nid001364:     13: ParallelTransformerLayerPipe
nid001364:     14: ParallelTransformerLayerPipe
nid001364:     15: ParallelTransformerLayerPipe
nid001364:     16: ParallelTransformerLayerPipe
nid001364:     17: ParallelTransformerLayerPipe
nid001364:     18: ParallelTransformerLayerPipe
nid001364:     19: ParallelTransformerLayerPipe
nid001364:     20: ParallelTransformerLayerPipe
nid001364:     21: ParallelTransformerLayerPipe
nid001364:     22: ParallelTransformerLayerPipe
nid001364: stage=2 layers=11
nid001364:     23: ParallelTransformerLayerPipe
nid001364:     24: ParallelTransformerLayerPipe
nid001364:     25: ParallelTransformerLayerPipe
nid001364:     26: ParallelTransformerLayerPipe
nid001364:     27: ParallelTransformerLayerPipe
nid001364:     28: ParallelTransformerLayerPipe
nid001364:     29: ParallelTransformerLayerPipe
nid001364:     30: ParallelTransformerLayerPipe
nid001364:     31: ParallelTransformerLayerPipe
nid001364:     32: ParallelTransformerLayerPipe
nid001364:     33: ParallelTransformerLayerPipe
nid001364: stage=3 layers=11
nid001364:     34: ParallelTransformerLayerPipe
nid001364:     35: ParallelTransformerLayerPipe
nid001364:     36: ParallelTransformerLayerPipe
nid001364:     37: ParallelTransformerLayerPipe
nid001364:     38: ParallelTransformerLayerPipe
nid001364:     39: ParallelTransformerLayerPipe
nid001364:     40: ParallelTransformerLayerPipe
nid001364:     41: ParallelTransformerLayerPipe
nid001364:     42: _post_transformer_block
nid001364:     43: NormPipe
nid001364:     44: ParallelLinearPipe
nid001364:   loss: partial
nid003037: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001376: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002264: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002268: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001377: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001372: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003037: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002265: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002261: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002268: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002264: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001377: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001372: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002109: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001364: Configuring Optimizer type: Adam with params: {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}
nid001364: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002261: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001365: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001369: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001373: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002424: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003209: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001369: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001365: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002109: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002424: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001373: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001368: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003209: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001368: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001364: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001376: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002265: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003037: Detected CUDA files, patching ldflags
nid003037: Emitting ninja build file /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
nid003037: Building extension module fused_adam...
nid003037: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
nid001365: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001365: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003209: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001376: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003209: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002264: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001376: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002264: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002109: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002424: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001373: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002268: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002109: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002424: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001365: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003209: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001373: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001372: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002268: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001369: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001364: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001365: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003209: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001372: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001376: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001368: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001364: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001376: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003209: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001369: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003209: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002264: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001368: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001376: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001376: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001364: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002264: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001365: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001368: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001364: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002264: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001365: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002109: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001368: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002264: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002109: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002424: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001369: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002109: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001372: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002424: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001364: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002109: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001369: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001372: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002268: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001364: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002268: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001368: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001368: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002424: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001369: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002261: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002424: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001369: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002268: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001373: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002268: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002261: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001373: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001372: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001372: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001373: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001373: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002261: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002261: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002261: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002261: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003037: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003037: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003037: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003037: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001377: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003037: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid003037: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001377: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001377: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001377: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid001377: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid001377: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002265: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002265: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002265: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002265: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid002265: WARNING: APEX not installed - defaulting to deepspeed's fused adam
nid002265: Using /global/u1/z/zby2022/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
nid003037: [1/3] /opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/TH -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/THC -isystem /opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++14 -c /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
nid003037: [2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/TH -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/include/THC -isystem /opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/include -isystem /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
nid003037: [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/pscratch/sd/z/zby2022/envs/gpt_neox_20240914/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/lib64 -lcudart -o fused_adam.so
nid002268: Loading extension module fused_adam...
nid002268: Loading extension module fused_adam...
nid003037: Loading extension module fused_adam...
nid002424: Loading extension module fused_adam...
nid001373: Loading extension module fused_adam...
nid003037: Loading extension module fused_adam...
nid002424: Loading extension module fused_adam...
nid003037: Loading extension module fused_adam...
nid002424: Loading extension module fused_adam...
nid001377: Loading extension module fused_adam...
nid001373: Loading extension module fused_adam...
nid003037: Time to load fused_adam op: 24.774460315704346 seconds
nid003037: Time to load fused_adam op: 24.839738845825195 seconds
nid003037: Time to load fused_adam op: 26.417305946350098 seconds
nid002109: Loading extension module fused_adam...
nid001372: Loading extension module fused_adam...
nid002424: Time to load fused_adam op: 26.388609170913696 seconds
nid002424: Time to load fused_adam op: 24.946243286132812 seconds
nid002424: Time to load fused_adam op: 25.00413203239441 seconds
nid002109: Loading extension module fused_adam...
nid002261: Loading extension module fused_adam...
nid002109: Loading extension module fused_adam...
nid002268: Time to load fused_adam op: 24.96026039123535 seconds
nid002268: Time to load fused_adam op: 26.410178899765015 seconds
nid002109: Loading extension module fused_adam...
nid001369: Loading extension module fused_adam...
nid002268: Loading extension module fused_adam...
nid001369: Loading extension module fused_adam...
nid001377: Loading extension module fused_adam...
nid001377: Loading extension module fused_adam...
nid002268: Time to load fused_adam op: 25.005846977233887 seconds
nid001377: Time to load fused_adam op: 24.740878582000732 seconds
nid001373: Time to load fused_adam op: 25.007159948349 seconds
nid001373: Time to load fused_adam op: 24.918739080429077 seconds
nid002261: Loading extension module fused_adam...
nid001377: Time to load fused_adam op: 24.77866768836975 seconds
nid001377: Time to load fused_adam op: 26.413783311843872 seconds
nid002109: Time to load fused_adam op: 24.980936288833618 seconds
nid002109: Time to load fused_adam op: 24.973803520202637 seconds
nid002109: Time to load fused_adam op: 26.405313730239868 seconds
nid001372: Time to load fused_adam op: 24.92991018295288 seconds
nid002109: Time to load fused_adam op: 25.012595176696777 seconds
nid002261: Time to load fused_adam op: 24.947408199310303 seconds
nid002261: Time to load fused_adam op: 26.415796518325806 seconds
nid001369: Time to load fused_adam op: 25.00329566001892 seconds
nid001369: Time to load fused_adam op: 26.4013729095459 seconds
nid001364: Loading extension module fused_adam...
nid001376: Loading extension module fused_adam...
nid001364: Loading extension module fused_adam...
nid001364: Loading extension module fused_adam...
nid001364: Time to load fused_adam op: 25.01201820373535 secondsTime to load fused_adam op: 24.981014728546143 seconds
nid001364: 
nid001373: Loading extension module fused_adam...
nid001376: Time to load fused_adam op: 25.003396034240723 seconds
nid001373: Loading extension module fused_adam...
nid001364: Time to load fused_adam op: 26.41476273536682 seconds
nid001364: > learning rate decay style: cosine
nid001364: DeepSpeed is enabled.
nid001364: [2024-10-09 21:47:26,707] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4+02e2ebf, git-hash=02e2ebf, git-branch=HEAD
nid001373: Time to load fused_adam op: 24.957727193832397 seconds
nid001373: Time to load fused_adam op: 26.410353899002075 seconds
nid003037: Loading extension module fused_adam...
nid003037: Time to load fused_adam op: 24.90196704864502 seconds
nid002268: Loading extension module fused_adam...
nid002268: Time to load fused_adam op: 24.98098921775818 seconds
nid001369: Loading extension module fused_adam...
nid002424: Loading extension module fused_adam...
nid001369: Time to load fused_adam op: 25.009501218795776 seconds
nid003209: Loading extension module fused_adam...
nid003209: Loading extension module fused_adam...
nid002424: Time to load fused_adam op: 25.011950969696045 seconds
nid001369: Loading extension module fused_adam...
nid002261: Loading extension module fused_adam...
nid001369: Time to load fused_adam op: 24.991552352905273 seconds
nid002261: Time to load fused_adam op: 24.95151662826538 seconds
nid003209: Time to load fused_adam op: 25.04936981201172 seconds
nid003209: Time to load fused_adam op: 25.039013862609863 seconds
nid001368: Loading extension module fused_adam...
nid001368: Loading extension module fused_adam...
nid001368: Loading extension module fused_adam...
nid001377: Loading extension module fused_adam...
nid001368: Loading extension module fused_adam...
nid001377: Time to load fused_adam op: 24.811764001846313 seconds
nid002265: Loading extension module fused_adam...
nid001368: Time to load fused_adam op: 25.008268117904663 seconds
nid001368: Time to load fused_adam op: 25.03346848487854 seconds
nid001368: Time to load fused_adam op: 26.43250799179077 seconds
nid001368: Time to load fused_adam op: 25.044996738433838 seconds
nid002265: Time to load fused_adam op: 24.67184352874756 seconds
nid002265: Loading extension module fused_adam...
nid001376: Loading extension module fused_adam...
nid003209: Loading extension module fused_adam...
nid001376: Loading extension module fused_adam...
nid002265: Time to load fused_adam op: 26.47836422920227 seconds
nid001376: Time to load fused_adam op: 25.094690084457397 seconds
nid003209: Time to load fused_adam op: 25.099618673324585 seconds
nid001376: Time to load fused_adam op: 25.06467580795288 seconds
nid002265: Loading extension module fused_adam...
nid003209: Loading extension module fused_adam...
nid002265: Time to load fused_adam op: 24.701891899108887 seconds
nid003209: Time to load fused_adam op: 26.462205171585083 seconds
nid001365: Loading extension module fused_adam...
nid001376: Loading extension module fused_adam...
nid001365: Loading extension module fused_adam...
nid001365: Loading extension module fused_adam...
nid001376: Time to load fused_adam op: 26.48723292350769 seconds
nid001372: Loading extension module fused_adam...
nid002264: Loading extension module fused_adam...
nid001372: Loading extension module fused_adam...Loading extension module fused_adam...
nid001372: 
nid001372: Time to load fused_adam op: 25.076831340789795 seconds
nid002264: Loading extension module fused_adam...
nid001372: Time to load fused_adam op: 25.0457866191864 seconds
nid001372: Time to load fused_adam op: 26.48899221420288 seconds
nid001365: Time to load fused_adam op: 25.113327980041504 seconds
nid001365: Time to load fused_adam op: 25.080448389053345 seconds
nid001365: Time to load fused_adam op: 26.47654962539673 seconds
nid002264: Time to load fused_adam op: 25.10139536857605 secondsTime to load fused_adam op: 25.061838150024414 seconds
nid002264: 
nid002261: Loading extension module fused_adam...
nid002261: Time to load fused_adam op: 24.97083854675293 seconds
nid002265: Loading extension module fused_adam...
nid002265: Time to load fused_adam op: 24.693289041519165 seconds
nid002264: Loading extension module fused_adam...
nid001364: Loading extension module fused_adam...
nid002264: Time to load fused_adam op: 26.504205465316772 seconds
nid001364: Time to load fused_adam op: 25.07547640800476 seconds
nid002264: Loading extension module fused_adam...
nid002264: Time to load fused_adam op: 25.088468313217163 seconds
nid001365: Loading extension module fused_adam...
nid001365: Time to load fused_adam op: 25.183400869369507 seconds
nid001364: [2024-10-09 21:47:27,039] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
nid001364: [2024-10-09 21:47:27,040] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
nid001364: [2024-10-09 21:47:27,040] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
nid001364: [2024-10-09 21:47:27,040] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
nid001364: [2024-10-09 21:47:27,041] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
nid001364: [2024-10-09 21:47:27,041] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
nid001364: [2024-10-09 21:47:27,041] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
nid001364: [2024-10-09 21:47:27,041] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
nid001364: [2024-10-09 21:47:27,041] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
nid001364: [2024-10-09 21:47:27,041] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
nid001376: [2024-10-09 21:47:27,581] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001372: [2024-10-09 21:47:27,607] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002268: [2024-10-09 21:47:27,641] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003037: [2024-10-09 21:47:27,642] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003209: [2024-10-09 21:47:27,731] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003209: [2024-10-09 21:47:27,751] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002424: [2024-10-09 21:47:27,766] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002424: [2024-10-09 21:47:27,766] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001376: [2024-10-09 21:47:27,773] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003209: [2024-10-09 21:47:27,781] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003209: [2024-10-09 21:47:27,787] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001372: [2024-10-09 21:47:27,790] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002261: [2024-10-09 21:47:27,790] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002261: [2024-10-09 21:47:27,791] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002424: [2024-10-09 21:47:27,820] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002268: [2024-10-09 21:47:27,820] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002424: [2024-10-09 21:47:27,825] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001365: [2024-10-09 21:47:27,829] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001368: [2024-10-09 21:47:27,835] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002268: [2024-10-09 21:47:27,849] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002268: [2024-10-09 21:47:27,870] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001377: [2024-10-09 21:47:27,875] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001377: [2024-10-09 21:47:27,875] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001364: [2024-10-09 21:47:27,877] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
nid001364: [2024-10-09 21:47:27,878] [INFO] [utils.py:803:see_memory_usage] MA 1.59 GB         Max_MA 1.59 GB         CA 1.59 GB         Max_CA 2 GB 
nid001364: [2024-10-09 21:47:27,878] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 40.16 GB, percent = 16.0%
nid001373: [2024-10-09 21:47:27,881] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001369: [2024-10-09 21:47:27,889] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001369: [2024-10-09 21:47:27,889] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001372: [2024-10-09 21:47:27,891] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001372: [2024-10-09 21:47:27,897] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001376: [2024-10-09 21:47:27,901] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001364: [2024-10-09 21:47:27,912] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001376: [2024-10-09 21:47:27,914] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003037: [2024-10-09 21:47:27,928] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003037: [2024-10-09 21:47:27,928] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001364: [2024-10-09 21:47:27,932] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid003037: [2024-10-09 21:47:27,942] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002109: [2024-10-09 21:47:27,945] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001364: [2024-10-09 21:47:27,947] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
nid001364: [2024-10-09 21:47:27,948] [INFO] [utils.py:803:see_memory_usage] MA 3.18 GB         Max_MA 3.97 GB         CA 3.98 GB         Max_CA 4 GB 
nid001364: [2024-10-09 21:47:27,948] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 36.02 GB, percent = 14.3%
nid001364: [2024-10-09 21:47:27,948] [INFO] [stage_1_and_2.py:517:__init__] optimizer state initialized
nid001364: [2024-10-09 21:47:27,949] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001369: [2024-10-09 21:47:27,950] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001369: [2024-10-09 21:47:27,952] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001377: [2024-10-09 21:47:27,956] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001377: [2024-10-09 21:47:27,967] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002261: [2024-10-09 21:47:27,992] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001364: [2024-10-09 21:47:27,992] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
nid001364: [2024-10-09 21:47:27,993] [INFO] [utils.py:803:see_memory_usage] MA 3.18 GB         Max_MA 3.18 GB         CA 3.98 GB         Max_CA 4 GB 
nid001364: [2024-10-09 21:47:27,993] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 36.02 GB, percent = 14.3%
nid001364: [2024-10-09 21:47:27,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
nid001364: [2024-10-09 21:47:27,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
nid001364: [2024-10-09 21:47:27,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f6a6fdace80>
nid001364: [2024-10-09 21:47:27,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:47:27,994] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   activation_checkpointing_config  {
nid001364:     "partition_activations": false, 
nid001364:     "contiguous_memory_optimization": false, 
nid001364:     "cpu_checkpointing": false, 
nid001364:     "number_checkpoints": null, 
nid001364:     "synchronize_checkpoint_boundary": false, 
nid001364:     "profile": false
nid001364: }
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   amp_enabled .................. False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   amp_params ................... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   autotuning_config ............ {
nid001364:     "enabled": false, 
nid001364:     "start_step": null, 
nid001364:     "end_step": null, 
nid001364:     "metric_path": null, 
nid001364:     "arg_mappings": null, 
nid001364:     "metric": "throughput", 
nid001364:     "model_info": null, 
nid001364:     "results_dir": "autotuning_results", 
nid001364:     "exps_dir": "autotuning_exps", 
nid001364:     "overwrite": true, 
nid001364:     "fast": true, 
nid001364:     "start_profile_step": 3, 
nid001364:     "end_profile_step": 5, 
nid001364:     "tuner_type": "gridsearch", 
nid001364:     "tuner_early_stopping": 5, 
nid001364:     "tuner_num_trials": 50, 
nid001364:     "model_info_path": null, 
nid001364:     "mp_size": 1, 
nid001364:     "max_train_batch_size": null, 
nid001364:     "min_train_batch_size": 1, 
nid001364:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
nid001364:     "min_train_micro_batch_size_per_gpu": 1, 
nid001364:     "num_tuning_micro_batch_sizes": 3
nid001364: }
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6a6fd67160>
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   communication_data_type ...... None
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   disable_allgather ............ False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   dump_state ................... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   elasticity_enabled ........... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   flops_profiler_config ........ {
nid001364:     "enabled": false, 
nid001364:     "recompute_fwd_factor": 0.0, 
nid001364:     "profile_step": 1, 
nid001364:     "module_depth": -1, 
nid001364:     "top_modules": 1, 
nid001364:     "detailed": true, 
nid001364:     "output_file": null
nid001364: }
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   fp16_enabled ................. True
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   global_rank .................. 0
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
nid001364: [2024-10-09 21:47:27,995] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 16
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   gradient_clipping ............ 0.0
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 65536
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   loss_scale ................... 0
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   memory_breakdown ............. False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   mics_shard_size .............. -1
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   nebula_config ................ {
nid001364:     "enabled": false, 
nid001364:     "persistent_storage_path": null, 
nid001364:     "persistent_time_interval": 100, 
nid001364:     "num_of_version_in_retention": 2, 
nid001364:     "enable_nebula_load": true, 
nid001364:     "load_path": null
nid001364: }
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   optimizer_name ............... adam
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   optimizer_params ............. {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   pld_enabled .................. False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   pld_params ................... False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   prescale_gradients ........... False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   scheduler_name ............... None
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   scheduler_params ............. None
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   sparse_attention ............. None
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   steps_per_print .............. 1
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   train_batch_size ............. 128
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  4
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   use_node_local_storage ....... False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   wall_clock_breakdown ......... True
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   weight_quantization_config ... None
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   world_size ................... 2
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   zero_enabled ................. True
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:983:print]   zero_optimization_stage ...... 1
nid001364: [2024-10-09 21:47:27,996] [INFO] [config.py:969:print_user_config]   json = {
nid001364:     "train_batch_size": 128, 
nid001364:     "train_micro_batch_size_per_gpu": 4, 
nid001364:     "gradient_accumulation_steps": 16, 
nid001364:     "optimizer": {
nid001364:         "type": "Adam", 
nid001364:         "params": {
nid001364:             "lr": 0.0002, 
nid001364:             "betas": [0.9, 0.95], 
nid001364:             "eps": 1e-08
nid001364:         }
nid001364:     }, 
nid001364:     "fp16": {
nid001364:         "fp16": true, 
nid001364:         "enabled": true, 
nid001364:         "loss_scale": 0, 
nid001364:         "loss_scale_window": 1000, 
nid001364:         "hysteresis": 2, 
nid001364:         "min_loss_scale": 1
nid001364:     }, 
nid001364:     "zero_optimization": {
nid001364:         "stage": 1, 
nid001364:         "allgather_partitions": true, 
nid001364:         "allgather_bucket_size": 5.000000e+08, 
nid001364:         "overlap_comm": true, 
nid001364:         "reduce_scatter": true, 
nid001364:         "reduce_bucket_size": 5.000000e+08, 
nid001364:         "contiguous_gradients": true
nid001364:     }, 
nid001364:     "steps_per_print": 1, 
nid001364:     "wall_clock_breakdown": true
nid001364: }
nid001364: [2024-10-09 21:47:27,996] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=16 micro_batch_size=4
nid001364: [2024-10-09 21:47:27,997] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001368: [2024-10-09 21:47:28,006] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001368: [2024-10-09 21:47:28,033] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002261: [2024-10-09 21:47:28,051] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001368: [2024-10-09 21:47:28,053] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001365: [2024-10-09 21:47:28,058] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001365: [2024-10-09 21:47:28,094] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001365: [2024-10-09 21:47:28,107] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001373: [2024-10-09 21:47:28,129] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001373: [2024-10-09 21:47:28,195] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001373: [2024-10-09 21:47:28,209] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002109: [2024-10-09 21:47:28,211] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002109: [2024-10-09 21:47:28,259] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002109: [2024-10-09 21:47:28,260] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002264: [2024-10-09 21:47:28,372] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002265: [2024-10-09 21:47:28,438] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002264: [2024-10-09 21:47:28,487] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002264: [2024-10-09 21:47:28,508] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002264: [2024-10-09 21:47:28,520] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002265: [2024-10-09 21:47:28,530] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002265: [2024-10-09 21:47:28,551] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid002265: [2024-10-09 21:47:28,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
nid001364: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=426163200 (426.163M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001364: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=1 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=426163200 (426.163M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001364: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=3 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=426163200 (426.163M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002109: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=35 STAGE=2 LAYERS=11 [23, 34) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002109: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=32 STAGE=2 LAYERS=11 [23, 34) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002109: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=33 STAGE=2 LAYERS=11 [23, 34) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002109: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=34 STAGE=2 LAYERS=11 [23, 34) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001372: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=17 STAGE=1 LAYERS=11 [12, 23) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002268: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=50 STAGE=3 LAYERS=11 [34, 45) STAGE_PARAMS=347489280 (347.489M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001364: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=2 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=426163200 (426.163M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002424: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=52 STAGE=3 LAYERS=11 [34, 45) STAGE_PARAMS=347489280 (347.489M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002268: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=48 STAGE=3 LAYERS=11 [34, 45) STAGE_PARAMS=347489280 (347.489M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001372: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=16 STAGE=1 LAYERS=11 [12, 23) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002424: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=55 STAGE=3 LAYERS=11 [34, 45) STAGE_PARAMS=347489280 (347.489M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002424: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=53 STAGE=3 LAYERS=11 [34, 45) STAGE_PARAMS=347489280 (347.489M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002424: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=54 STAGE=3 LAYERS=11 [34, 45) STAGE_PARAMS=347489280 (347.489M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002268: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=51 STAGE=3 LAYERS=11 [34, 45) STAGE_PARAMS=347489280 (347.489M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002268: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=49 STAGE=3 LAYERS=11 [34, 45) STAGE_PARAMS=347489280 (347.489M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002261: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=38 STAGE=2 LAYERS=11 [23, 34) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001372: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=18 STAGE=1 LAYERS=11 [12, 23) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001365: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=5 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=426163200 (426.163M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001373: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=23 STAGE=1 LAYERS=11 [12, 23) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001372: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=19 STAGE=1 LAYERS=11 [12, 23) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001373: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=20 STAGE=1 LAYERS=11 [12, 23) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001373: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=21 STAGE=1 LAYERS=11 [12, 23) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001373: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=22 STAGE=1 LAYERS=11 [12, 23) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001365: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=7 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=426163200 (426.163M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001365: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=6 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=426163200 (426.163M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002261: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=37 STAGE=2 LAYERS=11 [23, 34) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001365: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=4 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=426163200 (426.163M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002261: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=36 STAGE=2 LAYERS=11 [23, 34) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid002261: [2024-10-09 21:47:29,562] [INFO] [engine.py:158:__init__] RANK=39 STAGE=2 LAYERS=11 [23, 34) STAGE_PARAMS=432734720 (432.735M) TOTAL_PARAMS=13112975360 (13112.975M) UNIQUE_PARAMS=13112975360 (13112.975M)
nid001365:  > number of parameters on model parallel rank 4: 426163200
nid001365:  > number of parameters on model parallel rank 5: 426163200
nid002424:  > number of parameters on model parallel rank 6: 347489280
nid002268:  > number of parameters on model parallel rank 0: 347489280
nid001364:  > number of parameters on model parallel rank 0: 426163200
nid001364:  > number of parameters on model parallel rank 1: 426163200
nid002268:  > number of parameters on model parallel rank 1: 347489280
nid002424:  > number of parameters on model parallel rank 4: 347489280
nid001365:  > number of parameters on model parallel rank 6: 426163200
nid002424:  > number of parameters on model parallel rank 7: 347489280
nid001365:  > number of parameters on model parallel rank 7: 426163200
nid002424:  > number of parameters on model parallel rank 5: 347489280
nid002268:  > number of parameters on model parallel rank 2: 347489280
nid002268:  > number of parameters on model parallel rank 3: 347489280
nid001364:  > number of parameters on model parallel rank 2: 426163200
nid001364:  > number of parameters on model parallel rank 3: 426163200
nid001373:  > number of parameters on model parallel rank 4: 432734720
nid002261:  > number of parameters on model parallel rank 4: 432734720
nid002109:  > number of parameters on model parallel rank 1: 432734720
nid002109:  > number of parameters on model parallel rank 0: 432734720
nid001372:  > number of parameters on model parallel rank 1: 432734720
nid001372:  > number of parameters on model parallel rank 0: 432734720
nid001373:  > number of parameters on model parallel rank 6: 432734720
nid002261:  > number of parameters on model parallel rank 6: 432734720
nid001373:  > number of parameters on model parallel rank 7: 432734720
nid002261:  > number of parameters on model parallel rank 7: 432734720
nid002261:  > number of parameters on model parallel rank 5: 432734720
nid001373:  > number of parameters on model parallel rank 5: 432734720
nid002109:  > number of parameters on model parallel rank 2: 432734720
nid001372:  > number of parameters on model parallel rank 2: 432734720
nid002109:  > number of parameters on model parallel rank 3: 432734720
nid001372:  > number of parameters on model parallel rank 3: 432734720
nid001364:  > total params: 13,112,975,360
nid001364: > building train, validation, and test datasets ...
nid001364:     reading sizes...
nid001364:     reading pointers...
nid001364:     reading document index...
nid001364:     creating numpy buffer of mmap...
nid001364:     creating memory view of numpy buffer...
nid001364:  > dataset split:
nid001364:     train:
nid001364:      document indices in [0, 204076229) total of 204076229 documents
nid001364:     validation:
nid001364:      document indices in [204076229, 210394379) total of 6318150 documents
nid001364:     test:
nid001364:      document indices in [210394379, 210604984) total of 210605 documents
nid001364:  > WARNING: could not find index map files, building the indices on rank 0 ...
nid001364:  > elapsed time to build and save doc-idx mapping (seconds): 12.346258
nid001364:     using:
nid001364:      number of documents:       204076229
nid001364:      number of epochs:          1
nid001364:      sequence length:           2048
nid001364:      total number of samples:   177124620
nid001364:  > elapsed time to build and save sample-idx mapping (seconds): 4.416673
nid001364:  > elapsed time to build and save shuffle-idx mapping (seconds): 9.514065
nid001364:  > loading doc-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_train_indexmap_1024ns_2048sl_1234s_packedpi_ac_doc_idx.npy
nid001364:  > loading sample-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_train_indexmap_1024ns_2048sl_1234s_packedpi_ac_sample_idx.npy
nid001364:  > loading shuffle-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_train_indexmap_1024ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy
nid001364:     loaded indexed file in 0.011 seconds
nid001364:     total number of samples: 177124621
nid001364:     total number of epochs: 1
nid001364:  > loading doc-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_valid_indexmap_12800ns_2048sl_1234s_packedpi_ac_doc_idx.npy
nid001364:  > loading sample-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_valid_indexmap_12800ns_2048sl_1234s_packedpi_ac_sample_idx.npy
nid001364:  > loading shuffle-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_valid_indexmap_12800ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy
nid001364:     loaded indexed file in 0.019 seconds
nid001364:     total number of samples: 5477521
nid001364:     total number of epochs: 1
nid001364:  > loading doc-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_test_indexmap_12800ns_2048sl_1234s_packedpi_ac_doc_idx.npy
nid001364:  > loading sample-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_test_indexmap_12800ns_2048sl_1234s_packedpi_ac_sample_idx.npy
nid001364:  > loading shuffle-idx mapping from /pscratch/sd/z/zby2022/models/gpt-neox/data/pile_text_document_test_indexmap_12800ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy
nid001364:     loaded indexed file in 0.012 seconds
nid001364:     total number of samples: 179782
nid001364:     total number of epochs: 1
nid001364: setting training data start iteration to 0
nid001364: setting validation data start iteration to 0
nid001364: done with setups ...
nid001364: time (ms) | model and optimizer: 33005.41 | train/valid/test data iterators: 29349.93
nid001364: training ...
nid001364: [2024-10-09 21:48:02,501] [INFO] [checkpointing.py:540:forward] Activation Checkpointing Information
nid001364: [2024-10-09 21:48:02,502] [INFO] [checkpointing.py:541:forward] ----Partition Activations True, CPU CHECKPOINTING False
nid001364: [2024-10-09 21:48:02,502] [INFO] [checkpointing.py:542:forward] ----contiguous Memory Checkpointing False with 40 total layers
nid001364: [2024-10-09 21:48:02,502] [INFO] [checkpointing.py:544:forward] ----Synchronization True
nid001364: [2024-10-09 21:48:02,502] [INFO] [checkpointing.py:545:forward] ----Profiling time in checkpointing False
nid001364: [2024-10-09 21:49:09,837] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 341.66 | optimizer_gradients: 5.62 | optimizer_step: 5.91
nid001364: [2024-10-09 21:49:09,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.00019407330742385963, 0.00019407330742385963], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:49:09,838] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 786.76 | fwd_microstep: 14740.81 | bwd_microstep: 26898.01 | bwd_inner_microstep: 26897.83 | bwd_allreduce_microstep: 0.01 | step_microstep: 406.92
nid001364: [2024-10-09 21:49:09,839] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 14740.84 | bwd: 26898.01 | bwd_inner: 26897.84 | bwd_allreduce: 0.01 | step: 406.92
nid001364: steps: 1 loss: 11.0281 iter time (s): 68.509 samples/sec: 1.868
nid001364: [2024-10-09 21:49:10,147] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 5020.33 | pipe_recv_grad: 19647.05
nid001364:  samples/sec: 1.868 | iteration        1/       8 | elapsed time per iteration (ms): 68513.2 | learning rate: 1.941E-04 | approx flops per GPU: 6.5TFLOPS | lm_loss: 1.102809E+01 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
nid001364: after 1 iterations memory (MB) | allocated: 3280.8798828125 | max allocated: 5839.0771484375 | reserved: 7972.0 | max reserved: 7972.0
nid001364: time (ms)
nid003209: STAGE:2024-10-09 21:49:58 1846364:1846364 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001364: [2024-10-09 21:49:58,933] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 348.39 | optimizer_gradients: 3.27 | optimizer_step: 5.95
nid001364: [2024-10-09 21:49:58,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.00017513606342945632, 0.00017513606342945632], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:49:58,934] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 15.63 | fwd_microstep: 11476.27 | bwd_microstep: 25986.80 | bwd_inner_microstep: 25986.64 | bwd_allreduce_microstep: 0.00 | step_microstep: 424.81
nid001364: [2024-10-09 21:49:58,935] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11476.32 | bwd: 25986.79 | bwd_inner: 25986.64 | bwd_allreduce: 0.00 | step: 424.82
nid001364: steps: 2 loss: 11.0290 iter time (s): 48.785 samples/sec: 2.624
nid001364: [2024-10-09 21:49:58,936] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 165.78 | pipe_recv_grad: 10009.86
nid001364:  samples/sec: 2.624 | iteration        2/       8 | elapsed time per iteration (ms): 48788.3 | learning rate: 1.751E-04 | approx flops per GPU: 9.1TFLOPS | lm_loss: 1.102900E+01 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
nid001364: time (ms)
nid001364: STAGE:2024-10-09 21:49:58 817966:817966 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid001372: STAGE:2024-10-09 21:49:58 1207264:1207264 ActivityProfilerController.cpp:294] Completed Stage: Warm Up
nid003209: [W CPUAllocator.cpp:231] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event
nid001372: [W CPUAllocator.cpp:231] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event
nid001364: [W CPUAllocator.cpp:231] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event
nid001364: [2024-10-09 21:50:47,321] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
nid001364: [2024-10-09 21:50:47,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=1, lr=[0.00017513606342945632, 0.00017513606342945632], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:50:47,322] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 26.34 | fwd_microstep: 11536.02 | bwd_microstep: 26185.90 | bwd_inner_microstep: 26185.75 | bwd_allreduce_microstep: 0.00 | step_microstep: 61.53
nid001364: [2024-10-09 21:50:47,324] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11536.08 | bwd: 26185.90 | bwd_inner: 26185.75 | bwd_allreduce: 0.00 | step: 61.55
nid001364: steps: 3 loss: 13.8119 iter time (s): 48.384 samples/sec: 2.646
nid001364: [2024-10-09 21:50:47,325] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 77.05 | pipe_recv_grad: 9779.54
nid001364:  samples/sec: 2.645 | iteration        3/       8 | elapsed time per iteration (ms): 48389.2 | learning rate: 1.751E-04 | approx flops per GPU: 9.2TFLOPS | lm_loss: 0.000000E+00 | loss scale: 65536.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
nid001364: time (ms)
nid001364: [2024-10-09 21:51:36,308] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
nid001364: [2024-10-09 21:51:36,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=2, lr=[0.00017513606342945632, 0.00017513606342945632], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:51:36,310] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 22.78 | fwd_microstep: 11697.39 | bwd_microstep: 26381.54 | bwd_inner_microstep: 26381.38 | bwd_allreduce_microstep: 0.00 | step_microstep: 53.06
nid001364: [2024-10-09 21:51:36,312] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11697.47 | bwd: 26381.54 | bwd_inner: 26381.38 | bwd_allreduce: 0.00 | step: 53.08
nid001364: steps: 4 loss: 13.7675 iter time (s): 48.985 samples/sec: 2.613
nid001364: [2024-10-09 21:51:36,313] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 78.01 | pipe_recv_grad: 10039.87
nid001364:  samples/sec: 2.613 | iteration        4/       8 | elapsed time per iteration (ms): 48987.9 | learning rate: 1.751E-04 | approx flops per GPU: 9.1TFLOPS | lm_loss: 0.000000E+00 | loss scale: 32768.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
nid001364: time (ms)
nid001364: [2024-10-09 21:52:24,894] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
nid001364: [2024-10-09 21:52:24,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=3, lr=[0.00017513606342945632, 0.00017513606342945632], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:52:24,896] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 22.60 | fwd_microstep: 11330.28 | bwd_microstep: 25721.83 | bwd_inner_microstep: 25721.68 | bwd_allreduce_microstep: 0.00 | step_microstep: 51.86
nid001364: [2024-10-09 21:52:24,897] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11330.32 | bwd: 25721.83 | bwd_inner: 25721.68 | bwd_allreduce: 0.00 | step: 51.88
nid001364: steps: 5 loss: 13.9103 iter time (s): 48.583 samples/sec: 2.635
nid001364: [2024-10-09 21:52:24,899] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 78.98 | pipe_recv_grad: 10651.21
nid001364:  samples/sec: 2.635 | iteration        5/       8 | elapsed time per iteration (ms): 48585.6 | learning rate: 1.751E-04 | approx flops per GPU: 9.2TFLOPS | lm_loss: 0.000000E+00 | loss scale: 16384.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
nid001364: time (ms)
nid001364: [2024-10-09 21:53:13,672] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 346.63 | optimizer_gradients: 3.37 | optimizer_step: 6.00
nid001364: [2024-10-09 21:53:13,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=3, lr=[0.00014608374818659523, 0.00014608374818659523], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:53:13,674] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 22.07 | fwd_microstep: 11252.64 | bwd_microstep: 25684.93 | bwd_inner_microstep: 25684.78 | bwd_allreduce_microstep: 0.00 | step_microstep: 426.26
nid001364: [2024-10-09 21:53:13,675] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11252.67 | bwd: 25684.93 | bwd_inner: 25684.78 | bwd_allreduce: 0.00 | step: 426.30
nid001364: steps: 6 loss: 13.8335 iter time (s): 48.776 samples/sec: 2.624
nid001364: [2024-10-09 21:53:13,677] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 81.07 | pipe_recv_grad: 10596.64
nid001364:  samples/sec: 2.624 | iteration        6/       8 | elapsed time per iteration (ms): 48778.1 | learning rate: 1.461E-04 | approx flops per GPU: 9.1TFLOPS | lm_loss: 1.383350E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
nid001364: time (ms)
nid001364: [2024-10-09 21:54:02,739] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 347.77 | optimizer_gradients: 3.30 | optimizer_step: 5.94
nid001364: [2024-10-09 21:54:02,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=3, lr=[0.00011142793674513272, 0.00011142793674513272], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:54:02,741] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 25.77 | fwd_microstep: 11451.46 | bwd_microstep: 26121.57 | bwd_inner_microstep: 26121.42 | bwd_allreduce_microstep: 0.00 | step_microstep: 375.54
nid001364: [2024-10-09 21:54:02,742] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11451.48 | bwd: 26121.57 | bwd_inner: 26121.42 | bwd_allreduce: 0.00 | step: 375.59
nid001364: steps: 7 loss: 23.2560 iter time (s): 49.064 samples/sec: 2.609
nid001364: [2024-10-09 21:54:02,743] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 79.29 | pipe_recv_grad: 10268.73
nid001364:  samples/sec: 2.609 | iteration        7/       8 | elapsed time per iteration (ms): 49066.5 | learning rate: 1.114E-04 | approx flops per GPU: 9.1TFLOPS | lm_loss: 2.325602E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
nid001364: time (ms)
nid003209: STAGE:2024-10-09 21:54:04 1846364:1846364 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001364: STAGE:2024-10-09 21:54:04 817966:817966 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid001372: STAGE:2024-10-09 21:54:05 1207264:1207264 ActivityProfilerController.cpp:300] Completed Stage: Collection
nid003209: STAGE:2024-10-09 21:55:39 1846364:1846364 output_json.cpp:417] Completed Stage: Post Processing
nid001364: STAGE:2024-10-09 21:55:50 817966:817966 output_json.cpp:417] Completed Stage: Post Processing
nid001372: STAGE:2024-10-09 21:56:04 1207264:1207264 output_json.cpp:417] Completed Stage: Post Processing
nid001364: [2024-10-09 21:56:52,867] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 351.48 | optimizer_gradients: 3.24 | optimizer_step: 5.90
nid001364: [2024-10-09 21:56:52,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=3, lr=[7.655037899057054e-05, 7.655037899057054e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
nid001364: [2024-10-09 21:56:52,869] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 16.28 | fwd_microstep: 11604.33 | bwd_microstep: 26198.78 | bwd_inner_microstep: 26198.63 | bwd_allreduce_microstep: 0.00 | step_microstep: 453.69
nid001364: [2024-10-09 21:56:52,870] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11604.39 | bwd: 26198.78 | bwd_inner: 26198.63 | bwd_allreduce: 0.00 | step: 453.70
nid001364: steps: 8 loss: 10.7974 iter time (s): 62.597 samples/sec: 2.045
nid001364: [2024-10-09 21:56:52,871] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 76.51 | pipe_recv_grad: 23540.70
nid001364:  samples/sec: 0.752 | iteration        8/       8 | elapsed time per iteration (ms): 170127.4 | learning rate: 7.655E-05 | approx flops per GPU: 2.6TFLOPS | lm_loss: 1.079742E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
nid001364: time (ms)
nid001376: Connection to nid001376 closed by remote host.
nid003037: Connection to nid003037 closed by remote host.
nid001369: Connection to nid001369 closed by remote host.
nid001373: Connection to nid001373 closed by remote host.
nid001368: Connection to nid001368 closed by remote host.
nid002424: Connection to nid002424 closed by remote host.
nid001377: Connection to nid001377 closed by remote host.
nid001372: Connection to nid001372 closed by remote host.
nid001365: Connection to nid001365 closed by remote host.
nid003209: Connection to nid003209 closed by remote host.
pdsh@nid001364: nid001376: ssh exited with exit code 255
nid002109: Connection to nid002109 closed by remote host.
pdsh@nid001364: nid001372: ssh exited with exit code 255
nid002264: Connection to nid002264 closed by remote host.
pdsh@nid001364: nid003037: ssh exited with exit code 255
pdsh@nid001364: nid001365: ssh exited with exit code 255
pdsh@nid001364: nid001369: ssh exited with exit code 255
pdsh@nid001364: nid001377: ssh exited with exit code 255
nid002261: Connection to nid002261 closed by remote host.
nid002265: Connection to nid002265 closed by remote host.
nid001364: Connection to nid001364 closed by remote host.
nid002268: Connection to nid002268 closed by remote host.
pdsh@nid001364: nid002264: ssh exited with exit code 255
pdsh@nid001364: nid003209: ssh exited with exit code 255
pdsh@nid001364: nid001368: ssh exited with exit code 255
pdsh@nid001364: nid001373: ssh exited with exit code 255
pdsh@nid001364: nid002109: ssh exited with exit code 255
pdsh@nid001364: nid002424: ssh exited with exit code 255
pdsh@nid001364: nid002268: ssh exited with exit code 255
pdsh@nid001364: nid001364: ssh exited with exit code 255
pdsh@nid001364: nid002261: ssh exited with exit code 255
pdsh@nid001364: nid002265: ssh exited with exit code 255
