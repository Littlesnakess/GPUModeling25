cpu_op_0,cpu_op_0_id,cpu_op_0_input_dim,cpu_op_1,cpu_op_1_id,cpu_op_1_input_dim,kernel,kernel_id,kernel_overhead(us),kernel_dur(us)
aten::zeros,12379,"[[], [], [], [], []]",aten::fill_,12382,"[[1, 1, 4096, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<bool>, std::array<char*, 1ul> >(int, at::native::FillFunctor<bool>, std::array<char*, 1ul>)",12382,0,6.464057142857154
aten::zeros,12383,"[[], [], [], [], []]",aten::fill_,12386,"[[67108864], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",12386,8844.037923177084,41.057960714285734
aten::copy_,12390,"[[33554432], [33554432], []]",aten::copy_,12390,"[[33554432], [33554432], []]",Memcpy DtoD (Device -> Device),12390,1270.8966471354167,49.88314285714284
c10d::allgather_,12394,"[[], [[33554432]], [], []]",record_param_comms,12395,"[[[33554432]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllGather_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",12395,7801.361165364583,1793.2094857142856
c10d::allgather_,12394,"[[], [[33554432]], [], []]",aten::copy_,12402,"[[33554432], [33554432], []]",Memcpy DtoD (Device -> Device),12402,144.43538411458334,49.14042142857142
c10d::allgather_,12394,"[[], [[33554432]], [], []]",aten::copy_,12405,"[[33554432], [33554432], []]",Memcpy DtoD (Device -> Device),12405,142.5380859375,48.49765000000001
aten::to,12419,"[[], [], [], [], [], []]",aten::copy_,12422,"[[], [], []]",Memcpy HtoD (Pageable -> Device),12422,82.65576171875,0.7698214285714294
aten::linalg_vector_norm,12426,"[[4096, 4, 4096], [], [], [], []]",aten::linalg_vector_norm,12426,"[[4096, 4, 4096], [], [], [], []]","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<c10::Half, at::native::NormTwoOps<c10::Half, float, c10::Half>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::NormTwoOps<c10::Half, float, c10::Half>, unsigned int, c10::Half, 4>)",12426,8814.125651041666,51.2098142857143
aten::mul,12427,"[[4096, 4, 1], []]",aten::mul,12427,"[[4096, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",12427,286.4029947916667,1.6942678571428573
aten::add,12428,"[[4096, 4, 1], [], []]",aten::add,12428,"[[4096, 4, 1], [], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<c10::Half>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<c10::Half>, std::array<char*, 2ul>)",12428,2388.8465169270835,1.3822714285714248
aten::div,12429,"[[4096, 4, 4096], [4096, 4, 1]]",aten::div,12429,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",12429,2634.9889322916665,182.64028214285722
aten::mul,12430,"[[4096], [4096, 4, 4096]]",aten::mul,12430,"[[4096], [4096, 4, 4096]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",12430,144.40576171875,172.5822999999999
aten::linear,12435,"[[4096, 4, 4096], [6144, 4096], []]",aten::mm,12442,"[[16384, 4096], [4096, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,12442,312.9072265625,1076.7145821428571
aten::to,12455,"[[4096, 1, 1, 128], [], [], [], [], [], [], []]",aten::copy_,12458,"[[4096, 1, 1, 128], [4096, 1, 1, 128], []]",Memcpy HtoD (Pageable -> Device),12458,460.8274739583333,6.629653571428572
aten::to,12459,"[[4096, 1, 1, 128], [], [], [], [], [], [], []]",aten::copy_,12462,"[[4096, 1, 1, 128], [4096, 1, 1, 128], []]",Memcpy HtoD (Pageable -> Device),12462,4094.5231119791665,6.1028392857142855
apply_rotary_pos_emb,12463,"[[4096, 4, 16, 128], [4096, 4, 16, 128], [4096, 1, 1, 128], [4096, 1, 1, 128], []]",aten::neg,12472,"[[4096, 1, 1, 128], [4096, 4, 16, 64]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",12472,10347.786295572916,42.69188214285714
apply_rotary_pos_emb,12463,"[[4096, 4, 16, 128], [4096, 4, 16, 128], [4096, 1, 1, 128], [4096, 1, 1, 128], []]",aten::cat,12484,"[[[4096, 4, 16, 64], [4096, 4, 16, 64]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",12484,2611.658203125,139.92704642857146
None,None,None,None,None,None,kernel_1,12485,315.5281575520833,61.17327499999993
apply_rotary_pos_emb,12463,"[[4096, 4, 16, 128], [4096, 4, 16, 128], [4096, 1, 1, 128], [4096, 1, 1, 128], []]",aten::neg,12495,"[[4096, 4, 16, 128], [4096, 1, 1, 128], [4096, 4, 16, 64]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",12495,153.70475260416666,45.270564285714286
apply_rotary_pos_emb,12463,"[[4096, 4, 16, 128], [4096, 4, 16, 128], [4096, 1, 1, 128], [4096, 1, 1, 128], []]",aten::cat,12506,"[[4096, 4, 16, 128], [[4096, 4, 16, 64], [4096, 4, 16, 64]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",12506,155.3271484375,139.72474999999997
None,None,None,None,None,None,kernel_1,12507,135.7353515625,61.201964285714325
FlashAttnFunc,12525,"[[4, 4096, 16, 128], [4, 4096, 16, 128], [4, 4096, 16, 128], [], [], [], [], [], [], []]",FlashAttnFunc,12525,"[[4, 4096, 16, 128], [4, 4096, 16, 128], [4, 4096, 16, 128], [], [], [], [], [], [], []]","void flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t> >, false, true, false, false, true, true, false>(Flash_fwd_params)",12525,236.04915364583334,847.5460535714285
aten::linear,12535,"[[4096, 4, 2048], [4096, 2048], []]",aten::mm,12542,"[[16384, 2048], [2048, 4096]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_on_kernel__5x_cublas,12542,557.5,333.8213285714287
_ReduceFromModelParallelRegion,12544,"[[4096, 4, 4096]]",nccl:all_reduce,12547,"[[4096, 4, 4096]]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",12546,218.07568359375,3026.4250642857137
aten::add,12552,"[[4096, 4, 4096], [4096, 4, 4096], []]",aten::add,12552,"[[4096, 4, 4096], [4096, 4, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12552,143.52962239583334,112.939325
aten::linalg_vector_norm,12553,"[[4096, 4, 4096], [], [], [], []]",aten::linalg_vector_norm,12553,"[[4096, 4, 4096], [], [], [], []]","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<c10::Half, at::native::NormTwoOps<c10::Half, float, c10::Half>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::NormTwoOps<c10::Half, float, c10::Half>, unsigned int, c10::Half, 4>)",12553,136.09521484375,50.63410357142857
aten::mul,12554,"[[4096, 4, 1], []]",aten::mul,12554,"[[4096, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",12554,139.13639322916666,1.8728999999999978
aten::add,12555,"[[4096, 4, 1], [], []]",aten::add,12555,"[[4096, 4, 1], [], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<c10::Half>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<c10::Half>, std::array<char*, 2ul>)",12555,106.21923828125,1.3789571428571403
aten::div,12556,"[[4096, 4, 4096], [4096, 4, 1]]",aten::div,12556,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",12556,104.99837239583333,181.70692499999996
aten::mul,12557,"[[4096], [4096, 4, 4096]]",aten::mul,12557,"[[4096], [4096, 4, 4096]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",12557,151.79671223958334,172.45469999999995
aten::to,12560,"[[], [], [], [], [], []]",aten::copy_,12563,"[[], [], []]",Memcpy HtoD (Pageable -> Device),12563,80.37467447916667,0.772225000000001
aten::linear,12570,"[[4096, 4, 4096], [8192, 4096], []]",aten::mm,12577,"[[16384, 4096], [4096, 8192]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_on_kernel__5x_cublas,12577,16333.9130859375,1450.842867857143
aten::add,12579,"[[4096, 4, 8192], [8192], []]",aten::add,12579,"[[4096, 4, 8192], [8192], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",12579,135.60384114583334,338.96583571428556
aten::gelu,12580,"[[4096, 4, 8192], []]",aten::gelu,12580,"[[4096, 4, 8192], []]","void at::native::vectorized_elementwise_kernel<4, at::native::GeluCUDAKernelImpl(at::TensorIteratorBase&, at::native::GeluType)::{lambda()#2}::operator()() const::{lambda()#3}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul> >(int, at::native::GeluCUDAKernelImpl(at::TensorIteratorBase&, at::native::GeluType)::{lambda()#2}::operator()() const::{lambda()#3}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul>)",12580,136.33837890625,217.45113928571445
aten::linear,12581,"[[4096, 4, 8192], [4096, 8192], []]",aten::mm,12588,"[[16384, 8192], [8192, 4096]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x256x64_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,12588,359.0911458333333,1287.3052892857145
_ReduceFromModelParallelRegion,12590,"[[4096, 4, 4096]]",nccl:all_reduce,12593,"[[4096, 4, 4096]]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",12592,237.84114583333334,3045.0304928571427
aten::add,12600,"[[4096, 4, 4096], [4096, 4, 4096], []]",aten::add,12600,"[[4096, 4, 4096], [4096, 4, 4096], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",12600,145.97412109375,174.05313928571422
aten::add,12602,"[[4096, 4, 4096], [4096, 4, 4096], []]",aten::add,12602,"[[4096, 4, 4096], [4096, 4, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12602,134.94449869791666,110.37104999999997
autograd::engine::evaluate_function: ExpandBackward0,12607,,aten::sum,12609,"[[4096, 4, 4096], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",12609,367.68505859375,58.20779642857147
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,12611,,aten::add_,12613,"[[4096], [4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12613,140.68147786458334,1.8206678571428572
autograd::engine::evaluate_function: MmBackward0,12620,,aten::mm,12625,"[[4096, 16384], [16384, 8192]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x256x32_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,12625,385.9928385416667,1278.4172321428566
autograd::engine::evaluate_function: MmBackward0,12620,,aten::mm,12632,"[[16384, 4096], [4096, 8192]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize256x128x32_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,12632,447.7109375,1303.815139285714
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,12642,,aten::add_,12644,"[[4096, 8192], [4096, 8192], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12644,171.58365885416666,52.8410392857143
autograd::engine::evaluate_function: GeluBackward0,12645,,aten::gelu_backward,12647,"[[4096, 4, 8192], [4096, 4, 8192], []]","void at::native::vectorized_elementwise_kernel<4, at::native::GeluBackwardCUDAKernelImpl(at::TensorIteratorBase&, at::native::GeluType)::{lambda()#2}::operator()() const::{lambda()#3}::operator()() const::{lambda(c10::Half, c10::Half)#1}, std::array<char*, 3ul> >(int, at::native::GeluBackwardCUDAKernelImpl(at::TensorIteratorBase&, at::native::GeluType)::{lambda()#2}::operator()() const::{lambda()#3}::operator()() const::{lambda(c10::Half, c10::Half)#1}, std::array<char*, 3ul>)",12647,151.87955729166666,287.0320535714287
autograd::engine::evaluate_function: AddBackward0,12648,,aten::sum,12650,"[[4096, 4, 8192], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",12650,398.1619466145833,88.85703571428571
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,12652,,aten::add_,12654,"[[8192], [8192], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12654,139.79361979166666,1.836785714285715
autograd::engine::evaluate_function: MmBackward0,12659,,aten::mm,12664,"[[8192, 16384], [16384, 4096]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize256x128x64_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,12664,388.4775390625,1257.9818750000002
autograd::engine::evaluate_function: MmBackward0,12659,,aten::mm,12671,"[[16384, 8192], [8192, 4096]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize256x128x64_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,12671,436.224609375,1284.8690964285718
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,12681,,aten::add_,12683,"[[8192, 4096], [8192, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12683,156.76513671875,52.082435714285744
autograd::engine::evaluate_function: _CopyToModelParallelRegionBackward,12684,,record_param_comms,12687,"[[[4096, 4, 4096]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",12687,246.24560546875,3068.4078321428574
autograd::engine::evaluate_function: MulBackward0,12690,,aten::mul,12692,"[[4096, 4, 4096], [4096]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",12692,188.25651041666666,173.37862500000006
autograd::engine::evaluate_function: MulBackward0,12690,,aten::mul,12693,"[[4096, 4, 4096], [4096, 4, 4096]]","void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",12693,137.15169270833334,110.34384285714286
autograd::engine::evaluate_function: MulBackward0,12690,,aten::sum,12694,"[[4096, 4, 4096], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",12694,388.0139973958333,58.114885714285705
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,12696,,aten::add_,12698,"[[4096], [4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12698,140.27913411458334,1.829471428571427
autograd::engine::evaluate_function: DivBackward0,12699,,aten::neg,12701,"[[4096, 4, 4096]]","void at::native::vectorized_elementwise_kernel<4, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul> >(int, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul>)",12701,119.48421223958333,81.54847499999997
autograd::engine::evaluate_function: DivBackward0,12699,,aten::div,12702,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",12702,166.57014973958334,184.79924285714284
autograd::engine::evaluate_function: DivBackward0,12699,,aten::div,12703,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",12703,166.24104817708334,184.01066428571437
autograd::engine::evaluate_function: DivBackward0,12699,,aten::mul,12704,"[[4096, 4, 4096], [4096, 4, 4096]]","void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",12704,167.05843098958334,110.31445357142859
autograd::engine::evaluate_function: DivBackward0,12699,,aten::div,12705,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",12705,152.35042317708334,184.85446428571436
autograd::engine::evaluate_function: DivBackward0,12699,,aten::sum,12706,"[[4096, 4, 4096], [], [], []]","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",12706,136.54345703125,51.06018214285716
autograd::engine::evaluate_function: DivBackward0,12699,,aten::add,12707,"[[4096, 4, 4096], [4096, 4, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12707,136.71321614583334,105.93059999999991
autograd::engine::evaluate_function: MulBackward0,12710,,aten::mul,12712,"[[4096, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",12712,137.82486979166666,1.902392857142855
autograd::engine::evaluate_function: LinalgVectorNormBackward0,12713,,aten::div,12715,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",12715,117.76578776041667,183.64677857142854
autograd::engine::evaluate_function: LinalgVectorNormBackward0,12713,,aten::eq,12716,"[[4096, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, bool, at::native::(anonymous namespace)::CompareEqFunctor<c10::Half> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, bool, at::native::(anonymous namespace)::CompareEqFunctor<c10::Half> >, std::array<char*, 2ul>)",12716,167.60286458333334,1.911424999999999
autograd::engine::evaluate_function: LinalgVectorNormBackward0,12713,,aten::masked_fill_,12717,"[[4096, 4, 4096], [4096, 4, 1], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1} const&)::{lambda(int)#1})",12717,118.26936848958333,188.82771428571428
autograd::engine::evaluate_function: LinalgVectorNormBackward0,12713,,aten::mul,12720,"[[4096, 4, 1], [4096, 4, 4096]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",12720,166.04508463541666,173.58136785714288
autograd::engine::evaluate_function: LinalgVectorNormBackward0,12713,,aten::add_,12721,"[[4096, 4, 4096], [4096, 4, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12721,153.3203125,108.84452857142855
autograd::engine::evaluate_function: MmBackward0,12730,,aten::mm,12735,"[[4096, 16384], [16384, 2048]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize256x128x64_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,12735,139.32861328125,324.5201
autograd::engine::evaluate_function: MmBackward0,12730,,aten::mm,12742,"[[16384, 4096], [4096, 2048]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,12742,439.3865559895833,336.15762500000017
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,12752,,aten::add_,12754,"[[4096, 2048], [4096, 2048], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",12754,139.80631510416666,13.221696428571445
autograd::engine::evaluate_function: FlashAttnFuncBackward,12767,,FlashAttnFuncBackward,12768,"[[4, 4096, 16, 128]]","void flash_bwd_dot_do_o_kernel<true, Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::half_t> > >(Flash_bwd_params)",12768,117.94156901041667,77.92618214285707
autograd::engine::evaluate_function: FlashAttnFuncBackward,12767,,FlashAttnFuncBackward,12768,"[[4, 4096, 16, 128]]","void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::half_t> >, false, true, false, false, true, true>(Flash_bwd_params)",12768,167.48209635416666,2301.573771428573
autograd::engine::evaluate_function: FlashAttnFuncBackward,12767,,FlashAttnFuncBackward,12768,"[[4, 4096, 16, 128]]","void flash_bwd_convert_dq_kernel<Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::half_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::half_t> > >(Flash_bwd_params, int)",12768,167.88232421875,70.27882500000003
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,14852,,aten::mul,14855,"[[4096, 4, 16, 128], [4096, 1, 1, 128]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",14855,166.66634114583334,96.6238428571429
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,14852,,aten::mul,14859,"[[[4096, 4, 16, 64], [4096, 4, 16, 64]], [4096, 4, 16, 128], [4096, 1, 1, 128]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",14859,152.73372395833334,96.34144642857152
autograd::engine::evaluate_function: NegBackward0,14860,,aten::neg,14862,"[[4096, 4, 16, 64]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",14862,137.17106119791666,44.46820357142855
autograd::engine::evaluate_function: SliceBackward0,14863,,aten::fill_,14869,"[[4096, 4, 16, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",14869,135.80647786458334,21.369546428571425
autograd::engine::evaluate_function: SliceBackward0,14863,,aten::copy_,14872,"[[4096, 4, 16, 64], [4096, 4, 16, 64], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",14872,136.45328776041666,45.850874999999995
autograd::engine::evaluate_function: SliceBackward0,14863,,aten::add_,14873,"[[4096, 4, 16, 128], [4096, 4, 16, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",14873,150.24967447916666,55.522389285714304
autograd::engine::evaluate_function: SliceBackward0,14874,,aten::fill_,14880,"[[4096, 4, 16, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",14880,166.33430989583334,21.28474642857141
autograd::engine::evaluate_function: SliceBackward0,14874,,aten::copy_,14883,"[[4096, 4, 16, 64], [4096, 4, 16, 64], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",14883,168.24723307291666,45.794199999999975
autograd::engine::evaluate_function: SliceBackward0,14874,,aten::add_,14884,"[[4096, 4, 16, 128], [4096, 4, 16, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",14884,165.78125,55.4785
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,14885,,aten::mul,14888,"[[4096, 4, 16, 128], [4096, 1, 1, 128]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",14888,151.466796875,96.77867142857141
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,14885,,aten::mul,14892,"[[[4096, 4, 16, 64], [4096, 4, 16, 64]], [4096, 4, 16, 128], [4096, 1, 1, 128]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",14892,137.61962890625,96.27149999999995
autograd::engine::evaluate_function: NegBackward0,14893,,aten::neg,14895,"[[4096, 4, 16, 64]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",14895,136.71565755208334,44.39780714285717
autograd::engine::evaluate_function: SliceBackward0,14896,,aten::fill_,14902,"[[4096, 4, 16, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",14902,135.8515625,21.301757142857145
autograd::engine::evaluate_function: SliceBackward0,14896,,aten::copy_,14905,"[[4096, 4, 16, 64], [4096, 4, 16, 64], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",14905,150.32861328125,45.796246428571415
autograd::engine::evaluate_function: SliceBackward0,14896,,aten::add_,14906,"[[4096, 4, 16, 128], [4096, 4, 16, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",14906,165.95914713541666,55.445175
autograd::engine::evaluate_function: SliceBackward0,14907,,aten::fill_,14913,"[[4096, 4, 16, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",14913,167.16617838541666,21.291242857142855
autograd::engine::evaluate_function: SliceBackward0,14907,,aten::copy_,14916,"[[4096, 4, 16, 64], [4096, 4, 16, 64], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",14916,167.140625,45.8034392857143
autograd::engine::evaluate_function: SliceBackward0,14907,,aten::add_,14917,"[[4096, 4, 16, 128], [4096, 4, 16, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",14917,151.69856770833334,55.48437857142861
autograd::engine::evaluate_function: SplitBackward0,14918,,aten::cat,14920,"[[[4096, 4, 16, 128], [4096, 4, 16, 128], [4096, 4, 16, 128]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy_contig<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 128, 1>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",14920,135.26318359375,254.9206535714286
autograd::engine::evaluate_function: MmBackward0,14929,,aten::mm,14934,"[[6144, 16384], [16384, 4096]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize256x128x64_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,14934,386.2672526041667,943.2502714285711
autograd::engine::evaluate_function: MmBackward0,14929,,aten::mm,14941,"[[16384, 6144], [6144, 4096]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize256x128x32_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,14941,434.4938151041667,966.0446178571427
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,14951,,aten::add_,14953,"[[6144, 4096], [6144, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",14953,153.970703125,38.99064999999999
autograd::engine::evaluate_function: _CopyToModelParallelRegionBackward,14954,,record_param_comms,14957,"[[[4096, 4, 4096]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",14957,255.61686197916666,3041.787439285713
autograd::engine::evaluate_function: MulBackward0,14960,,aten::mul,14962,"[[4096, 4, 4096], [4096]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",14962,201.24544270833334,173.37305
autograd::engine::evaluate_function: MulBackward0,14960,,aten::mul,14963,"[[4096, 4, 4096], [4096, 4, 4096]]","void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",14963,152.35074869791666,110.34553214285711
autograd::engine::evaluate_function: MulBackward0,14960,,aten::sum,14964,"[[4096, 4, 4096], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",14964,401.0100911458333,58.139889285714276
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,14966,,aten::add_,14968,"[[4096], [4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",14968,169.56689453125,1.8089035714285708
autograd::engine::evaluate_function: DivBackward0,14969,,aten::neg,14971,"[[4096, 4, 4096]]","void at::native::vectorized_elementwise_kernel<4, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul> >(int, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul>)",14971,121.15250651041667,81.60530714285709
autograd::engine::evaluate_function: DivBackward0,14969,,aten::div,14972,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",14972,167.28629557291666,184.754925
autograd::engine::evaluate_function: DivBackward0,14969,,aten::div,14973,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",14973,165.95914713541666,184.0343428571428
autograd::engine::evaluate_function: DivBackward0,14969,,aten::mul,14974,"[[4096, 4, 4096], [4096, 4, 4096]]","void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",14974,166.42138671875,110.33053214285715
autograd::engine::evaluate_function: DivBackward0,14969,,aten::div,14975,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",14975,164.70084635416666,185.95332500000018
autograd::engine::evaluate_function: DivBackward0,14969,,aten::sum,14976,"[[4096, 4, 4096], [], [], []]","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",14976,166.62353515625,50.98749642857143
autograd::engine::evaluate_function: DivBackward0,14969,,aten::add_,14977,"[[4096, 4, 4096], [4096, 4, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",14977,167.68050130208334,104.65302142857146
autograd::engine::evaluate_function: MulBackward0,14980,,aten::mul,14982,"[[4096, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",14982,166.54736328125,1.9229607142857112
autograd::engine::evaluate_function: LinalgVectorNormBackward0,14983,,aten::div,14985,"[[4096, 4, 4096], [4096, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",14985,120.57291666666667,184.1974142857143
autograd::engine::evaluate_function: LinalgVectorNormBackward0,14983,,aten::eq,14986,"[[4096, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, bool, at::native::(anonymous namespace)::CompareEqFunctor<c10::Half> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, bool, at::native::(anonymous namespace)::CompareEqFunctor<c10::Half> >, std::array<char*, 2ul>)",14986,166.826171875,1.8995285714285692
autograd::engine::evaluate_function: LinalgVectorNormBackward0,14983,,aten::masked_fill_,14987,"[[4096, 4, 4096], [4096, 4, 1], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1} const&)::{lambda(int)#1})",14987,121.34993489583333,188.8156892857142
autograd::engine::evaluate_function: LinalgVectorNormBackward0,14983,,aten::mul,14990,"[[4096, 4, 1], [4096, 4, 4096]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",14990,165.64339192708334,173.58728571428568
autograd::engine::evaluate_function: LinalgVectorNormBackward0,14983,,aten::add_,14991,"[[4096, 4, 4096], [4096, 4, 4096], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",14991,136.35091145833334,108.79506071428574
