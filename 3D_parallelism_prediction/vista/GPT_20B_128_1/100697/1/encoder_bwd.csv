cpu_op_0,cpu_op_0_id,cpu_op_0_input_dim,cpu_op_1,cpu_op_1_id,cpu_op_1_input_dim,kernel,kernel_id,kernel_overhead(us),kernel_dur(us)
aten::zeros,15915,"[[], [], [], [], []]",aten::fill_,15918,"[[1, 1, 2048, 2048], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<bool>, std::array<char*, 1ul> >(int, at::native::FillFunctor<bool>, std::array<char*, 1ul>)",15918,0,2.5751255555555512
aten::to,15924,"[[], [], [], [], [], []]",aten::copy_,15927,"[[], [], []]",Memcpy HtoD (Pageable -> Device),15927,59219.074055989586,0.761667777777773
aten::layer_norm,15931,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,15932,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",15932,27113.832682291668,91.54979333333338
aten::layer_norm,15938,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,15939,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",15939,592.0047200520834,91.26877555555569
aten::linear,15948,"[[2048, 4, 6144], [2304, 6144], [2304]]",aten::addmm,15954,"[[2304], [8192, 6144], [6144, 2304], [], []]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,15954,14973.000813802084,291.3435722222225
aten::to,15976,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,15979,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),15979,1379.2958984375,2.827396666666669
aten::to,15980,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,15983,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),15983,12018.4638671875,2.7540066666666627
apply_rotary_pos_emb,15984,"[[2048, 4, 8, 24], [2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 1, 1, 24], []]",aten::neg,15993,"[[2048, 1, 1, 24], [2048, 4, 8, 12]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",15993,30073.81396484375,5.306142222222237
apply_rotary_pos_emb,15984,"[[2048, 4, 8, 24], [2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 1, 1, 24], []]",aten::cat,16005,"[[[2048, 4, 8, 12], [2048, 4, 8, 12]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",16005,17205.22265625,10.189545555555569
None,None,None,None,None,None,kernel_1,16006,10990.226399739584,4.689861111111125
apply_rotary_pos_emb,15984,"[[2048, 4, 8, 24], [2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 1, 1, 24], []]",aten::neg,16016,"[[2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 4, 8, 12]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",16016,15606.014811197916,5.0809977777777915
apply_rotary_pos_emb,15984,"[[2048, 4, 8, 24], [2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 1, 1, 24], []]",aten::cat,16027,"[[2048, 4, 8, 24], [[2048, 4, 8, 12], [2048, 4, 8, 12]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",16027,14808.384928385416,10.111867777777746
None,None,None,None,None,None,kernel_1,16028,9707.179850260416,4.56188555555557
aten::cat,16034,"[[[2048, 4, 8, 24], [2048, 4, 8, 72]], []]",aten::cat,16034,"[[[2048, 4, 8, 24], [2048, 4, 8, 72]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",16034,17324.863118489582,35.19125222222227
aten::cat,16035,"[[[2048, 4, 8, 24], [2048, 4, 8, 72]], []]",aten::cat,16035,"[[[2048, 4, 8, 24], [2048, 4, 8, 72]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",16035,570.8040364583334,34.855861111111125
aten::baddbmm,16045,"[[32, 2048, 2048], [32, 2048, 96], [32, 96, 2048], [], []]",aten::baddbmm,16045,"[[32, 2048, 2048], [32, 2048, 96], [32, 96, 2048], [], []]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16045,31629.6669921875,108.60845555555561
ScaledUpperTriangMaskedSoftmax,16048,"[[32, 2048, 2048], []]",ScaledUpperTriangMaskedSoftmax,16048,"[[32, 2048, 2048], []]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_forward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half const*, float, int, int, int)",16048,11455.236002604166,171.39504555555547
aten::bmm,16065,"[[32, 2048, 2048], [32, 2048, 96]]",aten::bmm,16065,"[[32, 2048, 2048], [32, 2048, 96]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16065,12015.347493489584,99.85313333333329
aten::contiguous,16069,"[[2048, 4, 8, 96], []]",aten::copy_,16073,"[[2048, 4, 8, 96], [2048, 4, 8, 96], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",16073,578.4161783854166,18.22591555555555
aten::linear,16075,"[[2048, 4, 768], [6144, 768], []]",aten::mm,16082,"[[8192, 768], [768, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16082,25408.058268229168,101.10299777777774
aten::add,16087,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,16087,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",16087,1956.9034830729167,132.59437555555547
aten::linear,16092,"[[2048, 4, 6144], [3072, 6144], []]",aten::mm,16099,"[[8192, 6144], [6144, 3072]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x128x32_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,16099,1972.2819010416667,381.6909911111109
None,None,None,None,None,None,kernel_2,16103,476.6998697916667,46.26760555555559
aten::linear,16107,"[[2048, 4, 3072], [6144, 3072], []]",aten::mm,16114,"[[8192, 3072], [3072, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16114,995.3531901041666,394.24040222222243
aten::add,16119,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,16119,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",16119,424.1984049479167,129.19809111111118
aten::add,16121,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,16121,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",16121,433.333984375,83.51953555555566
_ReduceFromModelParallelRegion,16122,"[[2048, 4, 6144]]",nccl:all_reduce,16125,"[[2048, 4, 6144]]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",16124,699.1350911458334,3639.8037688888935
aten::add,16129,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,16129,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",16129,589.89892578125,85.33485777777778
autograd::engine::evaluate_function: ExpandBackward0,16138,,aten::sum,16140,"[[2048, 4, 6144], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",16140,1122.5810546875,47.08581666666672
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,16142,,aten::add_,16144,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",16144,443.0930989583333,1.8130722222222173
autograd::engine::evaluate_function: MmBackward0,16149,,aten::mm,16154,"[[6144, 8192], [8192, 3072]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16154,1133.31640625,370.7534200000004
autograd::engine::evaluate_function: MmBackward0,16149,,aten::mm,16161,"[[8192, 6144], [6144, 3072]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16161,1215.4833984375,371.03415333333305
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,16171,,aten::add_,16173,"[[6144, 3072], [6144, 3072], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",16173,436.0305989583333,28.617035555555546
None,None,None,None,None,None,kernel_3,16177,467.19677734375,49.68110777777774
autograd::engine::evaluate_function: GeLUFunctionBackward,16174,,aten::sum,16181,"[[2048, 4, 3072], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",16181,1218.7197265625,31.564988888888934
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,16183,,aten::add_,16185,"[[3072], [3072], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",16185,398.2931315104167,1.7160822222222176
autograd::engine::evaluate_function: MmBackward0,16190,,aten::mm,16195,"[[3072, 8192], [8192, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16195,1268.38623046875,397.43792333333334
autograd::engine::evaluate_function: MmBackward0,16190,,aten::mm,16202,"[[8192, 3072], [3072, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16202,1273.8717447916667,362.30964666666665
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,16212,,aten::add_,16214,"[[3072, 6144], [3072, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",16214,799.4523111979166,29.516565555555566
autograd::engine::evaluate_function: _CopyToModelParallelRegionBackward,16215,,nccl:all_reduce,16219,"[[2048, 4, 6144]]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",16218,1002.375,3723.4915822222156
autograd::engine::evaluate_function: ExpandBackward0,16223,,aten::sum,16225,"[[2048, 4, 6144], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",16225,1158.2236328125,49.53219555555555
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,16227,,aten::add_,16229,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",16229,479.5983072916667,1.8056388888888824
autograd::engine::evaluate_function: MmBackward0,16234,,aten::mm,16239,"[[6144, 8192], [8192, 768]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16239,1238.51806640625,125.54744888888887
autograd::engine::evaluate_function: MmBackward0,16234,,aten::mm,16246,"[[8192, 6144], [6144, 768]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize256x128x64_warpgroupsize2x1x1_execute_segment_k_on_kernel__5x_cublas,16246,1274.89013671875,108.07251111111118
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,16256,,aten::add_,16258,"[[6144, 768], [6144, 768], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",16258,434.5771484375,8.168521111111076
autograd::engine::evaluate_function: BmmBackward0,16273,,aten::bmm,16277,"[[32, 2048, 2048], [32, 2048, 96]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16277,1229.5641276041667,91.87896777777776
autograd::engine::evaluate_function: BmmBackward0,16273,,aten::bmm,16280,"[[32, 2048, 96], [32, 96, 2048]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16280,1275.6095377604167,107.31681444444446
autograd::engine::evaluate_function: ScaledUpperTriangMaskedSoftmaxBackward,16297,,ScaledUpperTriangMaskedSoftmaxBackward,16298,"[[32, 2048, 2048]]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_backward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half*, c10::Half const*, float, int, int, int)",16298,482.6754557291667,281.48268444444415
autograd::engine::evaluate_function: BaddbmmBackward0,16311,,aten::bmm,16315,"[[32, 2048, 2048], [32, 2048, 96]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16315,1292.2506510416667,96.73611999999999
autograd::engine::evaluate_function: BaddbmmBackward0,16311,,aten::mul,16317,"[[32, 2048, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",16317,483.2158203125,9.397204444444453
autograd::engine::evaluate_function: BaddbmmBackward0,16311,,aten::bmm,16320,"[[32, 96, 2048], [32, 2048, 2048]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize64x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,16320,1263.3186848958333,90.89849111111126
autograd::engine::evaluate_function: BaddbmmBackward0,16311,,aten::mul,16322,"[[32, 96, 2048], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",16322,439.0377604166667,9.409188888888895
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,16359,,aten::mul,16362,"[[2048, 4, 8, 24], [2048, 1, 1, 24]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",16362,328.87939453125,7.29383333333334
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,16359,,aten::mul,16366,"[[[2048, 4, 8, 12], [2048, 4, 8, 12]], [2048, 4, 8, 24], [2048, 1, 1, 24]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",16366,375.46923828125,7.0474855555555695
autograd::engine::evaluate_function: NegBackward0,16367,,aten::neg,16369,"[[2048, 4, 8, 12]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",16369,417.0159505208333,3.6833244444444335
autograd::engine::evaluate_function: SliceBackward0,16370,,aten::fill_,16376,"[[2048, 4, 8, 24], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",16376,410.71044921875,2.191027777777771
autograd::engine::evaluate_function: SliceBackward0,16370,,aten::copy_,16379,"[[2048, 4, 8, 12], [2048, 4, 8, 12], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",16379,373.69970703125,7.419472222222217
autograd::engine::evaluate_function: SliceBackward0,16370,,aten::add_,16380,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",16380,346.1979166666667,13.717108888888896
autograd::engine::evaluate_function: SliceBackward0,16381,,aten::fill_,19459,"[[2048, 4, 8, 24], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",19459,324.5594075520833,2.146866666666675
autograd::engine::evaluate_function: SliceBackward0,16381,,aten::copy_,19462,"[[2048, 4, 8, 12], [2048, 4, 8, 12], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",19462,325.1805013020833,7.81370555555558
autograd::engine::evaluate_function: SliceBackward0,16381,,aten::add_,19463,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",19463,323.9890950520833,13.570328888888904
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,19464,,aten::mul,19467,"[[2048, 4, 8, 24], [2048, 1, 1, 24]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",19467,330.8131510416667,7.488520000000021
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,19464,,aten::mul,19471,"[[[2048, 4, 8, 12], [2048, 4, 8, 12]], [2048, 4, 8, 24], [2048, 1, 1, 24]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",19471,383.6681315104167,6.258872222222198
autograd::engine::evaluate_function: NegBackward0,19472,,aten::neg,19474,"[[2048, 4, 8, 12]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",19474,366.3294270833333,3.571107777777769
autograd::engine::evaluate_function: SliceBackward0,19475,,aten::fill_,19481,"[[2048, 4, 8, 24], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",19481,368.1671549479167,2.081941111111111
autograd::engine::evaluate_function: SliceBackward0,19475,,aten::copy_,19484,"[[2048, 4, 8, 12], [2048, 4, 8, 12], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",19484,336.2254231770833,3.938678888888889
autograd::engine::evaluate_function: SliceBackward0,19475,,aten::add_,19485,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",19485,328.9373372395833,6.4645944444444305
autograd::engine::evaluate_function: SliceBackward0,19486,,aten::fill_,19492,"[[2048, 4, 8, 24], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",19492,327.3829752604167,2.039054444444449
autograd::engine::evaluate_function: SliceBackward0,19486,,aten::copy_,19495,"[[2048, 4, 8, 12], [2048, 4, 8, 12], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",19495,322.2452799479167,3.8858399999999924
autograd::engine::evaluate_function: SliceBackward0,19486,,aten::add_,19496,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",19496,333.45654296875,6.387085555555541
autograd::engine::evaluate_function: SliceBackward0,19497,,aten::fill_,19503,"[[2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",19503,335.6114908854167,4.958298888888915
autograd::engine::evaluate_function: SliceBackward0,19497,,aten::copy_,19506,"[[2048, 4, 8, 72], [2048, 4, 8, 72], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",19506,410.0309244791667,35.501640000000044
autograd::engine::evaluate_function: SliceBackward0,19507,,aten::fill_,19513,"[[2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",19513,414.1472981770833,4.940020000000014
autograd::engine::evaluate_function: SliceBackward0,19507,,aten::copy_,19516,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",19516,369.71240234375,12.62665666666664
autograd::engine::evaluate_function: SliceBackward0,19507,,aten::add_,19517,"[[2048, 4, 8, 96], [2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19517,323.87890625,9.017829999999977
autograd::engine::evaluate_function: SliceBackward0,19518,,aten::fill_,19524,"[[2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",19524,325.1676432291667,4.997014444444471
autograd::engine::evaluate_function: SliceBackward0,19518,,aten::copy_,19527,"[[2048, 4, 8, 72], [2048, 4, 8, 72], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",19527,324.6025390625,14.824942222222234
autograd::engine::evaluate_function: SliceBackward0,19528,,aten::fill_,19534,"[[2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",19534,370.3264973958333,5.026278888888904
autograd::engine::evaluate_function: SliceBackward0,19528,,aten::copy_,19537,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",19537,388.8878580729167,6.702604444444438
autograd::engine::evaluate_function: SliceBackward0,19528,,aten::add_,19538,"[[2048, 4, 8, 96], [2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19538,401.0514322916667,9.02980000000001
autograd::engine::evaluate_function: SplitBackward0,19539,,aten::cat,19541,"[[[2048, 4, 8, 96], [2048, 4, 8, 96], [2048, 4, 8, 96]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",19541,354.68994140625,75.99980000000002
autograd::engine::evaluate_function: AddmmBackward0,19550,,aten::mm,19555,"[[8192, 2304], [2304, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,19555,1205.46533203125,276.22511888888897
autograd::engine::evaluate_function: AddmmBackward0,19550,,aten::mm,19559,"[[2304, 8192], [8192, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_on_kernel__5x_cublas,19559,2132.52685546875,287.8547422222223
autograd::engine::evaluate_function: AddmmBackward0,19550,,aten::sum,19563,"[[8192, 2304], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",19563,1249.9119466145833,25.75172555555554
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,19565,,aten::add_,19567,"[[2304], [2304], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19567,358.1189778645833,1.6503388888888815
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,19573,,aten::add_,19575,"[[2304, 6144], [2304, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19575,373.9695638020833,20.6387933333333
autograd::engine::evaluate_function: _CopyToModelParallelRegionBackward,19580,,nccl:all_reduce,19584,"[[2048, 4, 6144]]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",19583,992.5206705729166,3682.000821111114
autograd::engine::evaluate_function: NativeLayerNormBackward0,19586,,aten::native_layer_norm_backward,19588,"[[2048, 4, 6144], [2048, 4, 6144], [], [2048, 4, 1], [2048, 4, 1], [6144], [6144], []]","void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<c10::Half, float>(c10::Half const*, c10::Half const*, float const*, float const*, c10::Half const*, c10::Half*, int)",19588,636.5384114583334,140.86593555555547
autograd::engine::evaluate_function: NativeLayerNormBackward0,19586,,aten::native_layer_norm_backward,19588,"[[2048, 4, 6144], [2048, 4, 6144], [], [2048, 4, 1], [2048, 4, 1], [6144], [6144], []]","void at::native::(anonymous namespace)::GammaBetaBackwardCUDAKernel_32x32<c10::Half, float>(long, long, c10::Half const*, c10::Half const*, float const*, float const*, c10::Half*, c10::Half*)",19588,521.4723307291666,88.8659122222222
autograd::engine::evaluate_function: NativeLayerNormBackward0,19586,,aten::add,19592,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19592,518.6595052083334,78.97512333333323
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,19593,,aten::add_,19595,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19595,517.0602213541666,1.8832244444444406
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,19596,,aten::add_,19598,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19598,372.6915690104167,1.705768888888882
autograd::engine::evaluate_function: NativeLayerNormBackward0,19599,,aten::native_layer_norm_backward,19601,"[[2048, 4, 6144], [2048, 4, 6144], [], [2048, 4, 1], [2048, 4, 1], [6144], [6144], []]","void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<c10::Half, float>(c10::Half const*, c10::Half const*, float const*, float const*, c10::Half const*, c10::Half*, int)",19601,377.51318359375,137.57940888888902
autograd::engine::evaluate_function: NativeLayerNormBackward0,19599,,aten::native_layer_norm_backward,19601,"[[2048, 4, 6144], [2048, 4, 6144], [], [2048, 4, 1], [2048, 4, 1], [6144], [6144], []]","void at::native::(anonymous namespace)::GammaBetaBackwardCUDAKernel_32x32<c10::Half, float>(long, long, c10::Half const*, c10::Half const*, float const*, float const*, c10::Half*, c10::Half*)",19601,513.6697591145834,88.91071111111111
autograd::engine::evaluate_function: NativeLayerNormBackward0,19599,,aten::add_,19605,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19605,516.787109375,77.63718888888893
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,19610,,aten::add_,19612,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19612,516.47900390625,1.8612166666666585
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,19613,,aten::add_,19615,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19615,328.2233072916667,1.6984455555555478
