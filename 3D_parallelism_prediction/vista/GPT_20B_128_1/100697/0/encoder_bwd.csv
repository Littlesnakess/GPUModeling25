cpu_op_0,cpu_op_0_id,cpu_op_0_input_dim,cpu_op_1,cpu_op_1_id,cpu_op_1_input_dim,kernel,kernel_id,kernel_overhead(us),kernel_dur(us)
aten::zeros,17380,"[[], [], [], [], []]",aten::fill_,17383,"[[1, 1, 2048, 2048], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<bool>, std::array<char*, 1ul> >(int, at::native::FillFunctor<bool>, std::array<char*, 1ul>)",17383,0,2.5811624242424207
aten::to,17389,"[[], [], [], [], [], []]",aten::copy_,17392,"[[], [], []]",Memcpy HtoD (Pageable -> Device),17392,55589.384602864586,0.7492606060605993
aten::layer_norm,17396,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,17397,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",17397,26953.8291015625,91.54653454545456
aten::layer_norm,17403,"[[2048, 4, 6144], [], [6144], [6144], [], []]",aten::native_layer_norm,17404,"[[2048, 4, 6144], [], [6144], [6144], []]","void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::Half, float>(int, float, c10::Half const*, c10::Half const*, c10::Half const*, float*, float*, c10::Half*)",17404,805.34814453125,91.27632000000001
aten::linear,19973,"[[2048, 4, 6144], [2304, 6144], [2304]]",aten::addmm,19979,"[[2304], [8192, 6144], [6144, 2304], [], []]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,19979,18332.5009765625,292.56946666666664
aten::to,20001,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,20004,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),20004,1572.7335611979167,2.9319175757575735
aten::to,20005,"[[2048, 1, 1, 24], [], [], [], [], [], [], []]",aten::copy_,20008,"[[2048, 1, 1, 24], [2048, 1, 1, 24], []]",Memcpy HtoD (Pageable -> Device),20008,11980.821940104166,2.72859757575758
apply_rotary_pos_emb,20009,"[[2048, 4, 8, 24], [2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 1, 1, 24], []]",aten::neg,20018,"[[2048, 1, 1, 24], [2048, 4, 8, 12]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20018,28457.647623697918,5.612174545454562
apply_rotary_pos_emb,20009,"[[2048, 4, 8, 24], [2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 1, 1, 24], []]",aten::cat,20030,"[[[2048, 4, 8, 12], [2048, 4, 8, 12]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",20030,16416.743489583332,10.139873939393922
None,None,None,None,None,None,kernel_1,20031,10442.62060546875,4.70051393939395
apply_rotary_pos_emb,20009,"[[2048, 4, 8, 24], [2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 1, 1, 24], []]",aten::neg,20041,"[[2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 4, 8, 12]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20041,14642.098958333334,5.048581818181839
apply_rotary_pos_emb,20009,"[[2048, 4, 8, 24], [2048, 4, 8, 24], [2048, 1, 1, 24], [2048, 1, 1, 24], []]",aten::cat,20052,"[[2048, 4, 8, 24], [[2048, 4, 8, 12], [2048, 4, 8, 12]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",20052,13716.809407552084,10.050578181818157
None,None,None,None,None,None,kernel_1,20053,9669.628092447916,4.525067878787883
aten::cat,20059,"[[[2048, 4, 8, 24], [2048, 4, 8, 72]], []]",aten::cat,20059,"[[[2048, 4, 8, 24], [2048, 4, 8, 72]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",20059,15747.093098958334,35.05269454545449
aten::cat,20060,"[[[2048, 4, 8, 24], [2048, 4, 8, 72]], []]",aten::cat,20060,"[[[2048, 4, 8, 24], [2048, 4, 8, 72]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",20060,668.9046223958334,34.862562424242434
aten::baddbmm,20070,"[[32, 2048, 2048], [32, 2048, 96], [32, 96, 2048], [], []]",aten::baddbmm,20070,"[[32, 2048, 2048], [32, 2048, 96], [32, 96, 2048], [], []]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20070,30861.53466796875,108.2905963636363
ScaledUpperTriangMaskedSoftmax,20073,"[[32, 2048, 2048], []]",ScaledUpperTriangMaskedSoftmax,20073,"[[32, 2048, 2048], []]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_forward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half const*, float, int, int, int)",20073,10296.714192708334,171.06383515151504
aten::bmm,20090,"[[32, 2048, 2048], [32, 2048, 96]]",aten::bmm,20090,"[[32, 2048, 2048], [32, 2048, 96]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20090,13812.871419270834,99.80453454545457
aten::contiguous,20094,"[[2048, 4, 8, 96], []]",aten::copy_,20098,"[[2048, 4, 8, 96], [2048, 4, 8, 96], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20098,755.5462239583334,18.18423757575757
aten::linear,20100,"[[2048, 4, 768], [6144, 768], []]",aten::mm,20107,"[[8192, 768], [768, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20107,26269.466471354168,102.02893696969686
aten::add,20112,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,20112,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20112,622.7317708333334,132.8983866666668
aten::linear,20117,"[[2048, 4, 6144], [3072, 6144], []]",aten::mm,20124,"[[8192, 6144], [6144, 3072]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x128x32_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,20124,2548.5149739583335,383.9788521212116
None,None,None,None,None,None,kernel_2,20128,477.0234375,46.42913090909085
aten::linear,20132,"[[2048, 4, 3072], [6144, 3072], []]",aten::mm,20139,"[[8192, 3072], [3072, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20139,970.1883138020834,395.00225333333384
aten::add,20144,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,20144,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20144,431.1228841145833,129.17117333333343
aten::add,20146,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,20146,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20146,431.2755533854167,83.51235757575753
_ReduceFromModelParallelRegion,20147,"[[2048, 4, 6144]]",record_param_comms,20149,"[[[2048, 4, 6144]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",20149,620.2151692708334,3654.9640400000003
aten::add,20154,"[[2048, 4, 6144], [2048, 4, 6144], []]",aten::add,20154,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20154,548.8251953125,85.37897818181825
autograd::engine::evaluate_function: ExpandBackward0,20163,,aten::sum,20165,"[[2048, 4, 6144], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",20165,1052.7648111979167,46.76917333333335
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20167,,aten::add_,20169,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20169,441.400390625,1.8510606060606012
autograd::engine::evaluate_function: MmBackward0,20174,,aten::mm,20179,"[[6144, 8192], [8192, 3072]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20179,1084.8483072916667,372.54247393939363
autograd::engine::evaluate_function: MmBackward0,20174,,aten::mm,20186,"[[8192, 6144], [6144, 3072]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20186,1275.8826497395833,371.65257212121236
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20196,,aten::add_,20198,"[[6144, 3072], [6144, 3072], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20198,447.0071614583333,28.533095757575758
None,None,None,None,None,None,kernel_3,20202,469.7669270833333,49.65741696969693
autograd::engine::evaluate_function: GeLUFunctionBackward,20199,,aten::sum,20206,"[[2048, 4, 3072], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",20206,1183.4850260416667,31.452903030303048
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20208,,aten::add_,20210,"[[3072], [3072], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20210,413.86376953125,1.7516484848484783
autograd::engine::evaluate_function: MmBackward0,20215,,aten::mm,20220,"[[3072, 8192], [8192, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20220,1209.4098307291667,398.1580109090907
autograd::engine::evaluate_function: MmBackward0,20215,,aten::mm,20227,"[[8192, 3072], [3072, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20227,1214.0402018229167,361.77616848484837
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20237,,aten::add_,20239,"[[3072, 6144], [3072, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20239,463.0885416666667,29.493212121212142
autograd::engine::evaluate_function: _CopyToModelParallelRegionBackward,20240,,record_param_comms,20243,"[[[2048, 4, 6144]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",20243,699.7667643229166,3727.8228824242383
autograd::engine::evaluate_function: ExpandBackward0,20248,,aten::sum,20250,"[[2048, 4, 6144], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",20250,1073.2288411458333,49.429591515151486
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20252,,aten::add_,20254,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20254,483.2408854166667,1.806373333333331
autograd::engine::evaluate_function: MmBackward0,20259,,aten::mm,20264,"[[6144, 8192], [8192, 768]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20264,1176.04296875,125.69806060606052
autograd::engine::evaluate_function: MmBackward0,20259,,aten::mm,20271,"[[8192, 6144], [6144, 768]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize256x128x64_warpgroupsize2x1x1_execute_segment_k_on_kernel__5x_cublas,20271,1205.5128580729167,108.2796630303031
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20281,,aten::add_,20283,"[[6144, 768], [6144, 768], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20283,437.0231119791667,8.17793090909087
autograd::engine::evaluate_function: BmmBackward0,20298,,aten::bmm,20302,"[[32, 2048, 2048], [32, 2048, 96]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20302,1188.6259765625,91.99334666666677
autograd::engine::evaluate_function: BmmBackward0,20298,,aten::bmm,20305,"[[32, 2048, 96], [32, 96, 2048]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20305,1247.2853190104167,107.17821575757567
autograd::engine::evaluate_function: ScaledUpperTriangMaskedSoftmaxBackward,20322,,ScaledUpperTriangMaskedSoftmaxBackward,20323,"[[32, 2048, 2048]]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_backward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half*, c10::Half const*, float, int, int, int)",20323,484.4012044270833,281.06264484848487
autograd::engine::evaluate_function: BaddbmmBackward0,20336,,aten::bmm,20340,"[[32, 2048, 2048], [32, 2048, 96]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20340,1244.0008138020833,96.56497212121222
autograd::engine::evaluate_function: BaddbmmBackward0,20336,,aten::mul,20342,"[[32, 2048, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",20342,487.1018880208333,9.430227878787937
autograd::engine::evaluate_function: BaddbmmBackward0,20336,,aten::bmm,20345,"[[32, 96, 2048], [32, 2048, 2048]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize64x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20345,1204.7734375,90.84464727272719
autograd::engine::evaluate_function: BaddbmmBackward0,20336,,aten::mul,20347,"[[32, 96, 2048], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",20347,441.11083984375,9.420254545454569
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,20384,,aten::mul,20387,"[[2048, 4, 8, 24], [2048, 1, 1, 24]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20387,341.8771158854167,7.339696969696971
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,20384,,aten::mul,20391,"[[[2048, 4, 8, 12], [2048, 4, 8, 12]], [2048, 4, 8, 24], [2048, 1, 1, 24]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20391,385.5260416666667,7.057712727272716
autograd::engine::evaluate_function: NegBackward0,20392,,aten::neg,20394,"[[2048, 4, 8, 12]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20394,422.41455078125,3.6841478787878668
autograd::engine::evaluate_function: SliceBackward0,20395,,aten::fill_,20401,"[[2048, 4, 8, 24], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20401,389.4246419270833,2.1968848484848493
autograd::engine::evaluate_function: SliceBackward0,20395,,aten::copy_,20404,"[[2048, 4, 8, 12], [2048, 4, 8, 12], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20404,402.9957682291667,7.405101818181824
autograd::engine::evaluate_function: SliceBackward0,20395,,aten::add_,20405,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20405,338.8790690104167,13.983837575757578
autograd::engine::evaluate_function: SliceBackward0,20406,,aten::fill_,20412,"[[2048, 4, 8, 24], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20412,335.6638997395833,2.1461187878787946
autograd::engine::evaluate_function: SliceBackward0,20406,,aten::copy_,20415,"[[2048, 4, 8, 12], [2048, 4, 8, 12], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20415,339.0810546875,7.703624242424262
autograd::engine::evaluate_function: SliceBackward0,20406,,aten::add_,20416,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20416,334.904296875,13.763094545454555
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,20417,,aten::mul,20420,"[[2048, 4, 8, 24], [2048, 1, 1, 24]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20420,338.97021484375,7.464447272727272
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,20417,,aten::mul,20424,"[[[2048, 4, 8, 12], [2048, 4, 8, 12]], [2048, 4, 8, 24], [2048, 1, 1, 24]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20424,428.9474283854167,6.2534484848484695
autograd::engine::evaluate_function: NegBackward0,20425,,aten::neg,20427,"[[2048, 4, 8, 12]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20427,400.13232421875,3.5709006060606003
autograd::engine::evaluate_function: SliceBackward0,20428,,aten::fill_,20434,"[[2048, 4, 8, 24], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20434,430.6896158854167,2.0791333333333317
autograd::engine::evaluate_function: SliceBackward0,20428,,aten::copy_,20437,"[[2048, 4, 8, 12], [2048, 4, 8, 12], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20437,374.7854817708333,3.9214618181818155
autograd::engine::evaluate_function: SliceBackward0,20428,,aten::add_,20438,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20438,339.9055989583333,6.501023030303024
autograd::engine::evaluate_function: SliceBackward0,20439,,aten::fill_,20445,"[[2048, 4, 8, 24], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20445,339.2296549479167,2.0476666666666694
autograd::engine::evaluate_function: SliceBackward0,20439,,aten::copy_,20448,"[[2048, 4, 8, 12], [2048, 4, 8, 12], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20448,332.8463541666667,3.8903236363636284
autograd::engine::evaluate_function: SliceBackward0,20439,,aten::add_,20449,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20449,361.4822591145833,6.371821818181804
autograd::engine::evaluate_function: SliceBackward0,20450,,aten::fill_,20456,"[[2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20456,352.12255859375,4.954842424242441
autograd::engine::evaluate_function: SliceBackward0,20450,,aten::copy_,20459,"[[2048, 4, 8, 72], [2048, 4, 8, 72], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20459,430.1276041666667,35.247690909090906
autograd::engine::evaluate_function: SliceBackward0,20460,,aten::fill_,20466,"[[2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20466,426.34375,4.973455757575767
autograd::engine::evaluate_function: SliceBackward0,20460,,aten::copy_,20469,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20469,380.5032552083333,12.577512727272685
autograd::engine::evaluate_function: SliceBackward0,20460,,aten::add_,20470,"[[2048, 4, 8, 96], [2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20470,334.9495442708333,9.017614545454562
autograd::engine::evaluate_function: SliceBackward0,20471,,aten::fill_,20477,"[[2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20477,337.4143880208333,5.005766060606077
autograd::engine::evaluate_function: SliceBackward0,20471,,aten::copy_,20480,"[[2048, 4, 8, 72], [2048, 4, 8, 72], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20480,342.2560221354167,14.88068727272728
autograd::engine::evaluate_function: SliceBackward0,20481,,aten::fill_,20487,"[[2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20487,382.64013671875,5.029933333333341
autograd::engine::evaluate_function: SliceBackward0,20481,,aten::copy_,20490,"[[2048, 4, 8, 24], [2048, 4, 8, 24], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20490,415.3634440104167,6.689185454545453
autograd::engine::evaluate_function: SliceBackward0,20481,,aten::add_,20491,"[[2048, 4, 8, 96], [2048, 4, 8, 96], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20491,408.50634765625,9.048869090909085
autograd::engine::evaluate_function: SplitBackward0,20492,,aten::cat,20494,"[[[2048, 4, 8, 96], [2048, 4, 8, 96], [2048, 4, 8, 96]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",20494,365.53955078125,75.93540363636367
autograd::engine::evaluate_function: AddmmBackward0,20503,,aten::mm,20508,"[[8192, 2304], [2304, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20508,1163.5994466145833,275.9504072727273
autograd::engine::evaluate_function: AddmmBackward0,20503,,aten::mm,20512,"[[2304, 8192], [8192, 6144]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_on_kernel__5x_cublas,20512,1987.7255859375,289.42198181818213
autograd::engine::evaluate_function: AddmmBackward0,20503,,aten::sum,20516,"[[8192, 2304], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",20516,1197.28125,25.611848484848473
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20518,,aten::add_,20520,"[[2304], [2304], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20520,347.6555989583333,1.6315915151515086
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20526,,aten::add_,20528,"[[2304, 6144], [2304, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20528,389.32177734375,20.710464242424205
autograd::engine::evaluate_function: _CopyToModelParallelRegionBackward,20533,,record_param_comms,20536,"[[[2048, 4, 6144]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",20536,735.7897135416666,3688.7300424242417
autograd::engine::evaluate_function: NativeLayerNormBackward0,20539,,aten::native_layer_norm_backward,20541,"[[2048, 4, 6144], [2048, 4, 6144], [], [2048, 4, 1], [2048, 4, 1], [6144], [6144], []]","void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<c10::Half, float>(c10::Half const*, c10::Half const*, float const*, float const*, c10::Half const*, c10::Half*, int)",20541,598.662109375,140.72397575757583
autograd::engine::evaluate_function: NativeLayerNormBackward0,20539,,aten::native_layer_norm_backward,20541,"[[2048, 4, 6144], [2048, 4, 6144], [], [2048, 4, 1], [2048, 4, 1], [6144], [6144], []]","void at::native::(anonymous namespace)::GammaBetaBackwardCUDAKernel_32x32<c10::Half, float>(long, long, c10::Half const*, c10::Half const*, float const*, float const*, c10::Half*, c10::Half*)",20541,525.1070963541666,88.92509575757578
autograd::engine::evaluate_function: NativeLayerNormBackward0,20539,,aten::add,20545,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20545,523.32373046875,78.98286060606058
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20546,,aten::add_,20548,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20548,521.97265625,1.8585478787878729
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20549,,aten::add_,20551,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20551,390.2853190104167,1.7014909090909018
autograd::engine::evaluate_function: NativeLayerNormBackward0,20552,,aten::native_layer_norm_backward,20554,"[[2048, 4, 6144], [2048, 4, 6144], [], [2048, 4, 1], [2048, 4, 1], [6144], [6144], []]","void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<c10::Half, float>(c10::Half const*, c10::Half const*, float const*, float const*, c10::Half const*, c10::Half*, int)",20554,389.3199869791667,137.4687781818183
autograd::engine::evaluate_function: NativeLayerNormBackward0,20552,,aten::native_layer_norm_backward,20554,"[[2048, 4, 6144], [2048, 4, 6144], [], [2048, 4, 1], [2048, 4, 1], [6144], [6144], []]","void at::native::(anonymous namespace)::GammaBetaBackwardCUDAKernel_32x32<c10::Half, float>(long, long, c10::Half const*, c10::Half const*, float const*, float const*, c10::Half*, c10::Half*)",20554,521.0958658854166,88.75232848484845
autograd::engine::evaluate_function: NativeLayerNormBackward0,20552,,aten::add_,20558,"[[2048, 4, 6144], [2048, 4, 6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20558,520.623046875,77.6700763636365
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20563,,aten::add_,20565,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20565,520.1598307291666,1.8576084848484802
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20566,,aten::add_,20568,"[[6144], [6144], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20568,343.10546875,1.6766339393939311
