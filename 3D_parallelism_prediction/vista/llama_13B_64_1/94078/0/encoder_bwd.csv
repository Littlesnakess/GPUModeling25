cpu_op_0,cpu_op_0_id,cpu_op_0_input_dim,cpu_op_1,cpu_op_1_id,cpu_op_1_input_dim,kernel,kernel_id,kernel_overhead(us),kernel_dur(us)
aten::zeros,17037,"[[], [], [], [], []]",aten::fill_,17040,"[[1, 1, 2048, 2048], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<bool>, std::array<char*, 1ul> >(int, at::native::FillFunctor<bool>, std::array<char*, 1ul>)",17040,0,2.5756813333333337
aten::zeros,17041,"[[], [], [], [], []]",aten::fill_,17044,"[[41943040], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",17044,24474.257161458332,26.329699999999992
aten::copy_,17048,"[[5242880], [5242880], []]",aten::copy_,17048,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17048,6868.5078125,10.249147999999986
c10d::allgather_,17070,"[[], [[5242880]], [], []]",nccl:all_gather,17081,[[5242880]],"ncclDevKernel_AllGather_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",17071,48055.643717447914,1594.4876786666673
c10d::allgather_,17070,"[[], [[5242880]], [], []]",aten::copy_,17084,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17084,357.7770182291667,9.520469333333333
c10d::allgather_,17070,"[[], [[5242880]], [], []]",aten::copy_,17087,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17087,276.8434244791667,8.53377999999999
c10d::allgather_,17070,"[[], [[5242880]], [], []]",aten::copy_,17090,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17090,275.6207682291667,10.14320666666665
c10d::allgather_,17070,"[[], [[5242880]], [], []]",aten::copy_,17093,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17093,275.1082356770833,10.234421333333334
c10d::allgather_,17070,"[[], [[5242880]], [], []]",aten::copy_,17096,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17096,275.4361979166667,10.02621733333333
c10d::allgather_,17070,"[[], [[5242880]], [], []]",aten::copy_,17099,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17099,274.583984375,9.90968533333334
c10d::allgather_,17070,"[[], [[5242880]], [], []]",aten::copy_,17102,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17102,274.5787760416667,9.859666666666671
c10d::allgather_,17070,"[[], [[5242880]], [], []]",aten::copy_,17105,"[[5242880], [5242880], []]",Memcpy DtoD (Device -> Device),17105,274.9441731770833,9.903705333333349
aten::to,17119,"[[], [], [], [], [], []]",aten::copy_,17122,"[[], [], []]",Memcpy HtoD (Pageable -> Device),17122,213.78824869791666,0.7659039999999915
aten::linalg_vector_norm,17126,"[[2048, 4, 5120], [], [], [], []]",aten::linalg_vector_norm,17126,"[[2048, 4, 5120], [], [], [], []]","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<c10::Half, at::native::NormTwoOps<c10::Half, float, c10::Half>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::NormTwoOps<c10::Half, float, c10::Half>, unsigned int, c10::Half, 4>)",17126,22805.681803385418,33.809605333333366
aten::mul,17127,"[[2048, 4, 1], []]",aten::mul,17127,"[[2048, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",17127,2990.46044921875,1.4249293333333366
aten::add,17128,"[[2048, 4, 1], [], []]",aten::add,17128,"[[2048, 4, 1], [], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<c10::Half>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<c10::Half>, std::array<char*, 2ul>)",17128,7340.737955729167,1.3314466666666724
aten::div,17129,"[[2048, 4, 5120], [2048, 4, 1]]",aten::div,17129,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",17129,6670.946614583333,113.03087599999995
aten::mul,17130,"[[5120], [2048, 4, 5120]]",aten::mul,17130,"[[5120], [2048, 4, 5120]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",17130,495.8037109375,109.56652399999993
aten::linear,17135,"[[2048, 4, 5120], [1920, 5120], []]",aten::mm,17142,"[[8192, 5120], [5120, 1920]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17142,956.7399088541666,211.13749866666683
aten::to,17155,"[[2048, 1, 1, 128], [], [], [], [], [], [], []]",aten::copy_,17158,"[[2048, 1, 1, 128], [2048, 1, 1, 128], []]",Memcpy HtoD (Pageable -> Device),17158,1231.9983723958333,4.302774666666673
aten::to,17159,"[[2048, 1, 1, 128], [], [], [], [], [], [], []]",aten::copy_,17162,"[[2048, 1, 1, 128], [2048, 1, 1, 128], []]",Memcpy HtoD (Pageable -> Device),17162,10703.5087890625,4.080005333333331
apply_rotary_pos_emb,17163,"[[2048, 4, 5, 128], [2048, 4, 5, 128], [2048, 1, 1, 128], [2048, 1, 1, 128], []]",aten::neg,17172,"[[2048, 1, 1, 128], [2048, 4, 5, 64]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",17172,27047.065592447918,8.57571333333332
apply_rotary_pos_emb,17163,"[[2048, 4, 5, 128], [2048, 4, 5, 128], [2048, 1, 1, 128], [2048, 1, 1, 128], []]",aten::cat,17184,"[[[2048, 4, 5, 64], [2048, 4, 5, 64]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",17184,14405.625651041666,25.32390133333334
None,None,None,None,None,None,kernel_1,17185,6247.796875,9.764206666666672
apply_rotary_pos_emb,17163,"[[2048, 4, 5, 128], [2048, 4, 5, 128], [2048, 1, 1, 128], [2048, 1, 1, 128], []]",aten::neg,17195,"[[2048, 4, 5, 128], [2048, 1, 1, 128], [2048, 4, 5, 64]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",17195,12905.558919270834,9.905809333333334
apply_rotary_pos_emb,17163,"[[2048, 4, 5, 128], [2048, 4, 5, 128], [2048, 1, 1, 128], [2048, 1, 1, 128], []]",aten::cat,17206,"[[2048, 4, 5, 128], [[2048, 4, 5, 64], [2048, 4, 5, 64]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",17206,12019.560872395834,25.99156399999999
None,None,None,None,None,None,kernel_1,17207,4911.109700520833,9.846213333333344
aten::baddbmm,17222,"[[20, 2048, 2048], [20, 2048, 128], [20, 128, 2048], [], []]",aten::baddbmm,17222,"[[20, 2048, 2048], [20, 2048, 128], [20, 128, 2048], [], []]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17222,39682.7021484375,67.06896133333335
ScaledUpperTriangMaskedSoftmax,17225,"[[20, 2048, 2048], []]",ScaledUpperTriangMaskedSoftmax,17225,"[[20, 2048, 2048], []]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_forward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half const*, float, int, int, int)",17225,20908.870930989582,109.43303866666676
aten::bmm,17242,"[[20, 2048, 2048], [20, 2048, 128]]",aten::bmm,17242,"[[20, 2048, 2048], [20, 2048, 128]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17242,27193.719889322918,65.68964400000009
aten::contiguous,17246,"[[2048, 4, 5, 128], []]",aten::copy_,17250,"[[2048, 4, 5, 128], [2048, 4, 5, 128], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",17250,6702.4384765625,15.002846666666686
aten::linear,17252,"[[2048, 4, 640], [5120, 640], []]",aten::mm,17259,"[[8192, 640], [640, 5120]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17259,27888.195963541668,74.06290800000002
_ReduceFromModelParallelRegion,17261,"[[2048, 4, 5120]]",record_param_comms,17263,"[[[2048, 4, 5120]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",17263,14614.41015625,3099.997553333326
aten::add,17269,"[[2048, 4, 5120], [2048, 4, 5120], []]",aten::add,17269,"[[2048, 4, 5120], [2048, 4, 5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",17269,380.1160481770833,71.41632133333327
aten::linalg_vector_norm,17270,"[[2048, 4, 5120], [], [], [], []]",aten::linalg_vector_norm,17270,"[[2048, 4, 5120], [], [], [], []]","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<c10::Half, at::native::NormTwoOps<c10::Half, float, c10::Half>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::NormTwoOps<c10::Half, float, c10::Half>, unsigned int, c10::Half, 4>)",17270,382.9405924479167,33.92157866666666
aten::mul,17271,"[[2048, 4, 1], []]",aten::mul,17271,"[[2048, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",17271,395.9326171875,1.6171386666666618
aten::add,17272,"[[2048, 4, 1], [], []]",aten::add,17272,"[[2048, 4, 1], [], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<c10::Half>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<c10::Half>, std::array<char*, 2ul>)",17272,310.31640625,1.3453146666666718
aten::div,17273,"[[2048, 4, 5120], [2048, 4, 1]]",aten::div,17273,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",17273,306.7814127604167,112.89797733333349
aten::mul,17274,"[[5120], [2048, 4, 5120]]",aten::mul,17274,"[[5120], [2048, 4, 5120]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",17274,423.9791666666667,109.29348933333338
aten::to,17277,"[[], [], [], [], [], []]",aten::copy_,17280,"[[], [], []]",Memcpy HtoD (Pageable -> Device),17280,228.0185546875,0.7572866666666643
aten::linear,17287,"[[2048, 4, 5120], [2560, 5120], []]",aten::mm,17294,"[[8192, 5120], [5120, 2560]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17294,41092.726399739586,269.9104546666664
aten::add,17296,"[[2048, 4, 2560], [2560], []]",aten::add,17296,"[[2048, 4, 2560], [2560], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",17296,486.4503580729167,55.72250533333338
aten::gelu,17297,"[[2048, 4, 2560], []]",aten::gelu,17297,"[[2048, 4, 2560], []]","void at::native::vectorized_elementwise_kernel<4, at::native::GeluCUDAKernelImpl(at::TensorIteratorBase&, at::native::GeluType)::{lambda()#2}::operator()() const::{lambda()#3}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul> >(int, at::native::GeluCUDAKernelImpl(at::TensorIteratorBase&, at::native::GeluType)::{lambda()#2}::operator()() const::{lambda()#3}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul>)",17297,408.70458984375,35.868164000000014
aten::linear,17298,"[[2048, 4, 2560], [5120, 2560], []]",aten::mm,17305,"[[8192, 2560], [2560, 5120]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17305,845.6007486979166,263.28946266666645
_ReduceFromModelParallelRegion,17307,"[[2048, 4, 5120]]",record_param_comms,17309,"[[[2048, 4, 5120]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",17309,538.1044921875,3116.6112893333343
aten::add,17317,"[[2048, 4, 5120], [2048, 4, 5120], []]",aten::add,17317,"[[2048, 4, 5120], [2048, 4, 5120], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",17317,379.7400716145833,108.31470400000005
aten::add,17319,"[[2048, 4, 5120], [2048, 4, 5120], []]",aten::add,17319,"[[2048, 4, 5120], [2048, 4, 5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",17319,385.74658203125,70.07767999999993
autograd::engine::evaluate_function: ExpandBackward0,17324,,aten::sum,17326,"[[2048, 4, 5120], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",17326,950.4889322916666,42.65203066666667
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,17328,,aten::add_,17330,"[[5120], [5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",17330,397.3961588541667,1.789806666666661
autograd::engine::evaluate_function: MmBackward0,17337,,aten::mm,17342,"[[5120, 8192], [8192, 2560]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17342,973.84130859375,284.5425239999998
autograd::engine::evaluate_function: MmBackward0,17337,,aten::mm,17349,"[[8192, 5120], [5120, 2560]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17349,1006.03662109375,264.8447920000001
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,17359,,aten::add_,17361,"[[5120, 2560], [5120, 2560], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",17361,394.2361653645833,19.86684666666669
autograd::engine::evaluate_function: GeluBackward0,17362,,aten::gelu_backward,17364,"[[2048, 4, 2560], [2048, 4, 2560], []]","void at::native::vectorized_elementwise_kernel<4, at::native::GeluBackwardCUDAKernelImpl(at::TensorIteratorBase&, at::native::GeluType)::{lambda()#2}::operator()() const::{lambda()#3}::operator()() const::{lambda(c10::Half, c10::Half)#1}, std::array<char*, 3ul> >(int, at::native::GeluBackwardCUDAKernelImpl(at::TensorIteratorBase&, at::native::GeluType)::{lambda()#2}::operator()() const::{lambda()#3}::operator()() const::{lambda(c10::Half, c10::Half)#1}, std::array<char*, 3ul>)",17364,393.8271484375,46.84820000000001
autograd::engine::evaluate_function: AddBackward0,17365,,aten::sum,17367,"[[2048, 4, 2560], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",17367,984.9091796875,28.926013333333326
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,17369,,aten::add_,17371,"[[2560], [2560], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",17371,322.6953125,1.6916413333333276
autograd::engine::evaluate_function: MmBackward0,17376,,aten::mm,17381,"[[2560, 8192], [8192, 5120]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17381,1021.2130533854166,284.25808400000005
autograd::engine::evaluate_function: MmBackward0,17376,,aten::mm,17388,"[[8192, 2560], [2560, 5120]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,17388,1012.20458984375,258.974326666667
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,17398,,aten::add_,17400,"[[2560, 5120], [2560, 5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",17400,442.0428059895833,20.895088000000005
autograd::engine::evaluate_function: _CopyToModelParallelRegionBackward,17401,,record_param_comms,17404,"[[[2048, 4, 5120]], [], [], [], [], [], [], [], [], []]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",17404,634.6930338541666,3120.7017079999987
autograd::engine::evaluate_function: MulBackward0,17407,,aten::mul,19969,"[[2048, 4, 5120], [5120]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",19969,412.7335611979167,107.88937066666662
autograd::engine::evaluate_function: MulBackward0,17407,,aten::mul,19970,"[[2048, 4, 5120], [2048, 4, 5120]]","void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",19970,429.6310221354167,70.07906933333331
autograd::engine::evaluate_function: MulBackward0,17407,,aten::sum,19971,"[[2048, 4, 5120], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",19971,1080.2994791666667,41.170751999999986
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,19973,,aten::add_,19975,"[[5120], [5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19975,485.75830078125,1.8251426666666637
autograd::engine::evaluate_function: DivBackward0,19976,,aten::neg,19978,"[[2048, 4, 5120]]","void at::native::vectorized_elementwise_kernel<4, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul> >(int, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul>)",19978,308.22705078125,51.87444266666663
autograd::engine::evaluate_function: DivBackward0,19976,,aten::div,19979,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",19979,383.54541015625,116.74500000000018
autograd::engine::evaluate_function: DivBackward0,19976,,aten::div,19980,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",19980,386.7737630208333,116.31671199999998
autograd::engine::evaluate_function: DivBackward0,19976,,aten::mul,19981,"[[2048, 4, 5120], [2048, 4, 5120]]","void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",19981,387.5489908854167,69.97041066666655
autograd::engine::evaluate_function: DivBackward0,19976,,aten::div,19982,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",19982,429.4137369791667,118.0675066666669
autograd::engine::evaluate_function: DivBackward0,19976,,aten::sum,19983,"[[2048, 4, 5120], [], [], []]","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",19983,472.8658854166667,36.46721466666669
autograd::engine::evaluate_function: DivBackward0,19976,,aten::add,19984,"[[2048, 4, 5120], [2048, 4, 5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19984,478.5472005208333,65.75303600000002
autograd::engine::evaluate_function: MulBackward0,19987,,aten::mul,19989,"[[2048, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",19989,472.7293294270833,1.8501866666666646
autograd::engine::evaluate_function: LinalgVectorNormBackward0,19990,,aten::div,19992,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",19992,304.4337565104167,115.91213733333332
autograd::engine::evaluate_function: LinalgVectorNormBackward0,19990,,aten::eq,19993,"[[2048, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, bool, at::native::(anonymous namespace)::CompareEqFunctor<c10::Half> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, bool, at::native::(anonymous namespace)::CompareEqFunctor<c10::Half> >, std::array<char*, 2ul>)",19993,387.3640950520833,1.88601333333333
autograd::engine::evaluate_function: LinalgVectorNormBackward0,19990,,aten::masked_fill_,19994,"[[2048, 4, 5120], [2048, 4, 1], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1} const&)::{lambda(int)#1})",19994,307.0340169270833,119.03676800000002
autograd::engine::evaluate_function: LinalgVectorNormBackward0,19990,,aten::mul,19997,"[[2048, 4, 1], [2048, 4, 5120]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",19997,385.14599609375,110.14741733333314
autograd::engine::evaluate_function: LinalgVectorNormBackward0,19990,,aten::add_,19998,"[[2048, 4, 5120], [2048, 4, 5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",19998,431.4982096354167,68.39310133333332
autograd::engine::evaluate_function: MmBackward0,20007,,aten::mm,20012,"[[5120, 8192], [8192, 640]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize256x128x32_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas,20012,436.1554361979167,84.33431466666669
autograd::engine::evaluate_function: MmBackward0,20007,,aten::mm,20019,"[[8192, 5120], [5120, 640]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20019,1200.6458333333333,79.87070133333337
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20029,,aten::add_,20031,"[[5120, 640], [5120, 640], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20031,433.2218424479167,6.56953333333332
autograd::engine::evaluate_function: BmmBackward0,20046,,aten::bmm,20050,"[[20, 2048, 2048], [20, 2048, 128]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20050,1064.1090494791667,59.252397333333356
autograd::engine::evaluate_function: BmmBackward0,20046,,aten::bmm,20053,"[[20, 2048, 128], [20, 128, 2048]]",sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20053,1069.2521158854167,68.04951866666673
autograd::engine::evaluate_function: ScaledUpperTriangMaskedSoftmaxBackward,20070,,ScaledUpperTriangMaskedSoftmaxBackward,20071,"[[20, 2048, 2048]]","void (anonymous namespace)::scaled_upper_triang_masked_softmax_warp_backward<c10::Half, c10::Half, float, 11>(c10::Half*, c10::Half*, c10::Half const*, float, int, int, int)",20071,398.4970703125,177.88655866666653
autograd::engine::evaluate_function: BaddbmmBackward0,20084,,aten::bmm,20088,"[[20, 2048, 2048], [20, 2048, 128]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20088,1076.0655924479167,64.77624800000004
autograd::engine::evaluate_function: BaddbmmBackward0,20084,,aten::mul,20090,"[[20, 2048, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",20090,389.0113932291667,8.243691999999985
autograd::engine::evaluate_function: BaddbmmBackward0,20084,,aten::bmm,20093,"[[20, 128, 2048], [20, 2048, 2048]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize64x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20093,1084.0244140625,60.178224000000085
autograd::engine::evaluate_function: BaddbmmBackward0,20084,,aten::mul,20095,"[[20, 128, 2048], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",20095,482.60595703125,8.150285333333285
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,20116,,aten::mul,20119,"[[2048, 4, 5, 128], [2048, 1, 1, 128]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20119,391.47705078125,32.851215999999965
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,20116,,aten::mul,20123,"[[[2048, 4, 5, 64], [2048, 4, 5, 64]], [2048, 4, 5, 128], [2048, 1, 1, 128]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20123,334.91943359375,32.23144800000001
autograd::engine::evaluate_function: NegBackward0,20124,,aten::neg,20126,"[[2048, 4, 5, 64]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20126,301.5362955729167,7.641490666666667
autograd::engine::evaluate_function: SliceBackward0,20127,,aten::fill_,20133,"[[2048, 4, 5, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20133,306.41943359375,4.443706666666686
autograd::engine::evaluate_function: SliceBackward0,20127,,aten::copy_,20136,"[[2048, 4, 5, 64], [2048, 4, 5, 64], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20136,306.88916015625,19.783893333333353
autograd::engine::evaluate_function: SliceBackward0,20127,,aten::add_,20137,"[[2048, 4, 5, 128], [2048, 4, 5, 128], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20137,344.4606119791667,36.440217333333294
autograd::engine::evaluate_function: SliceBackward0,20138,,aten::fill_,20144,"[[2048, 4, 5, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20144,384.7086588541667,4.384870666666672
autograd::engine::evaluate_function: SliceBackward0,20138,,aten::copy_,20147,"[[2048, 4, 5, 64], [2048, 4, 5, 64], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20147,345.2815755208333,19.925006666666665
autograd::engine::evaluate_function: SliceBackward0,20138,,aten::add_,20148,"[[2048, 4, 5, 128], [2048, 4, 5, 128], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20148,369.3408203125,35.85156533333333
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,20149,,aten::mul,20152,"[[2048, 4, 5, 128], [2048, 1, 1, 128]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20152,333.1971028645833,16.252466666666674
autograd::engine::evaluate_function: torch::jit::(anonymous namespace)::DifferentiableGraphBackward,20149,,aten::mul,20156,"[[[2048, 4, 5, 64], [2048, 4, 5, 64]], [2048, 4, 5, 128], [2048, 1, 1, 128]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20156,302.833984375,13.231899999999973
autograd::engine::evaluate_function: NegBackward0,20157,,aten::neg,20159,"[[2048, 4, 5, 64]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20159,302.8146158854167,7.226663999999999
autograd::engine::evaluate_function: SliceBackward0,20160,,aten::fill_,20166,"[[2048, 4, 5, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20166,305.1077473958333,4.420276000000022
autograd::engine::evaluate_function: SliceBackward0,20160,,aten::copy_,20169,"[[2048, 4, 5, 64], [2048, 4, 5, 64], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20169,348.70751953125,7.609759999999993
autograd::engine::evaluate_function: SliceBackward0,20160,,aten::add_,20170,"[[2048, 4, 5, 128], [2048, 4, 5, 128], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20170,350.0935872395833,16.016824
autograd::engine::evaluate_function: SliceBackward0,20171,,aten::fill_,20177,"[[2048, 4, 5, 128], []]","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::Half>, std::array<char*, 1ul>)",20177,359.6661783854167,4.387716000000009
autograd::engine::evaluate_function: SliceBackward0,20171,,aten::copy_,20180,"[[2048, 4, 5, 64], [2048, 4, 5, 64], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})",20180,339.8636067708333,8.560238666666658
autograd::engine::evaluate_function: SliceBackward0,20171,,aten::add_,20181,"[[2048, 4, 5, 128], [2048, 4, 5, 128], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})",20181,321.7425130208333,15.396820000000027
autograd::engine::evaluate_function: SplitBackward0,20182,,aten::cat,20184,"[[[2048, 4, 5, 128], [2048, 4, 5, 128], [2048, 4, 5, 128]], []]","void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",20184,302.9635416666667,72.22602799999997
autograd::engine::evaluate_function: MmBackward0,20193,,aten::mm,20198,"[[1920, 8192], [8192, 5120]]",sm90_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20198,1048.8693033854167,203.16171999999995
autograd::engine::evaluate_function: MmBackward0,20193,,aten::mm,20205,"[[8192, 1920], [1920, 5120]]",sm90_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas,20205,1064.494140625,189.74999333333312
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20215,,aten::add_,20217,"[[1920, 5120], [1920, 5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20217,441.0642903645833,15.80016133333337
autograd::engine::evaluate_function: _CopyToModelParallelRegionBackward,20218,,nccl:all_reduce,20222,"[[2048, 4, 5120]]","ncclDevKernel_AllReduce_Sum_f16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",20221,586.8390299479166,3110.9744386666666
autograd::engine::evaluate_function: MulBackward0,20224,,aten::mul,20226,"[[2048, 4, 5120], [5120]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20226,541.7649739583334,107.88184133333338
autograd::engine::evaluate_function: MulBackward0,20224,,aten::mul,20227,"[[2048, 4, 5120], [2048, 4, 5120]]","void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",20227,429.56982421875,70.07553466666661
autograd::engine::evaluate_function: MulBackward0,20224,,aten::sum,20228,"[[2048, 4, 5120], [], [], []]","void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",20228,1080.82763671875,41.47435466666665
autograd::engine::evaluate_function: torch::autograd::AccumulateGrad,20230,,aten::add_,20232,"[[5120], [5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20232,483.47265625,1.8078559999999955
autograd::engine::evaluate_function: DivBackward0,20233,,aten::neg,20235,"[[2048, 4, 5120]]","void at::native::vectorized_elementwise_kernel<4, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul> >(int, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul>)",20235,351.3126627604167,51.851829333333434
autograd::engine::evaluate_function: DivBackward0,20233,,aten::div,20236,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",20236,470.5362955729167,116.74869333333342
autograd::engine::evaluate_function: DivBackward0,20233,,aten::div,20237,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",20237,471.751953125,116.3483613333336
autograd::engine::evaluate_function: DivBackward0,20233,,aten::mul,20238,"[[2048, 4, 5120], [2048, 4, 5120]]","void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",20238,474.60498046875,70.05778933333325
autograd::engine::evaluate_function: DivBackward0,20233,,aten::div,20239,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",20239,468.7125651041667,121.32044399999997
autograd::engine::evaluate_function: DivBackward0,20233,,aten::sum,20240,"[[2048, 4, 5120], [], [], []]","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)",20240,473.7652994791667,36.36041466666666
autograd::engine::evaluate_function: DivBackward0,20233,,aten::add_,20241,"[[2048, 4, 5120], [2048, 4, 5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20241,475.4625651041667,64.43563599999997
autograd::engine::evaluate_function: MulBackward0,20244,,aten::mul,20246,"[[2048, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",20246,472.0113932291667,1.8470253333333289
autograd::engine::evaluate_function: LinalgVectorNormBackward0,20247,,aten::div,20249,"[[2048, 4, 5120], [2048, 4, 1]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::DivFunctor<c10::Half> > const&)::{lambda(int)#1})",20249,347.0859375,116.3890253333334
autograd::engine::evaluate_function: LinalgVectorNormBackward0,20247,,aten::eq,20250,"[[2048, 4, 1], []]","void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::Half, c10::Half, bool, at::native::(anonymous namespace)::CompareEqFunctor<c10::Half> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::Half, c10::Half, bool, at::native::(anonymous namespace)::CompareEqFunctor<c10::Half> >, std::array<char*, 2ul>)",20250,472.1057942708333,1.8802653333333292
autograd::engine::evaluate_function: LinalgVectorNormBackward0,20247,,aten::masked_fill_,20251,"[[2048, 4, 5120], [2048, 4, 1], []]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#11}::operator()() const::{lambda(c10::Half, bool)#1} const&)::{lambda(int)#1})",20251,349.3968098958333,119.03561599999992
autograd::engine::evaluate_function: LinalgVectorNormBackward0,20247,,aten::mul,20254,"[[2048, 4, 1], [2048, 4, 5120]]","void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",20254,472.8289388020833,110.10750266666646
autograd::engine::evaluate_function: LinalgVectorNormBackward0,20247,,aten::add_,20255,"[[2048, 4, 5120], [2048, 4, 5120], []]","void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)",20255,384.3509114583333,68.40618266666668
